[
  {
    "id": "1763811020432",
    "title": "eu act",
    "description": "Uploaded 11/22/2025",
    "type": "PDF",
    "addedDate": "11/22/2025",
    "status": "Active Case",
    "colorClass": "bg-purple-100",
    "iconClass": "text-purple-600",
    "extractedText": "--- Page 1 ---\n\nThe European Union  Artificial Intelligence  Act  Latest developments and key takeaways  2   February 2024\n\n--- Page 2 ---\n\n1   Back to Contents  The EU Artificial Intelligence Act  Updated, 2 February 2024  ____________________________________________________________________________________________________  The EU institutions are now moving forward with an updated and “final” text of the Artificial Intelligence (AI)  Act, following December’s political agreem ent and   further technical meetings in January.  The AI Act is a landmark in global AI regulation, reflecting the EU’s objective to lead the way in promoting a  comprehensive legislative approach to support the trustworthy and responsible use of AI systems. T he AI Act  follows other major EU digital legislation, such as the General Data Protection Regulation (GDPR), the Digital  Services Act, the Digital Markets Act, the Data Act, and the Cyber Resilience Act.  This paper outlines key elements of the AI Act as i t currently stands   and   provides an overview of the Act’s  tiered compliance obligations.  This paper does not constitute legal advice.  The AI Act will unify how AI is regulated across the single market of the 27 EU Member States. It also has  important extraterritorial implications, as it covers   all AI systems impacting people in the EU , regardless of  where systems are developed or deploye d.  Compliance obligations are significant, and largely determined by the level of risk the usage of an AI system  poses to people’s safety, security, or fundamental rights. Obligations apply along the AI value chain.  The AI Act applies a tiered compliance   framework. Most requirements fall upon the developers and  deployers of AI systems classified as “high - risk”, and on general - purpose AI systems (including foundation  models and generative AI systems) posing “systemic risks”.  The agreement currently sets out   a phased timeline for enforcement, starting with prohibited AI systems in  late 2024 / early 2025 and progressively extending to   nearly   all AI systems by mid - 2027. There are  significant financial penalties for noncompliance.  It is important for business le aders in the EU and beyond to consider the implications of this complex  legislation before it comes into effect. This consideration includes understanding how the AI Act interacts  with existing and emerging rules and regulations in other jurisdictions, as   well as with voluntary AI codes  and principles.  Businesses and other organizations should ensure they have an up - to - date inventory of the AI systems that  they are developing or deploying. They will need to assess whether their systems are subject to compli ance  obligations and, if so, under which classification. Developers and deployers of high - risk and general - purpose  AI systems will also need to ensure that effective AI governance frameworks and compliance systems are in  place.  Key takeaways  Who will   the AI Act affect?  •   The AI Act applies to all AI systems impacting people in the EU (whether these AI systems are built and  operated from within the EU or from elsewhere). It applies across all sectors.  •   The AI Act imposes different obligations across all a ctors in the AI value chain.  •   In certain cases, the AI Act also applies to AI models and systems placed on the market prior to the Act  taking effect, including:  •   If these are G eneral Purpose AI   (GPAI, see definition below)   models .\n\n--- Page 3 ---\n\n2   Back to Contents  •   If these are AI   systems which fall into the “prohibited” category, or if these are “high - risk” AI systems  that are intended to be used by public authorities.  •   Moreover, if an existing AI system undergoes significant changes, it will be treated like the other  systems in its   “updated” risk category that are being placed on the market at the same time.  What are the key features of the AI Act?  •   Definition of AI : The AI Act applies a broad definition of an AI system derived from the recently updated  Organization   for Economic Co - operation and Development definition (see relevant section below).  •   Risk - based approach focuses on use cases : Obligations are primarily based on the level of risk posed by  how an AI system is used (or could be used), not the technology on which it is based.  •   GPAI models   are treated separately due to the breadth of their potential use cases.  •   Risk classification system : The AI Act establishes a tiered compliance framework consisting of different  categories of risk and different requirements for each such   category. All AI systems will need to be  inventoried and assessed to determine their risk category and the ensuing responsibilities.  •   Prohibited systems : Systems posing what legislators consider an unacceptable risk to people’s safety,  security and fundamen tal rights will be banned from use in the EU.  •   High - risk AI systems : These systems will carry the majority of compliance obligations (alongside GPAI  systems   -   see below), including the establishment of risk and quality management systems, data  governance,   human oversight, cybersecurity measures, post - market monitoring, and maintenance of  the required technical documentation. (Further obligations may be specified in subsequent AI  regulations for healthcare, financial services, automotive, aviation, and other   sectors.)  •   Minimal - risk AI systems : Beyond the initial risk assessment and some transparency requirements for  certain AI systems, the AI Act imposes no additional obligations on these systems but invites  companies to commit to codes of conduct on a volunta ry basis.  •   Pre - market conformity assessments for high - risk AI systems : High - risk systems will require a conformity  assessment to evidence their compliance before being placed on the market :  •   The application of harmonized standards (currently under   development, see below) will allow AI  system providers to demonstrate compliance by self - assessment.  •   In limited cases, a third - party conformity assessment performed by an accredited independent  assessor (“notified body”) will be required.  •   General purpose A I systems (GPAI), including foundation models and generative AI : These advanced  models and systems will be regulated through a separate tiered approach, with additional obligations for  models posing a “systemic risk”.  •   Measures to support innovation : Regula tory “sandboxes” will be made available across the EU for  operators (especially small and medium enterprises) to access voluntarily. Here they can innovate,  experiment, test, and validate the compliance of their AI systems with the AI Act in a safe environ ment.  •   Interaction with other EU laws : Obligations under the AI Act will need to be integrated into the  compliance processes already established to implement existing EU laws, e.g., laws regarding product  safety, privacy, and financial services.  •   Enforcemen t and penalties : National competent authorities will have enforcement powers with the  capacity to impose significant fines depending on the level of noncompliance.  •   For use of prohibited AI systems, fines may be up to 7% of worldwide annual turnover (revenu e), while  noncompliance with requirements for high - risk AI systems will be subject to fines of up to 3% of the  same.  When will the AI Act take effect?  •   The Act is currently expected to enter into force in   Q2 - Q3   2024 ,   with different obligations then   taking  effect in stages. Some key dates are outlined below:\n\n--- Page 4 ---\n\n3   Back to Contents  •   AI Act prohibitions will start to be enforced six months after the Act enters into force (Q4 2024   –   Q1  2025).  •   GPAI obligations will take effect 12 months after entry into force ( Q2 - Q3   2025), but   with one  exception: GPAI models which have been placed on the market before this date will have an additional  24 months to comply (so from   Q2 - Q3   2027).  •   Most other obligations will take effect 24 months after the Act enters into force (so ,   Q2 - Q3   2026).  •   How ever:  •   Obligations for AI systems that are classified as high - risk because they are a safety  component of a system that is subject to Union harmonization legislation   (listed in Annex II),  will only take effect 36 months after the Act enters in force (so fro m   Q2 - Q3   2027).  •   Obligations for high - risk AI systems intended for use by public authorities that were on the  market before the entry into force of the AI Act, will only take effect 48 months after entry  into force (so from   Q2 - Q3   2028).  What actions   should   companies and other organizations   take from the outset?  1)   Inventory all AI systems you have (or potentially will have) developed or deployed and determine  whether any of these systems falls within the scope of the AI Act.  2)   Assess and categorize the in - scope A I systems to determine their risk classification and identify the  applicable compliance requirements.  3)   Understand your organization’s position in relevant AI value chains, the associated compliance  obligations and how these obligations will be met. Complian ce will need to be embedded in all  functions responsible for the AI systems along the value chain throughout their lifecycle.  4)   Consider what other questions, risks (e.g., interaction with other EU or non - EU regulations, including  on data privacy), and oppo rtunities (e.g., access to AI Act sandboxes for innovators, small and  medium enterprises, and others) the AI Act poses to your organization’s operations and strategy.  5)   Develop and execute a plan to ensure that the appropriate accountability and governance  f rameworks, risk management and control systems, quality management, monitoring, and  documentation are in place when the Act comes into force.\n\n--- Page 5 ---\n\n4   Back to Contents  Contacts  For questions about AI public policy and regulation:   For questions about AI:  Shawn Maher  EY Global Vice Chair, Public Policy  shawn.maher@eyg.ey.com  Nicola Morini Bianzino  EY Global Chief Technology Officer  nicola.morini.bianzino@eyg.ey.com  Ansgar Koene  EY Global AI Ethics an d Regulatory Leader  ansgar.koene1@be.ey.com  Beatriz Saiz - Sanz  EY Global Data and AI Leader  beatriz.sanzsaiz@es.ey.com  Anne McCormick  EY EMEIA Digital Policy Leader  anne.mccormick@uk.ey.com  Jay Persaud  EY Global Emerging Technologies Ecosystem Leader  jay.persaud@ey.com  Andrew Hobbs  EY EMEIA Public Policy Leader  ahobbs@uk.ey.com  Richard Jackson  EY Global AI Assurance Leader  richard.jackson@ey.com  John Hallmark  EY Americas Public Policy  EY US Political and Legislative Leader  Ernst and Young LLP  john.hallmark@ey.com  Dan Diasio  EY Global AI Consulting Leader  dan.diasio@ey.com  Yi Xie  EY Asia - Pacific Public Policy  yi.y.xie@hk.ey.com  Frank de Jonghe  EY EMEIA Trusted AI Leader  frank.de.jonghe1@uk.ey.com  Dean Protheroe  EY EMEIA Public Policy  dprotheroe@uk.ey.com  Peter Katko  EY Global Digital Law Leader  peter.katko@de.ey.com\n\n--- Page 6 ---\n\n5   Back to Contents  Contents  Key takeaways   ................................ ................................ ................................ ................................ ..........   1  Contacts   ................................ ................................ ................................ ................................ ...................   4  Context   ................................ ................................ ................................ ................................ .....................   6  Who is affected?   ................................ ................................ ................................ ................................ ........   6  When will the AI Act be implement ed?   ................................ ................................ ................................ .........   7  How does the EU define an AI system?   ................................ ................................ ................................ ........   7  How are AI systems classified?   ................................ ................................ ................................ ...................   8  Prohibited systems: which use cases pose an unacceptable risk?   ................................ ................................ ..   8  High - risk   systems: which use cases are subject to conformity assessments and obligations?   ..........................   9  What are the ob ligations for providers of high - risk AI systems?   ................................ ................................ ..   10  General obligations   ................................ ................................ ................................ ................................ ..   10  Pre - market conformity assessment for high - risk systems   ................................ ................................ ...........   10  Post - market obligations   ................................ ................................ ................................ ...........................   10  What are the obligations for deployers, importers and distributors of high - risk AI systems?   .........................   11  Minimal - risk systems: what obligations apply?   ................................ ................................ ...........................   11  How will general - purpose AI be regulated?   ................................ ................................ ................................   12  How will the AI Act interact with existing legislation and standards?   ................................ ...........................   13  How will new standards be developed and when will they be ready?   ................................ ............................   13  Codes of Practice to support compliance with GPAI obligations   ................................ ................................ ...   13  How does the AI Act aim to support AI innovation in the EU?   ................................ ................................ ......   13  AI regulatory sandboxes   ................................ ................................ ................................ ...........................   13  Real - world testing   ................................ ................................ ................................ ................................ ....   14  What will the regulatory oversight model for   the AI Act look like?   ................................ ...............................   14  What are the penalties for noncompliance?   ................................ ................................ ...............................   15  What are the next steps around and beyond the AI Act?   ................................ ................................ .............   15  The EU AI Pact ................................ ................................ ................................ ................................ .........   15  Appendix   ................................ ................................ ................................ ................................ .................   16\n\n--- Page 7 ---\n\n6   Back to Contents  Context  The AI Act is intended to advance four key objectives: 1  (i)   To ensure that AI systems placed on the EU market are safe and respect fundamental rights  (ii)   To ensure legal certainty to facilitate investment and innovation in AI  (iii)   To   enhance governance and effective enforcement of EU law on fundamental rights and safety  requirements applicable to AI systems  (iv)   To facilitate the development of a single market for lawful, safe and trustworthy AI applications,  and prevent market fragmentatio n  Who is affected?  The AI Act is broad in scope and comes with significant obligations along the value chain. It focuses on the  impact of AI systems on people, specifically on their wellbeing and fundamental rights.  It also contains extraterritorial measu res, affecting any business or organization that offers an AI system  impacting people within the EU, regardless of where the organization is headquartered.  Under certain conditions the AI Act also applies to AI systems that were put on the market prior to   the Act  taking effect:  •   If these are GPAI models.  •   If these are AI systems which fall into the “prohibited” category, or if these are “high - risk” AI systems that  are intended to be used by public authorities.  •   Moreover, if an existing AI system undergoes sign ificant changes, it will be treated like the other systems  in its “updated” risk category that are being placed on the market at the same time.  The AI Act will apply to (please see the appendix section below for full definitions of terms):  •   Providers puttin g AI systems on the market within the EU, regardless of their location  •   Providers and deployers of AI systems located in a non - EU country, where the output of the AI system is  used within the EU  •   Deployers of AI systems located in the EU  •   Importers and distri butors placing AI systems on the EU market  •   Product manufacturers placing products with AI systems on the EU market under their own name or  trademark  The AI Act will   not   apply to:  •   Public authorities in non - EU countries and international organizations that h ave law enforcement and  judicial cooperation agreements with the EU, provided that adequate safeguards are in place  •   AI systems used for purposes outside the scope of EU law - making authority, such as military or defense  •   AI systems specifically developed an d used for the sole purpose of scientific research and discovery  •   Research, testing and development activity regarding AI systems prior to placement on the market or into  service  •   Free and open - source software, unless their use would classify them as a   prohibited or high - risk AI  system, or their use would subject them to transparency obligations  1   “ EU AI Act Proposal, 2021   –   Explanatory Memorandum”, European Commission, April 2021   https://eur -  lex.europa.eu/legal - content/EN/TXT/HTML/?uri=CELEX:52021PC0206\n\n--- Page 8 ---\n\n7   Back to Contents  When will the AI Act be implemented?  The AI Act is expected to be approved by the European Parliament and Council and published in the Official  Journal in   Q2 - Q3   of 2024, after which it will come into force. As an EU regulation (as opposed to a directive),  it will be directly effective in Memb er States without the need for local enabling legislation.  The timeline for compliance with the provisions of the AI Act will be as follows:  Timeframe   Development  Calendar   Q2 - Q3   2024   AI Act expected to come into force.  Immediately after entry into  force  The European Commission must begin work to establish the AI  Office (EU oversight body) while Member States make provisions to  establish AI regulatory sandboxes.  (To note: the work t o establish the AI Office has already begun).  Six months after entry into force  (Q4 2024   –   Q1 2025)   AI Act prohibitions will come into effect.  12 months after entry into force  ( Q2 - Q3   2025)  Requirements for GPAI models will come into effect.   However,  GPAI models that were already on the market before this date will  have an additional 24 months to comply (see below).  24 months after entry into force  ( Q2 - Q3   2026)  Requirements for high - risk AI systems (classified under uses listed  in Annex III)   will come into effect, alongside transparency  requirements for certain other AI systems.  36 months after entry into force  ( Q2 - Q3   2027)  Requirements for high - risk AI systems classified under EU  harmonization laws contained in Annex II will come into effect .  GPAI models that were already on the market before obligations  began to apply twelve months after entry into force (see above),  will now have to comply.  48 months after entry into force  ( Q2 - Q3   2028)  High - risk AI systems intended for use by public autho rities that  were on the market before the entry into force of the AI Act should  now be compliant.  How does the EU define an AI system?  The AI Act’s definition of an AI system is derived from the recently updated   definition used by the Organisation  for Economic Co - operation and Development (OECD) . The objective in using the OECD definition as a basis, is  to encourage international alignment and continuity with other laws and codes. The AI Act defines an AI  system   as follows:  “An AI system is a machine - based system designed to operate with varying levels of autonomy and  that may exhibit adaptiveness after deployment and that, for explicit or implicit objectives, infers,  from the input it receives, how to generate ou tputs such as predictions, content, recommendations, or  decisions that can influence physical or virtual environments.”  The AI Act emphasizes that a key characteristic that differentiates AI systems from simpler and more  traditional software systems is th eir capability to infer. It states that the techniques that enable inference  while building an AI system include machine learning approaches that learn from data how to achieve a certain  objective, and logic -   and knowledge - based approaches that infer from   encoded knowledge or symbolic  representation of the task to be solved. The capacity of an AI system to infer goes beyond basic data  processing, enable learning, reasoning, or modelling.\n\n--- Page 9 ---\n\n8   Back to Contents  How are AI systems classified?  The AI Act sets compliance   obligations based on the inherent risks that arise from the application for which AI  systems are used .  General - purpose AI systems (GPAI), including foundation models and generative AI systems, follow a separate  classification framework. Please see the rel evant section below.  AI systems are classified as follows in the Act :  Classification  (Risk - based tier)   Description   Compliance  level  Use case examples  (see sections below for fuller details)  Prohibited AI  systems  Prohibited   because  uses pose an  unacceptable risk to  the safety, security,  and fundamental  rights of people.  Prohibition  Includes use of AI for   social scoring   which  could lead to detrimental treatment,  emotional recognition   systems in the  workplace,   biometric categorization   to  infer sensitive data, and   predictive policing  of individuals, among other uses. Some  exemptions will apply.  High - risk AI  systems  Permitted,   subject to  compliance with the  requirements of the  AI Act (i ncluding  conformity  assessments before  being placed on the  market).  Significant  Includes use of AI in:  •   Recruitment ,  •   Biometric identification surveillance  systems,  •   Safety components   of systems  covered by harmoni z ed legislation  (e.g.,   medical devices, auto motive )  •   Access to essential private and public  services (e.g.,   creditworthiness,  benefits, health and life insurance ),  •   Safety of critical infrastructure   (e.g.,  energy, transport ).  Minimal risk AI  systems  Permitted,   subject to  specific   transparency  and disclosure  obligations where  uses pose a limited  risk.  Limited  Certain AI systems that interact directly  with people (e.g.,   chatbots ), and visual or  audio   “deepfake”   content that has been  manipulated by an AI system.  Permitted,   with no  additional AI Act  requirements where  uses pose minimal  risk.  Minimal  By default, all other AI systems that do not  fall into the above categories (e.g., photo -  editing software, product - recommender  systems, spam filtering software,  schedul ing software)  Prohibited systems: which use cases pose an unacceptable risk?  The AI Act prohibits AI systems that pose unacceptable risks and that can be used to undermine a person’s  fundamental rights, or that may subject them to physical or   psychological harm. These prohibitions include:  •   AI systems that exploit vulnerabilities, or deploy subliminal techniques, to manipulate a person or a  specific group (e.g., children, the elderly, or people with disabilities), circumventing the users’ free w ill in a  manner likely to cause harm.  •   AI systems used for the social scoring, evaluation, or classification of people based on their social  behavior, inferred, or predicted, or personal characteristics, leading to detrimental treatment.\n\n--- Page 10 ---\n\n9   Back to Contents  •   AI systems used to   infer emotions of people in the workplace (such as human resource functions) and  educational institutions. Exemptions apply for some safety systems (e.g., detection of the drowsiness of  pilots).  •   Biometric categorization to infer sensitive data, such as rac e, sexual orientation, or religious beliefs.  •   Indiscriminate and untargeted scraping of facial images from the internet or CCTV to populate facial  recognition databases.  •   Predictive policing of individuals , defined as predicting individual behavior such as i ndividual likelihood of  offense or re - offense.  •   Law enforcement use of real - time remote biometric identification (RBI) systems in publicly accessible  spaces (certain exceptions apply   subject to prior judicial authorization and for strictly defined lists of  criminal offenses ).  High - risk systems: which use cases are subject to conformity  assessments and obligations?  The AI Act identifies high - risk uses in Annex II and Annex III. The European Commission is empowered to  update these annexes as new uses and risks   are identified. The following high - risk uses are currently listed:  •   AI systems used as a safety component of a product covered by EU harmonization legislation, including  but not limited to: 2  •   Medical devices   •   Marine equipment  •   Motor vehicles   •   Agricultural vehicles  •   Machinery   •   Railway interoperability  •   Civil aviation   •   Toys  •   AI systems applied in uses that pose a   significant risk of harm to health, safety, or fundamental rights: 3  •   Biometric identification and categorization of people  •   Management and operation of critical infrastructure (specifically, safety components of traffic, water,  gas, heating, and electricity infrastructure)  •   Education and vocational training (specifically, systems determining access to education and  assessment of   students)  •   Employment, worker management and access to self - employment (including recruitment and  performance monitoring)  •   Access to and enjoyment of essential private and public services and benefits (including eligibility for  benefits, evaluating creditwo rthiness, and pricing of life and health insurance, although those used for  purposes of detecting financial fraud are specifically not included)  •   Law enforcement uses such as data analytics systems to assess evidence of criminal activity  •   Migration, asylum,   and border control management (including monitoring of migration trends, border  surveillance, verification of travel documents, and examination of applications for visas, asylum, and  residence permits)  •   Administration of justice and democratic processes (in cluding researching and interpreting the law)  Exceptions to high - risk classification :  However, an AI system will not be considered high - risk if it:  2   Annex II, List of Union harmonisation legislation, EU   Artificial Intelligence Act Proposal, Version 21 January 2024  3   Annex III, High - risk AI systems referred to in Article 6(2), EU Artificial Intelligence Act Proposal, Version 21 January 2024\n\n--- Page 11 ---\n\n10   Back to Contents  •   Performs a narrow procedural task with no direct safety or security implications  •   Is meant to review or impr ove the quality of human output  •   Is used to detect decision - making patterns (or deviations from existing patterns to flag inconsistencies)  without influencing decisions  •   Is used for purposes of detecting financial fraud  What are the obligations for provider s of high - risk AI systems?  General obligations  Requirements for high - risk   AI systems include :  •   Establishing and maintaining appropriate AI   risk   and   quality   management   systems  •   Effective   data governance  •   Maintaining appropriate   technical documentation   and   record - keeping  •   Transparency   and provision of information to users  •   Enabling   and conducting   human oversight  •   Compliance with standards for   accuracy, robustness, and cybersecurity   for the intended purpose  •   Registering high - risk AI   systems on the EU database   before placing them on the market; systems used  for law enforcement, migration, asylum and border control, and critical infrastructure will be registered in  a non - public section of the database  Pre - market conformity assessment fo r high - risk systems  Providers must perform a conformity assessment on the high - risk AI system before placing it on the market:  •   The conformity assessment should examine whether the requirements laid out above have been met  In most cases,   providers can self - assess   if:  •   They apply procedures and methodologies that follow EU approved technical standards (harmonized  standards) that allow a presumption of conformity  A third - party conformity assessment   by an accredited body (notified body) is required if any of th e following  criteria apply:  •   The AI system is   part of a safety component subject to third - party assessment under Union harmonized  regulations (see above)  •   The AI system is   part of a biometric identification system  •   Harmonized standards are not used  Post - market obligations  Once a high - risk AI system has been placed on the market, providers continue to have obligations to ensure  ongoing safe performance and conformity over the system’s lifecycle. These include:  •   Maintaining   logs   generated by high - risk s ystems, to the extent that they are under their control, for a  period of at least six months  •   Immediately taking the necessary corrective actions   for nonconforming systems already on the market  and informing other operators in the value chain of the nonconf orming systems  •   Cooperating with the national competent authorities or the AI Office   (see relevant section below)   by  sharing all the information and documentation   necessary to show conformity upon receiving a reasonable  request\n\n--- Page 12 ---\n\n11   Back to Contents  •   Monitoring performance and sa fety   of AI systems throughout their lifetime and actively evaluating  continuous compliance with the AI Act  •   Reporting to the appropriate authorities, serious incidents   and malfunctions that lead to breaches of  fundamental rights  •   Undergoing new conformity as sessments for substantial modifications   (e.g., changes to a system’s  intended purpose or changes that affect how it meets regulations):  •   This applies whether the changes are made by the original provider or any third party.  •   For AI systems that are considered to have limited or minimal risk, it will be important to check  whether the original risk classification still applies after any chang es.  What are the obligations for deployers, importers and distributors of  high - risk AI systems?  Obligations of deployers   of high - risk AI systems include:  •   Completing a fundamental rights impact assessment (FRIA) before putting the AI system in use, if   the  deployer:  •   Is a public body or private entity providing public services  •   Provides essential private service that cover creditworthiness evaluation of persons, and risk  assessment and pricing in relation to life and health insurance  •   Implementing human ove rsight by people with the appropriate training and competence  •   Ensuring that input data is relevant to the use of the system  •   Suspending the use of the system if it poses a risk at a national level  •   Informing the AI system provider of any serious incidents  •   R etaining the automatically - generated system logs  •   Complying with the relevant registration requirements when the user is a public authority  •   Complying with GDPR obligations to perform a data protection impact assessment  •   Verifying the AI system is compliant w ith the AI Act and that all relevant documentation is evidenced  •   Informing people, they might be subject to the use of high - risk AI  Before placing a high - risk AI system on the market, it is the responsibility of importers and distributors to :  •   Verify that t he system complies with the AI Act, ensure that all relevant documentation is evidenced, and  communicate with the provider and market surveillance authorities accordingly  Minimal - risk systems: what obligations apply?  For some specific AI systems,   limited transparency obligations apply.  Providers must:  •   Design and develop systems in a way to make certain that people understand that they are interacting  with an AI system from the outset (e.g., chatbots)  Deployers must:  •   Inform and obtain the   consent of people exposed to permitted emotion recognition or biometric  categorization systems (e.g., safety systems monitoring driver attentiveness)  •   Disclose and clearly label where visual or audio “deep fake” content has been manipulated by AI.\n\n--- Page 13 ---\n\n12   Back to Contents  How will   general - purpose AI be regulated?  The definition in the AI Act of general - purpose AI (GPAI) models is:  “General - purpose AI model means an AI model, including when trained with a large amount of data  using self - supervision at scale, that displays significan t generality and is capable to competently  perform a wide range of distinct tasks regardless of the way the model is placed on the market and  that can be integrated into a variety of downstream systems or applications. This does not cover AI  models that ar e used before release on the market for research, development and prototyping  activities.”  The AI Act adopts a tiered approach to compliance obligations,   differentiating between high - impact GPAI  models with systemic risk, and other GPAI models.   The AI   Act defines “systemic risk at Union level” as:  “A   risk that is specific to the high - impact capabilities of general - purpose AI models, having a  significant impact on the internal market due to its reach, and with actual or reasonably foreseeable  negative ef fects on public health, safety, public security, fundamental rights, or the society as a whole,  that can be propagated at scale across the value chain.”  The GPAI tiers are as follows:  Tier   Description   Compliance level  Base - level tier   Models meeting the   GPAI definition   Limited transparency obligations  Systemic risk  tier  High - impact GPAI models posing a systemic risk are  provisionally identified based on cumulative amount of  computing power used for training (with power  greater than 10 25   floating point op erations [FLOPs]).  A model can also be classified in this tier based on a  decision of the Commission that a general - purpose AI  model has capabilities or impact equivalent to those  above.  Significant obligations  Providers of all GPAI models   will be required to:  •   Keep and maintain up - to - date technical documentation.  •   Make information available to downstream providers who intend to integrate the GPAI model into their AI  systems.  •   Put in place a policy to respect EU copyright law.  •   Disseminate deta iled summaries about the content used for training.  Exceptions to base - level GPAI transparency obligations :  •   Unless the GPAI models present systemic risks, these obligations shall not apply to providers of GPAI  models that are made accessible to the   public under a free and open - source license, and whose  parameters are made publicly available.  In addition, providers of high - impact GPAI models posing a systemic risk must:  •   Perform model evaluations.  •   Assess and mitigate systemic risks.  •   Document and report   to the European Commission any serious incidents and the corrective action taken.  •   Conduct adversarial training of the model (i.e., “red - teaming”).  •   Ensure that an adequate level of both cybersecurity and physical protections are in place.  •   Document and rep ort the estimated energy consumption of the model.\n\n--- Page 14 ---\n\n13   Back to Contents  To provide agility for adapting to rapid GPAI technology developments, the AI Office (see relevant section  below) will:  •   Update the designation criteria for high - impact GPAI, with possible   inclusion of criteria related to the  number of model parameters, quality or size of datasets, number of registered business or end users.  •   Facilitate the formulation of codes of practice to support the application of the compliance requirements.  How will th e AI Act interact with existing legislation and standards?  •   AI providers must continue to adhere to all relevant EU laws while incorporating requirements of the AI  Act.  •   Providers can combine AI Act compliance with existing procedures to avoid duplication an d ease the  compliance workload.  •   Where applicable, the AI Act should be embedded into relevant EU laws (e.g., financial services  regulations). Sectoral regulators will be designated as the relevant competent authorities to supervise the  enforcement of the A I Act for their sector.  How will new standards be developed and when will they be ready?  To reduce compliance burdens and speed up time - to - market, the AI Act allows for compliance self - assessment,  provided the obligations are met using European Commission - approved industry best practices as formalized  in “harmonized standards”.  •   The European Commission has issued a “standardization request” to the European standards bodies (CEN  and CENELEC), listing a series of topics for which new harmonized standards are   required to cover the  compliance obligations in the AI Act (see section on pre - market obligations of high - risk AI systems above).  •   The European standardization bodies aim to have standards available in time for implementation of the AI  Act in accordance wi th the agreed timelines (see above), but their readiness is not guaranteed.  •   Where possible the European standardization bodies will seek to adopt standards created by the  international standards bodies (ISO and IEC), with minimal modification.  Codes of   Practice to support compliance with GPAI obligations  Providers of high - impact GPAI models posing a systemic risk may rely on codes of practice to demonstrate  compliance until a harmonized standard is published.  The EU’s new AI Office (see below) shall enco urage and actively support the drawing up of codes of practice at  Union level, to facilitate the effective implementation of the obligations regarding the detection and labelling  of artificially generated or manipulated content. The Commission is empowered   to adopt implementing acts to  approve these codes of practice.  How does the AI Act aim to support AI innovation in the EU?  AI regulatory sandboxes  The AI Act mandates the establishment of   AI regulatory sandboxes to offer   innovation support across the EU.  •   These regulatory sandboxes are   controlled environments   in which providers and deployers (e.g., small  and medium enterprises) can voluntarily experiment, test, train, and validate their systems under  regulatory supervision before placing them on the market .  •   Each Member State will be expected to create a sandbox with common rules for consistent use across the  EU.  •   AI system providers will be able to receive a written report about their sandbox activities as evidence that  they have met AI Act requirements. Thi s is intended to speed up the approval process to take AI systems  to market.\n\n--- Page 15 ---\n\n14   Back to Contents  Real - world testing  Testing of AI systems in real - world conditions outside of AI regulatory sandboxes may be conducted by  providers or prospective providers of the high - risk AI sys tems listed in Annex III of the AI Act (see above), at  any time before being placed on the market, if the following conditions are met:  •   A testing plan has been submitted to, and approved by the market surveillance authorities  •   The provider is established in   the EU  •   Data protection rules are observed  •   Testing does not last longer than necessary and no more than six months (with the option to extend by an  additional six months)  •   End users have been informed, given their consent and have been provided with releva nt instructions  •   The predictions, recommendations and decisions of the AI system can be effectively reversed or  disregarded  What will the regulatory oversight model for the AI Act look like?  National competent authorities will be given oversight powers in M ember States. These are likely to take  different forms depending on the Member State.  At an EU level, the AI Act governance framework also establishes the:  •   AI Office   within the EU Commission, but with functional independence  •   This new body will have   oversight responsibilities for GPAI models. It will contribute to the  development of standards and testing practices, coordinate with the national competent authorities  and help enforce the rules in Member States  •   AI Board   representing the Member States to   provide strategic oversight for the AI Office  •   The Board will support the implementation of the AI Act and regulations promulgated pursuant to it,  including the design of codes of practice for GPAI models  •   Scientific panel   of independent experts   to support t he activities of the AI Office  •   The panel will contribute to the development of methodologies for evaluating the capabilities of GPAI  models and their subsequent classification, while also monitoring possible safety risks  •   Advisory forum   with representatives   of industry and civil society  •   Will provide technical expertise to the AI Board\n\n--- Page 16 ---\n\n15   Back to Contents  What are the penalties for noncompliance?  The AI Act sets out a strict enforcement regime for noncompliance.  There are three notional levels of noncompliance, each with signif icant financial penalties. Depending on the  level of violation (in line with the risk - based approach), the Act applies the following penalties:  Noncompliance case   Proposed fine  Breach of AI Act prohibitions   Fines up to €35 million or 7% of total   worldwide  annual turnover (revenue), whichever is higher  Noncompliance with the obligations set out for  providers of high - risk AI systems or GPAI models,  authorized representatives, importers, distributors,  users or notified bodies  Fines up to €15 million   or 3% of total worldwide  annual turnover (revenue), whichever is higher  Supply of incorrect or misleading information to the  notified bodies or national competent authorities in  reply to a request  Fines up to €7.5 million or 1.5% of total worldwide  annua l turnover (revenue), whichever is higher  In the case of small and medium enterprises, fines will be as described above, but whichever amount is lower.  National competent authorities will determine the fines in line with the guidance provided above.  Wh at are the next steps around and beyond the AI Act?  The EU AI Act next steps:  Over the coming months the AI Act will be put to the European Parliament and Council for final approval.  It is currently expected that the Act will be approved by the end of April 2024, and then be published in the EU  official journal in   Q2 - Q3   2024. The AI Act will come into force 20 days after publication, at which point the  phased implementation timeline sh all begin.  International alignment:  At an international level, the European Commission and other EU institutions will continue to work with multi -  national organizations including the Council of Europe, the U.S. –   EU Trade and Technology Council   (TTC), the  G7, the OECD, the G20, and the UN to promote the development and adoption of rules beyond the EU that are  compatible with the requirements of the AI Act.  The EU AI Pact  The European Commission is planning to launch a voluntary   AI Pact   as soon as the AI Act is adopted. While a  number of aspects of this proposed Pact are still to be clarified, it appears that it will seek the voluntary  comm itment of industry to start implementing some of the requirements of the AI Act ahead of the legal  deadlines:  •   The Commission will convene interested industry actors (EU and non - EU) in early 2024 to discuss the  proposed AI Pact and start to exchange best p ractices .  •   Once the AI Pact is launched, organizations will have the opportunity to sign - up and to make voluntary  public commitments reflecting some of the steps they are taking to prepare for compliance with the AI  Act.  ___________________________________ _________________________________________________________________\n\n--- Page 17 ---\n\n16   Back to Contents  Appendix  AI Act term   AI Act definition  Provider  A natural or legal person, public authority, agency, or other body that is or has  developed   an AI system to place on the market, or to put into service under its own name  or trademark whether for payment, or free of charge.  Deployer   A natural or legal per son, public authority, agency, or other body   using   an AI system  under its authority.  Authorized  representative  Any natural or legal person located or established in the EU who has received and  accepted a written mandate from a provider to   carry out its ob ligations on its behalf.  Importer  Any natural or legal person located or established in the EU that places on the market an  AI system that bears the name or trademark of a natural or legal person established  outside the EU.  Distributor   Any natural or legal person in the supply chain,   not being the provider or importer, who  makes an AI system available in the EU market.  Product  manufacturer  A manufacturer of an AI system that is put on the market or a manufacturer that puts  into service   an AI system   together with its product   and under its own name or  trademark.  Operator   A general term referring to all the terms above (provider, deployer, authorized  representative, importer, distributor, or product manufacturer).\n\n--- Page 18 ---\n\n17   Back to Contents  EY |   Building a better working world  EY exists to build a better working world, helping to  create long - term value for clients, people and society  and build trust in the capital markets.  Enabled by data and technology, diverse EY teams in  over 150 countries pro vide trust through assurance  and help clients grow, transform and operate.  Working across assurance, consulting, law, strategy,  tax and transactions, EY teams ask better questions to  find new answers for the complex issues facing our  world today.  EY refer s to the global organization, and may refer to one or  more, of the member firms of Ernst & Young Global Limited, each  of which is a separate legal entity. Ernst & Young Global Limited, a  UK company limited by guarantee, does not provide services to  clients . Information about how EY collects and uses personal data  and a description of the rights individuals have under data  protection legislation are available via ey.com/privacy. EY member  firms do not practice law where prohibited by local laws. For more  inf ormation about our organization, please visit ey.com.  © 2024 EYGM Limited.  All Rights Reserved.  EYG no. 001061 - 24Gbl  ED None  This material has been prepared for general informational  purposes only and is not intended to be relied upon as accounting,  tax,   legal or other professional advice. Please refer to your  advisors for specific advice.  ey.com",
    "analysis": {
      "governance_power_accountability": "The Artificial Intelligence (AI) Act by the European Union (EU) represents a centralized governance structure where institutional power is enacted through legislation. The goal of this act, defined by the EU, is to promote trustworthy and responsible use of AI systems. Accountability is enforced through significant financial penalties, and regulatory obligations apply along the AI value chain, from development to deployment.",
      "plurality_inclusion_embodiment": "The AI Act applies to all AI systems impacting people in the EU, regardless of where they are developed or deployed, indicating a broad scope of inclusion. However, the document does not explicitly address whether the act values or incorporates diverse knowledge systems or embodied experiences, including Indigenous, disability, or non-Western perspectives.",
      "agency_codesign_self_determination": "The AI Act appears to impose external controls rather than allowing for community agency, co-design, or the right to refuse. While the Act is applicable across all sectors and affects all AI systems impacting people in the EU, it is unclear if there are mechanisms for affected parties to participate in its design or implementation.",
      "reflexivity_situated_praxis": "There's no explicit evidence that the designers of the AI Act have examined their own positionality, history, and value assumptions. The Act's emphasis on compliance and regulation suggests an orientation towards procedural legitimacy without clear reflection on the structural inequities that could shape enforcement and implementation.",
      "legitimacy_claims": {
        "source": "Technocratic",
        "mechanisms": "Legitimacy is established through the EU's institutional authority and legal mechanisms, with the Act forming part of a series of major EU digital legislation.",
        "tensions": "There may be tensions between the technocratic legitimacy of the AI Act and democratic or market-based legitimacy, given the Act's broad reach and potential impact on diverse stakeholders."
      },
      "key_insight": "The EU's AI Act illustrates a top-down, technocratic approach to AI regulation, with potential tensions between centralized governance and the need for diverse stakeholder inclusion.",
      "governance_scores": {
        "centralization": 85,
        "rights_focus": 70,
        "flexibility": 30,
        "market_power": 75,
        "procedurality": 90
      },
      "structural_pillars": {
        "risk": {
          "title": "Risk Management",
          "description": "AI system's risk determines compliance obligations",
          "badge": "Risk-based"
        },
        "enforcement": {
          "title": "Financial Penalties",
          "description": "Noncompliance can lead to significant financial penalties",
          "badge": "Penalizing"
        },
        "rights": {
          "title": "Broad Applicability",
          "description": "Applies to all AI systems impacting people in the EU",
          "badge": "Universal"
        },
        "scope": {
          "title": "Extraterritorial Reach",
          "description": "Covers systems deployed globally that impact people in the EU",
          "badge": "Global"
        }
      }
    },
    "cultural_framing": {
      "state_market_society": "The text implies a strong role for government in regulating technology, specifically AI, across markets and societies. It expects the public and private sectors to comply with these regulations. The private sector/businesses are framed as entities that must adapt to government regulations, reflecting a strong state-market dynamic where the state holds regulatory power. Society (citizens or consumers) is positioned as a group to be protected by such regulations.",
      "technology_role": "Technology, particularly AI, is portrayed as both an opportunity and a potential threat that needs to be responsibly managed. The potential of AI systems to impact safety, security, or fundamental rights is implicitly acknowledged, which implies that technology is seen as deeply integrated into social life, rather than a neutral tool.",
      "rights_conception": "The rights conception seems more focused on collective rights (e.g., safety, security, fundamental rights of people in the EU) rather than individual rights. It also leans towards a substantive interpretation of rights, with actual impacts of AI systems on rights being the basis for regulation, rather than the mere procedural fairness of AI systems.",
      "historical_context": "The EU's historical experiences with data privacy (e.g., GDPR) and commitment to human rights are reflected in the AI Act. Colonial history doesn't seem to have a direct influence on this particular document, but the EU's global influence can be seen as continuation of historical power dynamics.",
      "epistemic_authority": "The text constructs the EU institutions as the primary source of legitimate knowledge about, and regulation of, AI systems. It also implicitly positions technocrats (those who understand AI systems, their risks, and their regulations) as key actors whose knowledge is considered legitimate.",
      "cultural_distinctiveness_score": 0.7,
      "dominant_cultural_logic": "Regulatory technocratic universalism"
    },
    "publicationDate": "2024-02-24",
    "jurisdiction": "EU",
    "version": "1.0",
    "pageCount": 18,
    "institutional_logics": {
      "logics": {
        "market": {
          "strength": 0.7,
          "champions": [
            "EU institutions",
            "business leaders",
            "developers and deployers of AI systems"
          ],
          "material": "The AI Act organizes the field of AI by introducing tiered compliance obligations for different types of AI systems. It also mandates financial penalties for noncompliance, pushing actors to internalize the costs of responsible AI development.",
          "discursive": "The AI Act is framed as a way to ensure the 'trustworthy and responsible use' of AI, suggesting a market logic that sees regulation as a way to promote innovation and protect consumers. It also emphasizes the need for businesses to consider the implications of this complex legislation before it comes into effect.",
          "key_tensions": [
            "Tensions with state logic over the locus of regulatory authority, and with professional logic over who should set technical standards."
          ]
        },
        "state": {
          "strength": 0.9,
          "champions": [
            "EU institutions"
          ],
          "material": "The AI Act serves as a new regulatory framework that unifies how AI is regulated across the EU single market, reflecting a strong state logic. It also has extraterritorial implications, covering all AI systems impacting people in the EU.",
          "discursive": "The text of the Act repeatedly underscores the role of 'EU institutions' in guiding AI regulation. It also asserts the EU's ambition to 'lead the way' in global AI regulation.",
          "key_tensions": [
            "Tensions with market logic over the role of regulations in AI development."
          ]
        },
        "professional": {
          "strength": 0.6,
          "champions": [
            "developers and deployers of AI systems"
          ],
          "material": "The AI Act places significant requirements on the developers and deployers of AI systems, who are expected to have effective AI governance frameworks and compliance systems in place.",
          "discursive": "The AI Act in its text recognizes the need for technical expertise in AI development, deployment, and regulation.",
          "key_tensions": [
            "Tensions with market logic regarding who bears the costs of compliance, and with state logic over the locus of regulatory authority."
          ]
        },
        "community": {
          "strength": 0.2,
          "champions": [],
          "material": "",
          "discursive": "",
          "key_tensions": []
        }
      },
      "dominant_logic": "state",
      "logic_conflicts": [
        {
          "between": "market and state",
          "site_of_conflict": "Role of regulations in the field of AI",
          "resolution_strategy": "The text attempts to reconcile this by presenting the AI Act as a necessary measure to ensure the 'trustworthy and responsible use' of AI."
        },
        {
          "between": "professional and state",
          "site_of_conflict": "Regulatory authority versus technical expertise",
          "resolution_strategy": "The text acknowledges the critical role of developers and deployers, but states that compliance with the AI Act is mandatory."
        }
      ],
      "overall_assessment": "The AI Act embodies a high degree of state logic, seeking to establish a unified regulatory framework for AI across the EU. At the same time, it recognizes the roles of market and professional logics, creating a space for businesses and technical experts in shaping the AI landscape. However, community logic is largely absent, with minimal emphasis on participation or collective well-being."
    }
  },
  {
    "id": "1763831619313",
    "title": "1-s2.0-S0048733321001165-main",
    "description": "Uploaded 11/22/2025",
    "type": "PDF",
    "addedDate": "11/22/2025",
    "status": "Active Case",
    "colorClass": "bg-purple-100",
    "iconClass": "text-purple-600",
    "extractedText": "--- Page 1 ---\n\nResearch Policy 51 (2022) 104315  Available online 21 July 2021 0048-7333/© 2021 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by- nc-nd/4.0/). A new tool for policymakers: Mapping cultural possibilities in an emerging AI entrepreneurial ecosystem  Timothy R. Hannigan, Anthony R. Briggs, Rodrigo Valadao, Marc-David L. Seidel, P. Devereaux Jennings   *  A R T I C L E   I N F O  Keywords:  Ecosystems Emergence Entrepreneurs Networks Culture Textual analysis Mapping AI  A B S T R A C T  Ecosystems are typically evaluated and understood using standard visible material metrics, such as new products, patents, startups, VC funding, jobs, and successful exits. Yet emerging entrepreneurial ecosystems (EEEs) provide many possibilities for members not signaled by such visible markers. Consequently, policymakers may have a difficult time making informed decisions about incentives and regulations to foster economic growth through ecosystem emergence. To address this methods and measurement issue, we conceptualize emerging systems using both cultural and material approaches to develop a comparative typology and apply it to an emerging regional ecosystem growing around artificial intelligence (AI). We render cultural and material maps using topic modeling of Twitter feeds versus well-placed others, identify strategies in each, and discuss relevant policies for enhancing EEEs to realize various economic opportunities. This method adds to policy analytics and suggests policies for building cultural infrastructure in EEEs.  Policymakers have a strong interest in entrepreneurial and innova -  tion ecosystems as a means for systemic wealth creation, economic well- being, and tackling grand challenges facing society (Autio et al., 2014; Feldman et al., 2019; Ferraro, Etzion   &   Gehman, 2015). But emerging entrepreneurial ecosystems, such as those in digital technology (Nam -  bisan et al., 2019) or around COVID-19 responses (Kuckertz et al., 2020), are not easily examined theoretically (Spigel, 2020), nor have they been sufficiently studied empirically (van Rijnsoever, 2020). The macro entrepreneurial context is too often treated as exogenous (Stam   &  Van de Ven, 2019), even though policy can directly shape the context, leaving many local cultural possibilities untapped. This is exasperated in early nascent periods when traditional entrepreneurial metrics are even less   informative.   Consequently,   policymakers   have   a   difficult   time making locally informed decisions to foster unique economic opportu -  nities during an ecosystem ’ s nascent period (Brown   &   Mawson, 2019; Isenberg, 2010; Lam   &   Seidel, 2020). Instead, policymakers are driven to inappropriately copy policies from other successful regions (Wurth, Stam   &   Spigel, 2021), which can create policies not well suited to the local macro entrepreneurial context. We provide a new policy analytics tool and approach that helps address this issue for policymakers, and illustrate their usefulness in an emerging AI entrepreneurial ecosystem. One reason emerging systems are difficult to study is that existing material resource and knowledge ties do not track closely with more radical innovations underpinning new systems, such as social enterprise innovations in older technological spaces (Autio et al., 2018; Thompson et al., 2018). Another is that in emerging ecosystems, the entrepreneurs ’  cultural mindsets may actually precede, not follow, patterns of tangible resource investment (Lounsbury   &   Glynn, 2019; Porac, Wilson, Paton   &  Kanfer, 1995). Furthermore, these mindsets are not reducible to local knowledge-based processes (Autio et al., 2018) or material patterns of innovation, and instead may be more dependent on cultural, field configuring events (Zilber, 2011) or subtle new specialist arrangements (Croidieu   &   Kim, 2018). Additionally, the speed of development in emerging entrepreneurial ecosystems is on a different scale than in most established ones (Markman et al., 2005; Stam, 2017). Being virtual and sometimes ephemeral makes direct observation of such systems and stable planning for them troublesome (Thompson et al., 2018). It is no surprise, then, that special issues, such as this one, and recent reviews (Acs et al., 2017; Feldman et al., 2019; O ’ Connor et al., 2018; Teece et al., 2019), call for refining our conceptualizations and measurement of emerging ecosystems to build better informed policies for nurturing innovation and entrepreneurial opportunities. Measuring and under -  standing the inter-relatedness of the macro entrepreneurial context is key   to   enabling   and   enhancing   ultimate   entrepreneurial   outcomes  Revised submission to Special Issue in Uncommon Methods and Metrics for Local Entrepreneurial Ecosystems   Research Policy . * Corresponding author.  E-mail address:   dev.jennings@ualberta.ca (P.D. Jennings).  Contents lists available at ScienceDirect  Research Policy  journal homepage: www.elsevier.com/locate/respol  https://doi.org/10.1016/j.respol.2021.104315 Received 27 November 2019; Received in revised form 22 April 2021; Accepted 15 June 2021\n\n--- Page 2 ---\n\nResearch Policy 51 (2022) 104315  2 through policy. In this paper we refine the conceptualizations and measures of emerging entrepreneurial ecosystems in a way that goes beyond the more stable and concrete   – “ material ”   or resource   –   approach by simultaneously using a cultural one. This stereoscopic cultural and material approach combines two relevant streams of theoretical work with a methodological one. We build theoretically on the new struc -  turalist approaches to culture (Lounsbury   &   Ventresca, 2002; Mohr et   al.,   2020;   Mohr   &   Duquenne,   1997),   cultural   entrepreneurship (DiMaggio, 1997; Lounsbury   &   Glynn, 2019; Navis   &   Glynn, 2010; Thompson, Purdy   &   Ventresca, 2018), and strands of the cultural approach found in entrepreneurial ecosystems theory (Autio et al., 2014; Spigel, 2015; Thomas   &   Ritala, 2021; Wurth et al., 2021). In order to provide newer measures and metrics, we apply an interpretive data science approach (Hannigan et al., 2019; Kennedy, 2008; Nelson, 2019), which   “ renders ”   theory from big data to capture dynamics and nuances in evolving systems. We highlight that emerging entrepreneurial ecosystems, like any nascent fields, depend not just on material resources but also on the evolving cultural understandings of   possibilities   among system members that contour that domain. Field-based understandings   –   that is, cultures - are reflected in field discourse (Huff, 1990; Porac et al., 1995). Discourse mapping using interpretive data science methods helps capture the emerging and evolving cultural topology of a field (Kennedy, 2008; Mohr   &   Duquenne, 1997; Thompson et al., 2018). Big textual data can help modellers track the evolving discourse in an emerging ecosystem (see Hannigan   &   Casasnovas, 2020; Kirsch et al., 2013; Powell   &   Oberg, 2017), and give a window to emerging possibilities for policymakers. Such data informed early insights can help policymakers move past having to place material bets on particular technologies that other re -  gions have had success with, and instead identify new unique high po -  tential emerging possibilities (Seidel et al., 2020). Combining these streams of research allows us to pursue a funda -  mental cultural proposition - that   cultural holes catalyze entrepreneurial possibilities in an emerging ecosystem   (Baker   &   Nelson, 2005; Lounsbury   &  Glynn, 2019; Pachucki   &   Breiger, 2010), and if properly identified, can serve as excellent emerging policy targets. As a concept, cultural holes parallels that of structural holes (Burt, 2004; 2005) in social networks, which is used to map material possibilities. To develop our cultural holes proposition for entrepreneurial ecosystems, we first conceptualize the nature   of   emerging   entrepreneurial   ecosystems   from   a   cultural perspective, relying on the aforementioned approaches in organiza -  tional sociology (e.g., Lounsbury   &   Glynn, 2019; Mohr et al., 2020) and contextual views of entrepreneurial ecosystems (e.g., Feldman et al., 2019; Spigel, 2020). We then theoretically isolate and elaborate key elements of an emerging ecosystem by developing a typology that compares ecosystem boundaries, levels of analysis, network tie types, forms of representation, notions of agency, key mechanisms, and effects on accumulation (Wurth et al., 2021). In a theoretical move similar to Spigel ’ s (2015, 2017) and Thompson et al. ’ s (2018), we compare the cultural side of such an ecosystem with its material side. Guided by this twin application of the typology, we examine an emerging regional entrepreneurial ecosystem in Artificial Intelligence (AI). We combine new forms of cultural mapping with big textual data from social media and traditional forms of resource tie mapping to identify entrepreneurial possibilities and culturally informed strategies. In doing so, we uncover early cultural markers in emergent ecosystems for both policy analysts and policymakers and develop a method of cultural mapping to add to policy analytics and help customize policy to emergent ecosystems.  1.   Conceptualizing emerging entrepreneurial ecosystems  Given their economic and policy importance, entrepreneurial eco -  systems have received increasing attention in the last decade (Autio et al., 2014; Brown et al., 2019; Feldman et al., 2019; Spigel, 2020; Wurth, Spigel   &   Stam, 2021). Related to concepts such as regional innovation networks (Cooke et al., 1997; Powell, White, Koput   &  Owen-Smith, 2005; Saxenian, 1990), innovation clusters (Bresnahan et al., 2001; Porter, 1998) and innovation ecosystems (Oh et al., 2016), the entrepreneurial ecosystem concept is distinct in considering, on the one hand, a broader set of elements in the region, and, on the other, focusing in on entrepreneurial action (Malecki, 2018). 1  A prominent example, one to which we subscribe, is Spigel ’ s view:  “ [e]ntrepreneurial ecosystems are combinations of social, political, economic, and cultural elements within a region that support the development and growth of innovative start-ups and encourage nascent entrepreneurs and other actors to take the risks of starting, funding, and otherwise assisting high-risk ventures ”   (Spigel, 2017, p. 50). As such, ecosystem policy components cannot be understood without a more complete comprehension of the wider context embedding the nascent ecosystem which helps that ecosystem arise (Autio et al., 2014). The actors   within   it,   whether   individuals,   networks   or   organizations, conceptualize possibilities and risks in that context, shaping the result -  ing pattern of economic activity. A distinct aspect of this approach is the concept of support for early or   “ prenatal ”   ventures (Clough et al., 2019). In this and other ways, entrepreneurial ecosystems differ from   “ inno -  vation ecosystems ”   (Oh et al., 2016). While   entrepreneurial   ecosystems   are   clearly   distinctive,   many conceptualizations and studies of ecosystems are still tilted towards more established, rather than nascent or emerging ecosystems. Estab -  lished systems, like Silicon Valley, may have been heavily entrepre -  neurial   in   their   earlier   stages   but   are   no   longer   as   nascent   and entrepreneurial (Audrestch, 2021; Brown et al., 2019; Feld, 2020; Isenberg, 2010; Lam   &   Seidel, 2020). In established entrepreneurial ecosystems, organizations and individual actors are likely to spend a great deal of time in standard organizational maintenance activity, not just building products or processes or creating new knowledge and innovating (Autio et al., 2018; Spigel, 2020). Established systems likely have most key elements and theoretical processes materially visible, and more easily measurable using established metrics (Heaton, Siegel   &  Teece, 2019). At this established stage the ecosystem is framed as helping entrepreneurial actors find established partners who will pro -  vide necessary resources (van Rijnsoever, 2020) and help the local re -  gions further grow (Stam, 2015; 2017). In contrast, emerging entrepreneurial ecosystems systems (EEEs) are not as likely to display material elements and properties to the same degree (Malecki, 2018). They may even lack some materially observable characteristics or processes, such as dedicated financing units or mar -  keting groups (Forbes   &   Kirsch, 2011). This means that classic material measures of entrepreneurial outputs such as start-up counts, IPOs, ac -  quisitions, or other exits may be less meaningful and evident in the emergence   stage   (Seidel   &   Greve,   2017).   Instead,   these   emerging entrepreneurial ecosystems may seem more akin to knowledge systems, with their focus on innovation and not yet on material output (Kay et al., 2018; Powell   &   Snellman, 2004). Typical ecosystem supporting infra -  structure in this mode is likely to be absent, low, or even simply a function of university institutional systems (Heaton et al., 2019; Powell  &   Oberg, 2017; Thomas   &   Ritala, 2021). This is particularly true for standard entrepreneurial financing, such as venture capital (Florida   &  Kenney, 1990; Hannah   &   Eisenhardt, 2018). Yet, policy aims to influ -  ence these emerging systems without fully understanding or observing them (Brown et al., 2019; Lam   &   Seidel, 2020; Nambisan et al., 2019). For this we need to add a more cultural approach, and data tools that enable policymakers to better identify and unleash the cultural possi -  bilities appropriately.  1   In the debate about the difference between entrepreneurial versus innova -  tion ecosystems (see Acs, 2017; O ’ Connor et al., 2018; Stam, 2015), we favor viewing the former as being innovative in intent, process and often in outcomes.  T.R. Hannigan et al.\n\n--- Page 3 ---\n\nResearch Policy 51 (2022) 104315  3 1.1.   A cultural approach to EEEs  A cultural approach is particularly useful for understanding and measuring the configurations of these emerging entrepreneurial eco -  systems where material markers are less meaningful. Early work on entrepreneurial clusters discussed differences in cultures in places like Silicon Valley versus Boston ’ s Route 128 (Saxenian, 1994), in mindsets (Huff, 1990) or how field level understandings (DiMaggio, 1997) un -  derpin actors ’   ability to see opportunities (Alvarez, Barney, McBride   &  Wuebker, 2014) and willingness to take risks (Aldrich, 1994; McMullen, Shepherd   &   Jennings,   2007).   Cognitive   mindsets   can   temper   the competitive-cooperative   dynamic,   allowing   industries   like   Scottish knitwear to thrive (Porac et al., 1995). Critically, these mindsets can develop well before any material markers of such activity surface. Broadly speaking   –   while acknowledging that any definition is likely to be debated - culture for us is the underlying pattern of beliefs, ideas, and practices in a social space (Mohr et al., 2020, p. 4). The pattern may be coherent or highly fragmented, but it is not shared by all people as a homogenous system (Martin, 1992; Mohr   &   Rawlings, 2018). The cul -  tural pattern may be evident at different levels of analysis, from the group to field to society, and expressed through or maintained by actors at each (Mohr et al., 2020). The pattern has been theorized and modelled around five different key conceptual system elements   –   values, stories, frames, categories, and toolkits (Giorgi et al., 2015). Common to these five elements and the patterns of culture they ex -  press is the meaning and meaning-making by the actors in the system (Hallett, 2003; Mohr et al., 2020). Collective meanings are produced and consumed by individual actors, the EEE ’ s groups and the ecosystem as a field (Bourdieu, 1993; Rawlings   &   Childress, 2019). This implies that cultural processes in and around EEEs are highly endogenous and agentic (Kaufman, 2004). Indeed, in entrepreneurship, compared to cultural   sociology,   culture   is   viewed   in   more   actor-focused   and actor-driven terms. As such, there has been an effort to show how stories, frames, and categories are actually deployed by entrepreneurs and related actors (Giorgi et al., 2015; Lounsbury   &   Glynn, 2019). Founders, startup teams, intermediary actors such as professionals, VCs, universities, and governments all deploy cultural devices to shape the culture of a field (Clayton et al., 2018; Jennings et al., 2013). In doing so, they ensconce deeper values in the field and shape collective meanings (Friedland, 2013), even if new meanings continue to emerge and old ones continue to evolve (Soubli ` ere   &   Gehman, 2019). At the same time, various actors, practices, and cultural elements combine to exhibit deeper patterns. Those patterns are structural ar -  rangements that shape   –   both constrain and provide affordances for   –  activities in the cultural system (Giddens, 1984). The patterns are evident if one zooms out to higher levels, such as to the level of orga -  nizational fields, or of society. At these higher levels, the patterns can be discerned in the interaction among key actors around culturally mean -  ingful objects in the field (Mohr et al., 2020).  1.1.1.   Cultural holes  When zooming out, one key cultural pattern in an entrepreneurial ecosystem that can be seen is around cultural holes. Structural holes have previously been used to conceptualize material opportunity space for entrepreneurial activity (Burt, 2005). As an analog to structural holes, cultural holes are the spaces between clusters of understandings or practices (Pachucki   &   Breiger, 2010). In the words of Pachuki   &  Breiger (2010),   “… [they are] the contingencies of meaning, practice, and discourse that enable social structure ”   (p. 213). The cultural spaces, being between such clusters, provide   new possibilities   for ideas, practices or values to emerge (Lounsbury   &   Glynn, 2019). Indeed, in this sense, key structural holes in an EEE may well just be a material subset of cultural holes, because such structural holes likely garner their meaning and worth from the cultural contingencies around existing material ties. Nevertheless, even more so than structural holes, cultural holes are likely to go initially unnoticed by field members, only gradually being discovered (Pachucki   &   Breiger, 2010; Mohr   &   Bogdanov, 2013; Powell  &   Snellman, 2004). Powerful field members may push their various understandings into parts of the system, becoming salient bulwarks in and around the discourse as it coalesces. Mohr and colleagues have traced types of idea contestation by watching how participants oper -  ating in different positions within the organizational field negotiate localized and regional topographies (maps) of meanings and resources (Mohr   &   Duquenne, 1997). In doing so, they generated new possibilities and programs for handling poverty. Goldberg (2011) and Lizardo (2014) have shown how   “ cultural omnivores ”   consume and connect disparate cultural genres, creating new underlying categories (also see Navis   &  Glynn, 2010). The cultural approach and cultural holes in particular, then, appear to offer important precursors to more materially observable entrepreneurial outcomes in EEEs, such as new venture formation. These elements can be conceptualized, measured and mapped as part of policy analytics to yield a fuller picture of the emerging possibilities.  2.   A cultural versus material ecosystem typology and measurements  Per this special issue ’ s call, how might one use this culturally grounded-approach? Put slightly differently, how might one use cultural holes to measure and map new possibilities and potential material outcomes in an EEE? Entrepreneurial ecosystem researchers, as already noted, acknowledge the importance of culture and the wider milieu   –   or  context   –   around entrepreneurial actors (Feldman et al., 2019; Stam, 2017). Many see culture as one dimension   – or important characteristic   –  of entrepreneurial ecosystems (Autio et al., 2014; Wurth, Stam   &   Spigel, 2021). Among entrepreneurial ecosystems researchers, Spigel (2015, 2020) has probably gone furthest in this direction of trying to explicitly capture the cultural and the material interaction (also see Stam 2015; 2017), and in this way bridging economic geography views of ecosystems with socio-cultural ones (Powell and Oberg, 2017; Thomas   &   Ritala, 2021; Thompson et al., 2018). Spigel has proposed that ecosystems can be understood and studied using their material, social, and cultural attri -  butes, and that these attributes can be stacked from the material down to the   cultural   as   dimensions   or   layers,   giving   each   EEE   its   unique configuration. In his 2015 article, he displayed these configurations as side-by-side pyramids for Calgary and Waterloo (Canada) entrepre -  neurial ecosystems. This unique local macro configuration is the motivating policy challenge we are attempting to address with this paper, as policies informed by and designed for a different configuration are by definition non-optimal. To delineate the power of a cultural approach, while avoiding the creation of a   “ straw person ”   on the material side, we draw on Spigel ’ s ecosystem conceptualization and notion of mapping levels or layers. Thompson et al. ’ s (2018) have a similar move in their work, if less in measurement terms. For the purpose of this article, which is focused on theorizing and capturing the cultural dimension, we will limit ourselves to elaborating and contrasting that cultural layer with the material, but allude to the social along the way and in the discussion section. In keeping with our broader cultural approach, by   “ culture ”   we mean   the   pattern   of   ideas,   values   and   practices   in   the   emerging ecosystem around entrepreneurship, which is then evident in the various faces of culture (from toolkits to values). A key structure is the set of cultural holes as forms of entrepreneurial possibilities that can materi -  alize as a set of tangible or intangible resources in the system. The ma -  terial refers, in contrast, to the set of tangible yet also somewhat intangible resources in that system (Spigel, 2015: 6). In Spigel ’ s social layer, he has   “ the resources composed of or acquired through the social networks within a region ”   (p. 6), which we include here in the material sides the network of resource ties among actors (Burt, 2005). Using this dimensionalization of cultural and material layers, we consider theoretically grounded components that are common across the dimension of layers of the ecosystem (also see O ’ Connor et al., 2018;  T.R. Hannigan et al.\n\n--- Page 4 ---\n\nResearch Policy 51 (2022) 104315  4 Stam, 2017; Stam   &   van de Ven, 2019). Given recent work on digiti -  zation and ecosystems, such as Autio et al. ’ s (2018) and Nambisan et al. ’ s (2019), the typological components must also be sensitive enough to capture virtual and digital aspects of the system, as well as their emergence. As a result, our typology for each dimension has seven components: 1) system boundaries   &   membership; 2) levels for analysis; 3) network ties   &   types; 4) forms of actor representations   &   roles; 5) the nature of agency; 6) mechanisms   &   processes; and 7) manner of accu -  mulation   &   outcomes. Table 1 shows the seven components (i.e., rows 2-8) for the cultural and material dimensions as columns 2   &   3, respectively. We display and discuss the operationalizations of these dimensions in the next section on empirics. As can be seen for the first two components, in rows 2   &   3 of Table 1, the cultural side of the emerging entrepreneurial ecosystem is broad and only partially local, certainly less so than the material side of the system, even if both are local in their focus and contain many local systems actors. Both the cultural and the material slices or   “ layers ”   (Spigel, 2020, p. 10) have sublayers; as shown in rows 4-6, the tie types, roles, and forms of agency overlap between the cultural and material conceptual -  izations, but the actors who are central and complexity of the ties differ, as do their interests. Consequently, as displayed in row 7 of Table 1, the mechanisms   and   processes   for   bridging   cultural   holes   formed   by different groups (bound by shared identity) develop in different ways and only partially map on the structural holes material mapping. The latter are around competitive and cooperative processes involved in bridging structural holes and closing (or opening up) networks. Finally, as captured in row 8 of the table, the more immediate outcomes of these mechanisms and processes are dissimilar: the cultural being focused on buzz and mindshare; the material on markers such as start-ups and IPOs.  3.   Empirics  Having theorized the cultural approach to EEEs and ways to typify the cultural dimension, our next step is to demonstrate the cultural di -  mension ’ s differences and complementarities with the material one. To do so, we examine the emerging Artificial Intelligence (AI) ecosystem in Edmonton, a large regional city in western Canada, which we argue is a modal case of regional ecosystem emergence. In the 2013-15 era, the local anchor university had several machine learning (ML) and AI or -  ganizations. These were of sufficient quality to garner some attention of global players in the AI/ML field, such as Google Deepmind and Google Brain, Apple, Microsoft and IBM (Bouslama, 2020). The ecosystem was also   recognized   outside   of   industry   outlets   by   traditional   media (Financial Post, 2017; Globe and Mail, 2017), by the provincial gov -  ernment (Globe Newsire, 2020), and in global rankings of AI   “ ecosys -  tems to watch ”   (Startup Genome, 2019). Nevertheless, many of the material trappings of larger established ecosystems, such as local ven -  ture capital, dedicated marketing groups, government development arms for AI, were lacking (Davenport, 2019). In 2015, the provincial government, in recognition of the system ’ s importance, earmarked $100 million Can. for companies over five years, half of which found its way into the system (CBC News, 2019). This smaller scale, local focus, and efforts at self-sustaining growth paths are characteristic of regional emerging systems (e.g., see Malecki, 2018; Stam, 2015; 2017).  3.1.   Rendering the Ecosystem  To form the cultural and material representations, respectively, of the   emerging   entrepreneurial   ecosystem,   we   used   the   three-stage rendering method   elaborated by Hannigan et al. (2019). The process of rendering refers to compiling a corpus, using mixed methods analysis on the corpus, and theorizing artifacts (constructs, linkages, processes) from those analyses   –   all in iterative and transparent fashion. Rendering is particularly effective with big textual data such as that from social media,   which   require   substantial   wrangling   to   build   a   corpora, open-ended analyses and more abductive theorization (Schmiedel et al.,  Table 1  Cultural and material characteristics and measures of emerging entrepreneurial ecosystems.  Characteristics   Cultural   Material  Systems boundaries   &  membership  Virtual boundaries around coherent discourse spaces, with members define by identities, new norms or conventions, and meaningful objects  •   Using leading edge databases (Crunchbase) on local startups, social media members (Twitter accounts) who discuss ecosystem group of the city   +   others. Often via public data. Geo-physical boundaries around emerging clusters with new, concentrated resource flows indicating boundaries. Membership formally defined by local schemes (industrial, govt., alliance ties).  •   Counting new AI starts within Edmonton city boundary, focus on concentrated entrepreneurial activity and funding (e.g., around key firms. Often via private data.  System levels   The virtual community, cliques within it, and its linkage to specific entrepreneurs and their startups.  •   The community constituted by the active, higher attention (#followed city AI orgs), specific handles and specific AI orgs in sub-communities. The regional ecosystem nodes and flows, and the dyads of exchange within it.  •   All the organizations tied to AI startups and indicating whether there is some form of exchange among each (dyad level).  Network ties   Discourse-based ties linking actors via commentary, especially about meaningful events.  •   Twitter discourse about startups and founders. Resource (money, materials, knowledge) ties; first- and second-order ones.  •   Recognized relationships and material support for startups.  Forms of representation   &  roles  Identity-based associations of diverse actors in discursive space (promoters, players) or network roles (bridgers, intermediaries).  •   Types of virtual actors, active/ passive accounts, assigned cultural roles and identities. Central actors, brokers, peripheral players.  •   Key local players and sources of support (corporate, not-for-profit, govt.) in the system.  Agency   Agency is distributed and varies by ecosystem; cultural entrepreneurs agentically employ toolkits.  •   Evidence of toolkit use, some sense of distribution of agency using key cultural activit(ies). Organizational interest reflected in positioning in the ecosystem; entrepreneurs are founders   &   key actors are startups.  •   Evidence that a founder or central actor is active in this way.  Dynamics   &  mechanisms  Building cultural understanding discursively, particularly by making sense of events and relationships. Cultural spaces appear for new meaning and practice possibilities.  •   Mapping local discursive ties, communities of understanding, and cultural holes between them. Balancing cooperation and competition using structural reconfigurations, esp. brokering structural holes or complementarities.  •   Mapping ties, locating complementarities   &   bottlenecks, then examining key actors and their behavior around resources.  Manner of accumulation   &  outcomes  Expansion of discursive community, intensification of   “ buzz ”   (meaning) in it   &   around potential new players   &   products.  •   Examining growth in Twitter handles in system and buzz about AI firms and innovations, along with other local social media and event data. Growth of the overall system, degree of innovation, diversity in membership, and number of   “ exit events. ”  •   Same measures gathered via event participation and media data.  T.R. Hannigan et al.\n\n--- Page 5 ---\n\nResearch Policy 51 (2022) 104315  5 2019). We build on this approach by showing, at each of the three rendering stages, the cultural versus material elements of the Edmonton EEE.  3.1.1.   Rendering the ecosystem ’ s cultural and material corpora  The cultural dimension of the ecosystem, as shown in Table 1 (col -  umn 2; row 2   &   3 2 ), is captured by examining virtual boundaries around coherent discourse spaces, with members defined by identities, new norms or conventions, and meaningful objects (Mohr et al., 2020). In this case, the meaningful objects are   “ AI ”   and   “ ML ”   (Nambisan et al., 2020). The cultural boundary is still anchored to a physical boundary (Table 1, row 3), which is the place with which actors pursing AI/ML identify (Mohr   &   Duquenne, 1997; Thompson et al., 2018). The   cultural   approach   to   measuring   and   mapping   this   locally anchored entrepreneurial ecosystem relies on some of the same key actors as a material (or social) ecosystem dimension (Table 1, rows 4   &  5)   –   i.e., investors, intermediaries, the government, universities, startups -   but   uses   different   information.   The   cultural   corpus   is   based   on discourse (communication via talk, text and visuals) used by and focused around AI/ML new ventures (Navis   &   Glynn, 2010; Thomas   &   Ritala, 2021; Thompson et al., 2018). These new ventures include startups or formal initiatives (e.g., spin-ins or special new units) launched by incumbent organizations. In measurement terms, this means it would include key emerging entrepreneurial actors and others in and around the local ecosystem who communicate with or about one another (Table 1, column 2; row 4-6). The outcome of this interaction is the amount of   “ buzz ”   about new possibilities from these actors (rows 7   &   8). Here we build our corpora using discourse in the form of tweets among ecosystem actors, although we acknowledge that other forms of communication (emails, newspapers, private conversations, meetings) are also valuable sources of information. Twitter has been used in other studies of cultural dynamics in fields (Shi et al., 2014; Zhao et al., 2011). Twitter is spontaneous, real time, and mostly public (i.e., there are some private facing features like direct messaging, but the platform is mostly used for public communication). While somewhat performative in na -  ture (Thomas   &   Ritala, 2021), Twitter also enables entrepreneurial ac -  tors to find each other and evolving opportunities (Fischer   &   Reuber, 2011; van Rijnsoever, 2020). For example, in the fall of 2020, several prominent Silicon Valley based venture capitalists and entrepreneurs openly speculated about moving to Miami. The mayor of Miami, Francis Suarez directly responded on Twitter to this speculation, saying   “ How can I help? ”   (Suarez, 2020). The New York Times recently documented Suarez ’   efforts on Twitter suggesting that   “ the glass door to his office has his mayoral seal and his Twitter handle ”   (Bowles, 2021). Tweets in the local Edmonton AI EEE appear to operate similarly. Three linked tweets among important actors are displayed below in Fig. 1. Two of these tweets   –   those from well-known AltaML CEO, Cory Janssen, and from Innovate Edmonton lead, Cheryll Watson   –   raise the issue of AI-related applications to health in Edmonton. A third tweet comes from the Provincial Minister of Jobs, Economy, and Innovation, Doug Schweitzer, in which he attempts to start an offline conversation with   entrepreneurs   in   the   ecosystem   regarding   economic   recovery relating to   “ traditional and emerging sectors ” . These three tweets are clearly intended as public facing messaging; they connect politicians, system entrepreneurs, and local initiatives. Public messaging by Tweet enables quick sharing of information and a mindset with the central actors, diffusion of the understanding to others. In constructing our corpora and analysis of topics, we examined both  who   said something and   what   was said by those actors in and around the ecosystem (see Table 1: column 2, rows 3-4). In our case, to capture   “ the  who ” , we first used Crunchbase, augmented by local media mentions, to identify the 40 AI ecosystem new ventures, as of 2019, in the local ecosystem. Crunchbase has been used by other researchers studying ecosystems and entrepreneurial networks (Ter Wal et al., 2016). Of these, 29 were fully operational by the time of this study and had an active account on Twitter. Scraping of all followers on Twitter (via Twint in Python and using the official Twitter Developer API) yielded a total of 33,200 Twitter accounts following any of the 29 AI new ventures. These ventures and their tweets are displayed in Appendix Table A1. An important rendering move to manage this corpus was then to select   all Twitter handles that were following four or more (4 + ) AI start-ups  in the system,   plus the 29 AI   new ventures handles that were sending out virtual ties. We made this decision by inspecting the maps of 2 + , 4 + , and 6 +   sent and/or received (all directional) links to followers to set up corpus selection criteria in order to identify more coherent networks and bundles of tweets flowing through their ties (Borgatti et al., 2018; Owen-Smith   &   Powell, 2004). This yielded 135 nodes, the most active sending and receiving ones being listed in Appendix Table A1. In the case of   “ the   what ”   (Table 1, rows 6   &   7), we examined up to the most recent tweets (up to 200) by the 135 actors (i.e., the nodes in the cultural network). These nodes ’   handles had tweeted, in total, 23,532 times in the past two years about the ecosystem. We used these tweets to form the documents for our study ’ s corpus (see Appendix Table A1). To render this corpus, we preprocessed the documents by lemmatizing the texts (Schmiedel et al., 2019). We lemmatized the words in these Tweets using the Stanford CoreNLP toolkit (v. 3.9.2 in Manning et al., 2014; see Goldenstein   &   Poschmann, 2019 for an application in social science) to create tokens for analysis. Of the 218,025 words 30,341 were unique tokens. These lemmatized tweets with unique tokens included AI/ML specific references, such as:   “ @deepmindai neurips18 head deepmind recruitment stand meet edmonton team group focus fun ”   and   “ This is really cool. Awesome to see #AI #MachineLearning in traditional in -  dustries in #yeg and Alberta. Cool project. ”   They also included entre -  preneurship related Tweets, e.g.,   “ Among the dreamers and the doers at @INVENTUREScan   #yeg   #researchinnovation   #Entrepreneurship. ”  Other tweets in the corpus were about lifestyle and other municipal area topics. The population of all tweets in this time period about the AI system formed the corpora to topic model   “ the what ”   in the model rendering stage.  The Material Dimension ’ s Corpus.   The material ecosystem, as shown in Table 1 (column 3; rows 2   &   3), is captured by examining members in  Fig. 1.   Sample tweets involving important Edmonton AI ecosystem actors.  2   If one counts the characteristics ’   column and dimensions ’   row, per the prior discussion of dimensionalization.  T.R. Hannigan et al.\n\n--- Page 6 ---\n\nResearch Policy 51 (2022) 104315  6 geo-physical boundaries around emerging clusters with new, concen -  trated resource flows indicating boundaries. The boundary for the ma -  terial map is based on government determined municipalities. The area includes surrounding suburbs, satellite towns, and the core city. Like membership in most material maps, members in the ecosystem can be defined by relevant tangible and intangible resource activities around identifiable actors in the local main city ’ s ecosystem (Table 1, rows 3-5). We encoded these members and their ties in the regional ecosystem via   reports   by   well-placed   others   (Borgatti   et   al.,   2018).   Each well-placed other was requested to discuss key actors and the others in the system with whom they were tied (Borgatti et al., 2018; Marsden, 2005). We then asked them to build upon these insights and draw a map that contained the key actors and their tangible and intangible resource ties (Mehra et al., 2001; Schiffer   &   Hauck, 2010). The respondents included a director of a local service organization for entrepreneurship in the regional ecosystem, the head of a health-based AI firm, a gov -  ernment service lead, and a prominent local investor in the ecosystem. Consistent with Schiffer   &   Hauck (2010), we then built a composite material map from these four maps. The composite contained all nodes and ties mentioned (the union of nodes), and had the most important (well-linked and critical resource) actors positioned more centrally. To systematize that map, we read the node list of bi-directional ties as an . xls symmetric matrix of ties among actors into UCINET (Ver. 6.7) and generated a map display (see Fig. 2). Most nodes, at the request of re -  spondents and coordinating author, were anonymized by coding them into broader categories, such as   “ InvestCo ”   and   “ FundCo ” . A select number of nodes were, with permission, revealed to match up the key organizations from the material map with those from the cultural one. Somewhat centrally are TechCo 4, PostSec 1, Invest 5 [Valhalla Angels], Service 3 [Startup Edmonton], and Gov2. Less central are In -  cumbents 4   &   11, TechCo 3, BigTech 1, and Invest 2. In between these two groups are several moderately linked and clustered actors, such as those around AICo 8 [AltaML], Service 3 [Startup Edmonton], and FundOrg 2. The map itself looks moderately dense and seems to have a diversity of organizations (AI startups, funders, government agencies and service providers, university units). Importantly, the organizations recognized for doing AI or ML research and application themselves vary, from broader tech companies (such as TechCo4), to AI-specific firms (the respected AICo8 [AltaML]), to AI-NGO type research-oriented units (NGO 2 [Amii], a prominent player). On the whole, the map contains organizations and patterns that look similar to those found in other EEE maps (Autio, 2017; Heaton et al., 2019; Spigel, 2020).  3.1.2.   Rendering the ecosystem ’ s cultural   &   material (structural) holes  Rendering topics and models   –   in particular, cultural and structural holes   –   from the corpora was our next major task (Hannigan et al., 2019). In the case of the cultural holes map and analysis, this required three steps. The first was to parse down and organize the set of 135 nodes of   “ who was linked to whom ”   in network terms (i.e., those in Ap -  pendix Table A1). The second was to find   “ the what ”   was being said as sets of topics and themes. The third was to link these parsed down and organized network members and topics together into a cultural holes map. As the first step, to capture the core group of actors shaping the local   cultural   discourse,   we   tried   to   identify   the   most   stable bi-directional (two-way) ties among actors, which, in network terms indicates reciprocity and mutual engagement (Borgatti et al., 2018;  Fig. 2.   Composite material map of Edmonton AI ecosystem.  T.R. Hannigan et al.\n\n--- Page 7 ---\n\nResearch Policy 51 (2022) 104315  7 Tasselli   &   Kilduff, 2020). Using a rendering move similar to that for creating the 4 +   networks in corpora stage, we examined 3 + , 4 + , and 5 +  tweet following networks, and created bi-directional network graphs for each, and, upon visual inspection settled on the 4 +   bi-directional as the most coherent and balanced network underlying the tweets in the  “ what ”   corpus. That 4 +   map is displayed in Appendix Fig. A1, rendered via Gephi (Ver. 9.2) with betweenness centrality and modularity of Q = .58 (Wang et al., 2017). To then understand   what   was being said about entrepreneurial pos -  sibilities by this core set of cultural actors, as our second step we applied topic modeling analysis (Mohr   &   Bogdanov, 2013). Topic modeling using Latent Dirichlet Allocation (LDA) is becoming the standard go-to analysis in management of big textual data (Hannigan et al., 2019). However, when each document (i.e., a tweet, in our current case) has a small number of words, LDA is not optimal. The distribution of topics across the documents was too sparse, making estimation of the LDA parameters for a document particularly difficult. Recently, analysts have advocated rendering topics using bi-term (concurrence of two tokens) analysis, otherwise known as   “ BTM ”   (Jonsson   &   Stolee, 2019; Yan, Liu, Chen   &   Tong, 2013). Like LDA, it uses a Bayesian iterative approxima -  tion of the distribution of words across documents, but considers the whole collection of documents, not each individual one, as its starting point. We used the R coding of BTM (Wijffels, 2019) to generate topics across the corpus of tweets. To find the optimal set of topics, given a large or small number can be generated, consistent with LDA best practice, we plotted the loglikelihoods of each BTM model, in 5 topic steps (from 5, 10, 15, up to 70 topics), to find this downward inflection point in the LL. In this rich cultural space, there were 65 topics. We still needed to generate some sense of meaning for those topics and relate the topics to one another. To do so, we needed to rely on both machine and human interpretation. Following Hannigan et al. (2019), three well-placed, co-author observers in the ecosystem axial coded the topics using three items: 1) the top twenty most probable words per topic; 2) the matrix of topics weighted by cleaned tweets about the topic, and 3) the LDAvis tool (Sievert   &   Shirley, 2014) applied to the topics and outputs produced the BTM package 3 . Each observer independently coded topic meanings, compared and the adjusted codes and generated a list of first-order topics. They then examined the agreed upon meanings and the LDAvis of all 65 topics to come up with relative groupings and second-order codes for those topics. Toggling relevance scores of words in the LDAvis helped ferret out this meaning. The details on these first-order axial 65 coded topics and the four second-order conversation spaces are available upon request. As our third step, we combined   “ the who ”   4 +   bi-directional network and   “ the what ”   65 topics into a novel artifact:   a cultural map of discourse  (see Fig. 3 below). We normalized the matrix of tweets by collapsing the distribution of topics across the 23,532 documents (the tweets) that were being jointly followed by the 135 handles (nodes). We then generated a cluster map of topics proximities based on the underlying network of bi-directional tweeters (see Fig. 3). The sets of 65 topics cluster into the four areas: AI/ML SciTech, AI to Business, AI Entrepre -  neurship, and Local Lifestyle   &   Community Issues. Each of these four areas appears to have somewhat coherent topics and active tweeting within them; that is, they may reveal subcultures of discourse in the cultural (Audretsch et al., 2021; Mohr et al., 2020). Among those four groups and other subgroups of clustered topics are spaces; that is cul -  tural holes where new entrepreneurial possibilities may exist (Louns -  bury   &   Glynn, 2019; Pachuki   &   Breiger, 2010). In Fig. 3, using circles in purple, we have identified a few of these cultural holes. We discuss the theoretical   importance   of   the   map   and   the   cultural   holes   in   the rendering theory section.  The Material Dimension . Parallel to our model rendering step for cultural holes, we rendered structural holes with a material map of entrepreneurial actor ties. In keeping with Burt ’ s well-known work on structural holes (Burt, 2004; 2005), we used our material map ’ s sym -  metric matrix in UCINET (Ver. 6.7) to generate centrality, constraint and  Fig. 3.   The cultural map of topic clusters and holes.  3   One of the co-authors developed software for this project to extend the BTM R package for the LDAvis functionality.  T.R. Hannigan et al.\n\n--- Page 8 ---\n\nResearch Policy 51 (2022) 104315  8 autonomy measures for actors in the map. A sample of these measures is displayed in Appendix Table A2. The highest centrality score in terms of sheer tie count (10) and betweenness (1228) is TechCo 4, followed by main city unit (10 and 730, respectively), and the post-secondary units 1  &   8 (7, 9; 726, 738). The AI specific firms, such as the well-regarded (AICo-08, AltaML) have lower degree and moderate betweenness scores. Next, we generated a more focused, distillate material map of the resource power and centrality of key actors in the network (Burt, 2004). To do so, we converted the betweenness centrality of actors to be rep -  resented as node size (Borgatti et al., 2018; Wang et al., 2017). Applying betweenness modularity (again at Q = .58) to parse down the number of ties and determine main cliques, as we did with the cultural map, reveals seven clusters of organizations (Fig. 4). Each is indicated by a color: black, blue, red, orange, green, pink and grey. Finally, using the scores per actor regarding centrality and number of structural holes (Appendix Table A2), along with the all-important vi -  sual inspection of the distillate material map in Fig. 4, we identified spaces that represented large or important structural holes. The most important structural holes are those spaces between the dense cliques and are signalled by the constraint scores (reversed by 1-constraint), per Burt (2004, 2005). We display three large structural holes in the figure, with a smaller one for contrast. A large structural hole appears between TechCo 4 (a tech med startup) and PostSec 8 (a university mentoring unit linking alumni to students). Currently, that hole is primarily bridged by PostSec 1. However, PostSec 1 is not a funder or a specialist group per se, whereas PostSec 8 is a specialist group, suggesting op -  portunities exist for less constrained actors who have relevant funding or knowledge skills to enter this space and potentially to become   “ brokers ”  between actors in different cliques (Burt, 2005). One of these potential brokers is an external very large tech firm, BigTech 3, (effsz = 4.6, cnstr  = .30), another is a bank, Incumbent 7 (ATB Financial ’ s new AI venture), (effsz = 3.0, cnstr = .33). Yet whether these spaces will be noticed and bridged, as we have argued above, depends on how the cultural map and its holes (possibilities) align with the material map and its holes.  3.1.3.   Rendering relevant entrepreneurship and policy theory  In the corpus and model rendering steps we crafted EEE maps of cultural and structural holes. Based on these, we were then able to interrogate the maps theoretically to generate insights and additional theoretical artifacts (Hannigan et al., 2019). In Fig. 3 ’ s cultural map, as mentioned above, we see four conversation areas -   AI/digital, tech entrepreneurship, local/lifestyle,   and   traditional industry . Theoretically  Fig. 4.   Material map of weighted nodes, group tie types, and a few structural holes.  T.R. Hannigan et al.\n\n--- Page 9 ---\n\nResearch Policy 51 (2022) 104315  9 they seem to represent different cultural conversation clusters, and perhaps indicate cultural subgroups or subculture in the AI industry (Audretsch et al, 2019; Mohr et al., 2020). Based on the direction of the tweets and the nature of the tweets, four areas appear to pull in different directions. On the one hand, this means that within subcultures topics are pulling in a similar direction; e.g., the core conversation of AI/ML research groups and applied engineering firms in the AI Digital space appear to have some discourse and likelihood of filling nearby cultural holes. On the other hand, this outward pull of the four groups leaves a sparser center in the map –   some form of low gravity well in the EEE. One might argue that this cultural hole is significant enough to represent some institutional or logics void (Thornton, Ocaio   &   Lounsbury, 2012). There is insufficient dialogue among the poles to fill this space with joint understanding (Lounsbury   &   Glynn, 2019). There may even be some degree of polarization. This may imply that these larger cultural holes and places of polarity are early traces of market failures (Heaton et al., 2019), which policymakers should evaluate and potentially redress with pre-emptive action. A second theory-inducing observation about the cultural map is around the arc of topics or conversations in the map. There is a promi -  nent arc (in green) on the map ’ s left-hand side representing engineering AI/ML clustered conversations. That cluster is only loosely touching on the large nebula of conversation about technology entrepreneurship at the map ’ s top, many tweets of which are generated by service providers. A great deal of policy work in EEEs is around connecting technology applications and knowledge to entrepreneurial opportunities and ser -  vice provision (Heaton Siegel   &   Teece, 2019; Nambisan, Thomas   &  Wright, 2018; Stam, 2017). The cultural map clearly captures these two important clusters and their evolving relationship   in the two-year snapshot. It suggests that more work needs to be done to connect these two important sets of discourse   –   another potential policy target. Turning to theory that might emerge from the material map in Fig. 4, interestingly, the three large structural holes displayed show evidence of big voids in the map between several cliques, at least at that map ’ s level of granularity. But there is no evidence of one major or central void that defines a central axis of potential polarization. Observations of the material map, therefore, might lead policymakers to construct generic connecting policies, like joint conferencing or funding pools, without recognizing   the   polarities   across   the   subcultures   and   misaligned resource cliques might lead to failures. Policymakers might also focus on particular structural holes and fund resource players based on their proximities, but without understanding the cultural tensions among proximate actors. As a result, event equal funding might be viewed as unequal because some critical segment of actors view their virtual po -  sition as being more important (Audretsch, 2021). Looking at the specific actors in the material map, we also see that the most central players are not necessarily the tech firms, but univer -  sities and service providers, a point that researchers have made (Heaton et al., 2019; Grimaldi et al., 2020). These actors provide human capital and maintain many social ties with tech firms, but the university and service firms do not often have enough fungible resources to help linked cliques in more substantial ways. On that point, in the material map we see some very long linked areas, such as to the lower left and right. Granted, this is partly a function of arbitrary map rotation, but the network indicates that there are disconnected AI/ML players in resource terms, yet these same actors appear in Fig. 3 to be more connected in  Fig. 5.   Discursive signatures.  T.R. Hannigan et al.\n\n--- Page 10 ---\n\nResearch Policy 51 (2022) 104315  10 Fig. 6.   Two examples of current regional ecosystem maps.  T.R. Hannigan et al.\n\n--- Page 11 ---\n\nResearch Policy 51 (2022) 104315  11 discursive   (cultural conversation) ones.   This suggests policymakers should consider how these different actors might be linked to new re -  sources via conversations.  3.1.4.   Discursive strategies  To capture how firms in the cultural map might actually not just locate and signal possibilities   –   but exploit them by bridging   –   we generated a new theoretical artifact:   spectrograms of discursive strategy bridging . To do so, we arranged the topics that actors tapped via tweets according to the major conversation clusters (color grouping) in Fig. 3 and recorded the frequency of that topic tweeting by the actor. Fig. 5 shows the sets of topics on the x-axis of the spectrograms from Fig. 3 ’ s the upper left-hand conversation spaces (AI digital   &   tech entrepre -  neurship) down to those in the lower right-hand (traditional industry   &  lifestyle). Each spectrogram represents a linear order of topics that we rendered in order to focus attention on clusters of topics and cultural holes. The clusters are highlighted with colors to help distinguish holes between them. In Fig. 5, we focused on displaying actors who seemed to be in positions to exploit cultural holes and tried to identify the key patterns of how they did so. Some actors ’   discursive strategies have a singular ( narrow ) tweeting focus. The top left in this figure shows ATB Financial (a provincial bank) with topic weights predominantly in Local Lifestyle   &   Community Conversations. Another example of this strategy is situated on the top left-hand side of Fig. 5. One organization   –   Valhalla Angels   –   has a predominate focus on the left-side of the spectrum, representing AI digital and tech entrepreneurship. The narrowness of the foci for the actors is also manifest in the lack of topic clusters that they seem to bridge. Valhalla, while able to bridge between tech entrepreneurship topics and ML/AI science ones via Topic 4, does not actually have much discourse (frequency of tweeting) in that linking space. So, in cultural, mapping and strategic terms, we can label this localized form of cultural conversation as a   “ narrow discursive strategy. ”  Other actors seem to pursue discursive strategies that are more clearly about bridging cultural holes. One signature is specifically about cases where organizations are explicitly attempting to fill cultural holes that are   immediately   adjacent   to them.   AltaML and   Amii   – unlike Valhalla   –   clearly both tweet in the bridging Topic 4 space, as shown by the frequency of tweets in that spot and the vertical rectangle used to highlight the link placed on the x-axis of the spectrogram. We labeled such signature types as a   “ near bridging discursive strategy. ”   In contrast, some firms tweet into a very different space from their own, trying to draw together two (or more) disparate discourses clusters. Two notable examples are Testfire Labs and Granify. Via topic 11 (UX-Design Teams) in Fig. 3, each has the opportunity to bridge between the AI to Business clusters of topics in green and the Local/Lifestyle topics in orange. We labelled this a   “ distant bridging discursive strategy ” .  This distinction of near and distant bridging cultural holes reflects an important   property   of   possibility   space   –   that   cultural   structures constituted by discourse are more or less distant from one another (Goldberg, 2011; Mohr et al., 2020). In related strategy research, for some years, related diversification (Markides   &   Williamson, 1994) and proximate spatial dispersion (Kim et al., 2015) have been lauded. In knowledge-based approaches to firms, drawing on notions of evolu -  tionary biology, Felin et al. (2014) have developed the concept of the  “ adjacent   possible ”   as   a   way   of   understanding   the   emergence   of  Fig. 7.   A two-layer, cultural-material map of Edmonton ’ s emerging AI entrepreneurial ecosystem.  T.R. Hannigan et al.\n\n--- Page 12 ---\n\nResearch Policy 51 (2022) 104315  12 economic opportunities in proximity terms. Proximity in this cultural map and bridging strategies may make it more likely that bridging strategies will be effective in the cultural map.  3.1.5.   Discursive strategy ’ s material outcomes?  A critical question, of course, is whether these three generic cultural (discursive) strategies will have systematic material effects? While this question is beyond the scope of an article devoted to conceptualizing and developing metrics for a novel mapping measure, we must discuss ways for systematic research with the construct to address the issue. If we re-examine the centrality-based material-clique map in Fig. 4, we see in blue-grey transparent bubbles with their actual names in the material map the six firms whose discursive strategies we just theorized about with regard to Fig. 5. Of the organizations actively using discursive strategies, at the end of our data collection in 2019, two were well- positioned in the material map around various structural holes: Amii (36 holes, low constraint), and AltaML (26 holes, low constraint). Two were   moderately   well-positioned:   Valhalla   Angels   (10   holes,   low constraint), ATB Financial (6 holes, low constraint). And two were not well-positioned   materially on   the   material   map:   Testfire   Labs and Granify.. As a follow up on this pattern, in early 2020, before covid, we found that AltaML and Amii were still doing well. Since covid, AltaML has been even more ascendant (Globe Newswire, 2020) and Amii has been granted additional funding from the provincial government (Joannou, 2020). In contrast, ATB Financial retreated some from the AI space, and Valhalla seems to have shifted efforts to British Columbia. Granify has survived, but repositioned culturally and materially, and Testfire is no longer actively tweeting. There appears, then, to be a simple correlation between firms employing near bridging discursive strategies and pres -  ence around structural holes (Amii and AltaML). In contrast, a distant bridging strategy either is associated with low material presence, or not even remaining on our material map. More detailed study, of course, would need to be done to systematically link these cultural discursive strategies to exploitation of these structural and cultural holes.  4.   Policy implications   &   research contributions  Our cultural approach to EEEs has shown value in considering it as a distinct dimension in emerging ecosystems. We see this as one way to conceptualize and measure the characteristics of the cultural dimension, and as a novel mapping technique for identifying cultural holes and discursive bridging strategies. In the remainder of this article, we consider the implications of our measurement and mapping approach for policy analytics and for policymakers.  4.1.   Implications for policy analytics  Policy analytics, as an area of data analytics, is becoming more prominent in both research and practice (De Marchi et al., 2016; Nam -  bisan et al., 2019). Governments have always created and relied on data for decision making; but the volume and velocity of data   –   big data   –  combined with the human-machine algorithms from computer science have created an array of new tools for policy analysts to apply and policymakers to deploy (Pencheva et al., 2020). Here we have focused on cultural mapping as one such tool. In the current approaches to emergent entrepreneurial ecosystems, it is common to see mapping done of ecosystem actors as a means of categorizing and organizing different spaces. Fig. 6 displays two recent maps of regional ecosystems in Can -  ada, one of the AI entrepreneurial ecosystem in the Waterloo area, a respected hub of high tech (e.g., Spigel   &   Vinodrai, 2020; Bramwell   &  Wolfe, 2008), and the other for the wider innovation ecosystem (not just the AI entrepreneurial one) in Edmonton, Alberta the region examined here. Policy analysts generated each map based on their relevant local knowledge; i.e., in theoretical terms, they acted as well-placed others in local context (Autio et al., 2014). The analysts ’   maps were designed to capture the main actors and relationship in the ecosystems. The maps foreground material (resource) ties and considerations, backgrounding cultural   understanding   of   these   relationships.   Looking   at   the   AI ecosystem map of Waterloo, we see topic clusters, such as   “ Machine Learning ”   and   “ Data Science and Analytics. ”   We also see key research labs boxed below the brain-shaped map. These clusters, their labels, and their proximities are arranged based on the contextualized under -  standing of the system, partly based on iterative discussions with local others about the best arrangements to depict the system. Similar to the Waterloo map, the Edmonton innovation ecosystem map is quite clear and captures key hubs of activity, with some agreed-upon labels. It goes a step further and displays the links within and across hubs, suggesting innovation opportunity spaces, the main one being in the center of the map. Both maps displayed in Fig. 6 have a winning expressiveness and are good talking points; but they would be difficult to use them systemati -  cally as policy analytic tools. The proximities may or may not mean much; it is difficult to know ex ante whether being close to one cluster or another in the Waterloo map matters, or whether being remotely con -  nected to actors in the Edmonton map is a problem. In addition, the new opportunity spaces (possibilities) are not obviously identified in either map. Hence, better and worse possibilities are difficult to locate in each figure. For policymakers, these drawbacks have implications on where and how to distribute funds   –   and to whom specifically. In the AI ecosystem map, the hubs below the map seem critical, but policymakers lack map metrics to prioritize them and to spread funds to nearby organizations. In contrast, we have argued that a good policy analytic tool would be to rely on Fig.s 3 and 4 in some combination. One possibility, similar to work in ecosystems by Spigel (2015) and Stam (2015), Wurth et al. (2021) is to layer the maps (also see Mohr et al., 2020). The key firms and possibilities discussed in each map layer can be linked across levels via   “ pipes. ”   These are forms of vertical or modal network ties (Padgett   &  Ansell, 1993; Tasselli   &   Kilduff, 2020). For illustrative purposes, we have displayed just three firms already discussed above: Testfire, ATB AI Financial, and AltaML. By observing the overall congruence of clusters and cliques across the two maps, and then carefully examining the positioning and discursive strategies of specific firms (or of similar types), policy analysts should be able to isolate better and worse spaces for attention and funding, and actors who might be encouraged (steered) towards or away from these areas. For example, AltaML has a discursive bridging position between the AI Digital and Tech Entrepreneurship spaces and in the material map, high centrality and a position besides several structural holes. It would appear to be an actor worth some culturally and materially informed policy attention, as would the joint spaces beside it in the two-layers. In contrast, in spite of Testfire ’ s location in the Tech Entrepreneurship cluster and high tweeting pro -  pensity, its far bridging strategy and incongruent material map location at the far edge of a distant clique may make its layered locations less auspicious. 4  4.1.1.   Policies to enable mapping analytics  In order to use such multilayered possibility maps, policy would be required for accessing, assembling and analyzing big cultural data along with more standard resource map data. As a precursor to such policies, it is important for policy analysts and policymakers first to embrace some of this cultural approach and combine it with the material or resource- focused one. That stereoscopic view of EEEs would not only allow for a deeper understanding of the ecosystems and their co-evolutionary dynamics (Johnson et al., 2019 ;   Thompson et al., 2018), but also lead policymakers to prioritize collecting such data. As we have seen, cultural  4   Of course, additional on site, contemporary due diligence would be needed to confirm these policy analytic points.  T.R. Hannigan et al.\n\n--- Page 13 ---\n\nResearch Policy 51 (2022) 104315  13 markers are carried in social conversations, such as those found in Twitter and other social media data (Obschonka et al., 2020). These tweets and are, in Nambisan et al. ’ s words (2019),   “ digital affordances ” –   virtual world enablers of various system facets. Some social media data are in the public domain or for purchase, but with the marketization of such data, it is not clear that will be true for long. Over the course of writing this paper, we shifted from collecting data manually using Py -  thon tools to working directly with Twitter through their academic developer program. The ease by which data was collected dramatically shifted as a result. Looking forward, it is conceivable that the company will continue to find ways to make this data more accessible for policy audiences. However, provisions for collecting and accessing the data as part of the package of tech company regulations would be useful if the public and policymakers are to benefit from these data as much as private business has to this point. Policy analysts need help from funders and policy -  makers to secure some of the corpora in order to use them for the business ecosystem and public ’ s behalf. Other open data efforts target -  ing such corpora will help as well (Perkmann   &   Schildt, 2015). Policy enabling local forms of federated learning (Bonawitz et al., 2019) may even enable such cultural mapping of more private communications within an ecosystem while preserving individual privacy. The ability to use such data in analyses and in policy formation is equally important to getting access to and assembling those data in corpora. In particular, the ability to generate and work with the various measures and metrics captured in Table 1 is important. Here, we employed a contemporary suite of computational social science tools (e. g., see Edelmann et al., 2020; Goldenstein   &   Poschmann, 2019), but took strides to recognize the localized knowledge of the author team in determining reasonable methodological decisions such as transparency and replicability (Nelson, 2019). To effectively render an EEE using our toolkit and approach requires, then, both well-placed quasi-ethno -  graphic abilities   and   computational tools on the part of policy analysts. Training in these tools to capture the stereoscopic nature of EEEs seems to be important in the world of emerging policy analytics.  4.2.   Implications for EEE policy  Policymakers guide regional and global economic opportunities. In recent years, there have been critiques of entrepreneurial ecosystem policy, highlighting conceptual and measurement issues (Autio, 2017; Spigel, 2020) as well as applicability challenges (Brown et al., 2017; Isenberg, 2010). Past policy analysis and thought about ecosystems appears to have been built upon some flawed assumptions. In the area of EEE policy, there are well-known success models: the Rainforest Model (Hwang   &   Horowitt, 2012) and similar depictions of the Silicon Valley (Isenberg, 2010). But not every region can be   –   or wants to be   –   a Silicon Valley (Audretsch, 2021; Feld, 2020; Isenberg, 2010). The bias towards this success model has been intensified by only measuring the material markers of success established in systems such as Silicon Valley with its hypergrowth exit mindset (Brown   &   Mawson, 2019; Lam   &   Seidel, 2020). Policymakers can address this past oversight and unlock numerous previously untapped possibilities hidden in previously unobserved cul -  tural holes. Ecosystems can flourish in unique ways with differing sets of cultural assumptions, such as those found in northern Italy (Tracey et al., 2018), Singapore (The Economist, 2014), the UK (Brown et al., 2017) or Finland (Autio, 2017). These locales have each benefitted from different local cultural infrastructures where some unique cultural cliques and conversations, key events and rollouts, and pre-venture mishmashes can thrive (Beltagui et al., 2020; Tracey et al., 2018). Without that, policy efforts to encourage that local cultural infrastructure, the material side of the ecosystem will rapidly become inflexible and ossify   –   or perhaps never emerge in the first place. But before creating policies to foster cultural infrastructure and co- evolutionary paths, we think policymakers and government should ask whether they   should be   directly, rather than indirectly, involved in fostering emergent entrepreneurial processes. Today most of the policy world accepts that some involvement is important (Heaton et al., 2019; O ’ Connor et al., 2018). This particularly holds where collective action failures hinder early stage ecosystem activities that can serve the public good in the long term but are unlikely to generate short term profits and are thus generally not properly addressed by the private sector without policy support (Seidel et al., 2020). But we believe that setting up the cultural and material infrastructure less directly is potentially more beneficial. Universities are just one example, and are part of a broader set of cultural and institutional possibilities that could be coordinated to help address the collective action failures in the more traditionally market reliant sectors (Heaton et al., 2019). In addition, efforts to coordinate across university and business sectors, such as via formal partnerships, like the HIBAR Research Alliance, can help catalyze policy solutions and cultural changes to solve such collective action failures in innovation ecosystems. They do so by more tightly culturally integrating univer -  sities with broader innovation ecosystems (Whitehead et al., 2020). Funding agencies are another critical component for EEEs. For example, an NSF Assistant Director recently outlined a shift demonstrating the importance of an   “ honest assessment of possible and likely outcomes rather than on the probability of specific outcomes. ”   (Social Science Space, 2021). This shift away from specific outcome bets is well-aligned with our cultural possibilities approach. Successful ecosystem policy intervention requires building a strong cultural fabric across sectors instead of simply focusing on material entrepreneur support. These points imply that EEEs should not be   “ over-engineered ”   and that   “ reform ”   of bureaucratic approaches to them is needed (Isenberg, 2010: p. 9). The historical context of the regional EEE becomes crucial here, as Isenberg himself notes. Some systems in some countries have a history of more direct involvement of local or non-local government. Such is the case of our focal emerging AI ecosystem, and the same for some in Europe (Stam, 2017) and China (Armanios et al., 2017). Un -  fortunately, in the recent past, this has also led government, under public pressure to justify EEE infrastructure funding and political sup -  port, to try to pick winners (Autio et al., 2014; Bresnahan et al., 2001; Isenberg, 2010; Reif, 2020). Today policymakers continue to do so, as evidenced by one example of how the emerging Endless Frontier Act in the United States. That acts makes major policy bets on future tech -  nologies instead of focusing on the possibilities of addressing longer term societal problems (Prahbakar, 2020; Seidel et al., 2020). But conceiving the government as successfully strategically guiding inno -  vation ecosystems is outdated (Flagg   &   Harris, 2020). Understanding cultural holes can help policymakers more broadly identify the core problems and possibilities ecosystems can likely address, as opposed to placing large bets based on guesses of future technological winners informed by metrics and trends originating in other ecosystems.  4.3.   Limitations and next steps  In this article we have contributed to research on policy regarding  T.R. Hannigan et al.\n\n--- Page 14 ---\n\nResearch Policy 51 (2022) 104315  14 EEEs   by   reconceptualizing   entrepreneurial   ecosystems   (Shipilov   &  Gawar, 2019; Wurth, Stam   &   Siegel, 2021), using a cultural approach (Audretsch et la., 2020; Lounsbury   &   Glynn, 2019; Powell   &   Oberg, 2017), one based on interpretive data science and big data (Hannigan et al., 2019; Mohr et al., 2020; Schmiedel et al., 2019). We have done so developing cultural-based theory around EEEs, with a dimensionaliza -  tion of the cultural layer, and a methodology for comparing and con -  trasting the cultural map to a material one in a sample emerging AI. This, in turn, has allowed us to offer suggestions for policy analytics and for policymakers who wish to use this cultural approach and mapping tool methodology. Being a measurement article, our study has some obvious research limitations, the first of which is the restrained ability to fully demon -  strate the impact of the measurement system ’ s usefulness for capturing long term system evolution. In this respect, we follow recent directions in entrepreneurship research (Obschonka   &   Audretsch, 2020; Louns -  bury   &   Glynn, 2019) that propose applying big data tools to enhance the articulation of entrepreneurial possibilities, particularly at early stages of an ecosystem. Instead of arguing for the technology alone, we explicitly emphasized how this new approach paired with a cultural perspective can be a valuable form of mapping EEEs. This moves beyond other big data studies that continue to carry forward the attribute-based framing to social media, such as Obschonka et al. (2020) that found correlations between tweets and psychological traits of a region. Our approach is a first step in pushing for cultural and material dimensions as intertwined and co-evolving in a particular configuration. But this is based on capturing a snapshot in time. We envision capturing more panels of the cultural and materials dimension ’ s evolution over time in a subsequent study to track some of these effects, and elaborate on the co-evolution of the dimensions. A second noticeable drawback is capturing the full dynamism of cultural discourse and artifacts in the cultural layer. We acknowledge that other important cultural activities take place in an emerging entrepreneurial ecosystem, such as field configuring events, like con -  ferences, or regularly organized meetups. Related work in management has demonstrated how key events generate cultural discourse (Hannigan  &   Casasnovas, 2020; Seidel, 2018; Hardy   &   Maguire, 2010; Zilber, 2011), and we have reason to believe the same is true in entrepreneurial ecosystems. The key criteria are whether sufficient sets of relevant actors are involved with (and paying attention to) the discourse, and whether it can be effectively captured. This points to our perspective as ultimately being relational. In planning this article, we conducted initial explor -  atory field work and determined that the Edmonton ecosystem was actively using Twitter. But this discourse was also in part fueled by conferences such as the SingularityU Canada Summit and other local events being organized on the   meetup.com   platform. Further work could study these events in more detail, considering the discourse produced, as well as the network of actors attending. A third drawback is that our methodology, while state-of-the-art, still needed to be adapted to the short, burstiness of tweets, and the topics that were rendering were thus only approximated and only if the system itself for that two-year period was adequately stable. Our central argu -  ment was based on the premise that tweets over a two-year period carried patterns and regularities of meaning. This was the basis of rendering   meaning   structures   that   form   cultural   holes.   While   we examined varieties of topic model numbers and cut-offs as forms of sensitivity analyses in our two-year panel, further work might disen -  tangle whether an adequate corpus is based on a minimum threshold of time, or is more a function of key events that generate discourse in that space. Our review of recent computer science work on the subject (i.e., Jonsson   &   Stolee, 2019) pointed to BTM as the leading approach for modeling short texts. Newer methods are continuing to be developed for the purpose of topic modeling twitter data with more structural as -  sumptions. For example, stLDA-C (Tierney, Bail   &   Volfovsky, 2021) is an emerging approach that shifts from modeling topics in individual tweets to mapping distributions of topics for users. Our modeling of discursive strategy signatures (spectrograms) is a contribution to that literature and suggests that a similar set of patterns within the time periods by user sets may be a way to capture stable discourse. Clearly, this is an area for future research.  5.   Conclusion  These limitations not withstanding, we hope that our work will still encourage EEE researchers to embrace less literal or concrete readings of ecosystems and their processes, in favor of a more cognitive and societally-grounded one anchored on cultural analysis (Audretsch et al., 2020; Lounsbury   &   Glynn, 2019) that reflects a systematic and tech -  nological bent (Mohr et al., 2020; Powell   &   Oberg, 2017). Introducing this new tool will enable policy analysts to provide policymakers with a more comprehensive data informed understanding of their local possi -  bilities. This reframing enables new insights on earlier stages of system emergence that are difficult to consider when only considering material markers. Local thought leaders (O ’ Conner et al., 2018) and intermediary organizations   (Clayton   et   al.,   2018;   Heaton   et   al.,   2019)   become important in part because of the visions and local buzz they create and promote in the cultural map (Nambisan et al., 2019). In addition, as demonstrated above, discursive strategies are a key component in their entrepreneurial toolkit (Lounsbury   &   Glynn, 2019; Swidler, 1986). These strategies help bridge these possibilities in the form of cultural holes (Pachucki   &   Breiger, 2010). Bridging, in turn, attracts attention and resources - personnel, funding, complementary technology, and other startups, helping turn   “ lead into gold ”   (Clough et al., 2019).  Credit author statement  Timothy R. Hannigan   –   conceptualization, data collection, mea -  surement, analysis (esp. of cultural models), core writing, and response letter.   Anthony   R.   Briggs   –   some   conceptualization,   material   data collection and some analysis, some editing and some writing. Rodrigo Valadao   –   some conceptualization, cultural data collection and analysis, editing, reference checking, some writing. Marc-David L. Seidel   –   some conceptualization, core writing on policy sections, some editing. P. Devereaux (Dev) Jennings   –   team coordination, corresponding author, conceptualization, data collection, measurement, analysis (esp. of ma -  terial models), core writing, and response letter. All in all, we worked as a team, and our contributions reflect our team ’ s effort.  Declaration of Competing Interest  None.  Acknowledgement  We would like to acknowledge the suggestions and reactions to this paper by the IDeaS (Interpretive Data Science) Group, the probing questions of Jennifer Jennings, the design assistance of Debjani Hanni -  gan,   the   theoretical considerations   raised   by   Mike Lounsbury, the friendly review by Marc Ventresca and the scraping code from Chris Cliff. This research was supported by the Social Science and Humanities Research Council (SSHRC), including grants #430-2018-01061 and #435-2017-0046.  T.R. Hannigan et al.\n\n--- Page 15 ---\n\nResearch Policy 51 (2022) 104315  15 Appendix  Table A2  Selected set of structural hole measures for edmonton AI ecosystem organizations a  Degree   EffSize   Efficiency   Constraint   Hierarchy   EgoBet   Ln(Constraint)   Indirects   Density   AvgDeg   Numholes  AICo01   1   1   1   0   1   0   0   0   0   0   0 AICo02   2   2   1   0.5   0   2   -0.693   0   0   0   2 AICo03   2   2   1   0.5   0   2   -0.693   0   0   0   2 AICo04   1   1   1   0   1   0   0   0   0   0   0 AICo05   2   2   1   0.5   0   2   -0.693   0   0   0   2 AICo06   1   1   1   0   1   0   0   0   0   0   0 AICo07   2   2   1   0.5   0   2   -0.693   0   0   0   2 AICo08   6   5.333   0.889   0.306   0.032   26   -1.186   0.333   0.133   0.667   26 AICo09   1   1   1   0   1   0   0   0   0   0   0 BigTech01   2   2   1   0.5   0   2   -0.693   0   0   0   2 BigTech02   1   1   1   0   1   0   0   0   0   0   0 BigTech03   5   4.6   0.92   0.3   0.05   18   -1.204   0.2   0.1   0.4   18 BigTech04   4   4   1   0.25   0   12   -1.386   0   0   0   12 BigTech05   1   1   1   0   1   0   0   0   0   0   0 BigTech06   2   1   0.5   1.125   0   0   0.118   0.5   1   1   0 BigTech07   1   1   1   0   1   0   0   0   0   0   0 Comm01   6   5.667   0.944   0.236   0.045   28   -1.443   0.167   0.067   0.333   28 Comm02   6   3.667   0.611   0.487   0.063   9.667   -0.719   0.661   0.467   2.333   16 Comm03   3   2.333   0.778   0.611   0.052   4   -0.492   0.333   0.333   0.667   4 Comm04   8   6.5   0.813   0.31   0.052   41   -1.172   0.531   0.214   1.5   44 Comm05   5   5   1   0.2   0   20   -1.609   0   0   0   20 Comm06   2   1   0.5   1.125   0   0   0.118   0.5   1   1   0 Comm07   2   2   1   0.5   0   2   -0.693   0   0   0   2 Exit01   4   4   1   0.25   0   12   -1.386   0   0   0   12 Exit02   1   1   1   0   1   0   0   0   0   0   0 Exit03   2   1   0.5   1.125   0   0   0.118   0.5   1   1   0 Exit04   1   1   1   0   1   0   0   0   0   0   0 ( continued on next page )  Table A1  Most active and mentioned tweeters  Alphabetical sorting   Most Active Tweeters (sent out)   Most Mentioned (received in) New ventures anchor list   New ventures a   Followers of new ventures b   New ventures   # Mentions   Followers of new ventures   # Mentions altaml_com   ATBFinancial   allantaylor   altaml_com   131   amiithinks   320 ATBFinancial   altaml_com   buzzilinear   testfirelabs   117   startupedmonton   307 bixscdn   bixscdn   cixcommunity   ATBFinancial   75   innovateyeg   257 BizPlanWorld   gabbi_ai   yegtweetup   Medo_ai   58   tecedmonton   171 boardeeapp   honest_door   yrrabyrrab   granify   40   valhallacap   159 BuddyTracker_io   testfirelabs   startupcalgary   SAM_Desk   34   nform   143 capstoneitdev   whitespark   startupedmonton   profilze   30   yeghealthcity   139 dhanalytics   StreamTechInc   take_roots   honest_door   28   taprootyeg   135 gabbi_ai   trustscience   donaterecycleit   dhanalytics   25   flyeia   112 granify   BizPlanWorld   innovateyeg   StreamTechInc   23   futurecite   105 honest_door   granify   themetaspaceyyc   boardeeapp   23   dbayeg   98 howtocreateart   SAM_Desk   govkid   mobiledatatech   17   inventurescan   86 innovills_tech   mymatchwork   bitcoinbrains   bixscdn   17   cheryllyeg   82 Medo_ai   boardeeapp   dbayeg   trustscience   12   edmontonglobal   81 mobiledatatech   Medo_ai   crowncathy   webdataguru   12   scaleupyeg   81 mpowered_tech   sportingcharts   chrislabossiere   touchmetric   11   worknicer   79 mymatchwork   innovills_tech   yeghealthcity   BizPlanWorld   11   valhallaangels   74 PitchSixty   dhanalytics   futurecite   ZanInitiative   6   rainforestyeg   74 profilze   WyvernSpace   worknicer   sportingcharts   6   cixcommunity   64 SAM_Desk   mobiledatatech   abudnick   mymatchwork   5   gethendrix   63 sportingcharts   ZanInitiative   sellarcast   gabbi_ai   5   fscammells   60 StreamTechInc   PitchSixty   flyeia   whitespark   4   justinc_ai   57 testfirelabs   webdataguru   clairemacyeg   WyvernSpace   2   albertaventure   44 touchmetric   capstoneitdev   naitmawjicentre   innovills_tech   2   startupcalgary   43 trustscience   profilze   clintonsenkow   PitchSixty   2   reg_joseph   37 webdataguru   touchmetric   amiithinks   howtocreateart   1   alondrac_ai   36 whitespark   BuddyTracker_io   planedmonton   capstoneitdev   0   williamsengca   35 WyvernSpace   howtocreateart   katesversion   BuddyTracker_io   0   ashifmawji   35 ZanInitiative   mpowered_tech   visibilitypower   mpowered_tech   0   naitmawjicentre   32  a   From most to least active  b   From most to least  T.R. Hannigan et al.\n\n--- Page 16 ---\n\nResearch Policy 51 (2022) 104315  16 References  Acs, Z.J., Stam, E., Audretsch, D.B., O ’ Connor, A, 2017. The lineages of the entrepreneurial ecosystem approach. Small Bus. Econ. 49 (1), 1 – 10. https://doi.org/ 10.1007/s11187-017-9864-8. Aldrich, H.E., 1994. Fools Rush in? The Institutional Context of Industry Creation, p. 27. Alvarez, S.A., Barney, J.B., McBride, R., Wuebker, R., 2014. Realism in the Study of Entrepreneurship. Acad. Manage. Rev. 39 (2), 227 – 231. Armanios, D.E., Eesley, C.E., Li, J., Eisenhardt, K.M., 2017. How entrepreneurs leverage institutional intermediaries in emerging economies to acquire public resources. Strateg. Manage. J. 38 (7), 1373 – 1390. https://doi.org/10.1002/smj.2575. Audretsch, D.B., 2021. Have we oversold the Silicon Valley model of entrepreneurship? Small Bus. Econ. 56 (2), 849 – 856. Audretsch, D.B., Cunningham, J.A., Kuratko, D.F., Lehmann, E.E., Menter, M., 2019. Entrepreneurial ecosystems: Economic, technological, and societal impacts. J. Technol. Transfer 44 (2), 313 – 325. Autio, E., 2017. Strategic Entrepreneurial Internationalization: A Normative Framework. Strateg. Entrepreneurship J. 11 (3), 211 – 227. https://doi.org/10.1002/sej.1261. Autio, E., Kenney, M., Mustar, P., Siegel, D., Wright, M., 2014. Entrepreneurial innovation: The importance of context. Res. Policy 43 (7), 1097 – 1108. Autio, E., Nambisan, S., Thomas, L.D.W., Wright, M, 2018. Digital affordances, spatial affordances, and the genesis of entrepreneurial ecosystems. Strateg. Entrepreneurship J. 12 (1), 72 – 95. https://doi.org/10.1002/sej.1266. Baker, T., Nelson, R.E., 2005. Creating Something from Nothing: Resource Construction through Entrepreneurial Bricolage. Adm. Sci. Q. 50 (3), 329 – 366. https://doi.org/ 10.2189/asqu.2005.50.3.329.  Fig. A1.   Bi-Directed (Mutual) Network Graph of 4 +   Twitter Handles.  Table A2   ( continued   )  Degree   EffSize   Efficiency   Constraint   Hierarchy   EgoBet   Ln(Constraint)   Indirects   Density   AvgDeg   Numholes  Exit05   3   2.333   0.778   0.611   0.052   4   -0.492   0.333   0.333   0.667   4 Exit06   2   2   1   0.5   0   2   -0.693   0   0   0   2 Exit07   2   2   1   0.5   0   2   -0.693   0   0   0   2 Exit08   4   3   0.75   0.563   0   8   -0.575   0.5   0.333   1   8 FundOrg01   4   3.5   0.875   0.406   0.055   10   -0.901   0.25   0.167   0.5   10 FundOrg02   4   4   1   0.25   0   12   -1.386   0   0   0   12 Gov01   1   1   1   0   1   0   0   0   0   0   0 Gov02   10   8.8   0.88   0.218   0.062   75   -1.522   0.425   0.133   1.2   78 Gov03   2   2   1   0.5   0   2   -0.693   0   0   0   2 Gov04   3   3   1   0.333   0   6   -1.099   0   0   0   6 Incumb01   1   1   1   0   1   0   0   0   0   0   0  a   The full set is available upon request.  T.R. Hannigan et al.\n\n--- Page 17 ---\n\nResearch Policy 51 (2022) 104315  17 Beltagui, A., Rosli, A., Candi, M., 2020. Exaptation in a digital innovation ecosystem: The disruptive impacts of 3D printing. Res. Policy 49 (1), 103833. https://doi.org/ 10.1016/j.respol.2019.103833. ...   &   Bonawitz, K., Eichner, H., Grieskamp, W., Huba, D., Ingerman, A., Ivanov, V., Roselander, J., 2019. Towards federated learning at scale: System design. arXiv preprint arXiv:1902.01046. Borgatti, S.P., Everett, M.G., Johnson, J.C., 2018. Analyzing social networks. Sage. Bourdieu, P., 1993. The Field of Cultural Production: Essays on Art and Literature. Columbia University Press. Bouslama, S., 2020. Join Edmonton Global at World Summit AI. Edmonton Global. https ://edmontonglobal.ca/join-edmonton-global-at-the-world-summit-ai/. Bowles, N, 2021. Join Us in Miami! Love, Masters of the Universe.   New York Times. https://www.nytimes.com/2021/01/29/technology/join-us-in-miami-love-maste rs-of-the-universe.html. Bramwell, A., Wolfe, D.A., 2008. Universities and regional economic development: The entrepreneurial University of Waterloo. Res. Policy 37 (8), 1175 – 1187. Bresnahan, T., Gambardella, A., Saxenian, A., Wallsten, S., 2001. Old Economy ”   Inputs for   “ New Economy ”   Outcomes: Cluster Formation in the New Silicon Valleys. Industr. Corp. Change 10 (4), 835 – 860. https://doi.org/10.1093/icc/10.4.835. Brown, R., Mawson, S., 2019. Entrepreneurial ecosystems and public policy in action: A critique of the latest industrial policy blockbuster. Cambridge J. Regions Economy Soc. 12 (3), 347 – 368. https://doi.org/10.1093/cjres/rsz011. Brown, R., Mawson, S., Lee, N., Peterson, L., 2019. Start-up factories, transnational entrepreneurs and entrepreneurial ecosystems: Unpacking the lure of start-up accelerator programmes. Eur. Plann. Stud. 27 (5), 885 – 904. https://doi.org/ 10.1080/09654313.2019.1588858. Brown, R., Mawson, S., Mason, C., 2017. Myth-busting and entrepreneurship policy: The case of high growth firms. Entrepreneurship Regional Dev. 29 (5 – 6), 414 – 443. https://doi.org/10.1080/08985626.2017.1291762. Burt, R., 2005. Brokerage and closure: An introduction to social capital. Burt, R.S., 2004. Structural Holes and Good Ideas. Am. J. Sociol. 110 (2), 349 – 399. https://doi.org/10.1086/421787. News, CBC, 2019. Alberta ’ s high-tech sector to get $100M boost over 5 years | CBC News. CBC. https://www.cbc.ca/news/canada/calgary/notley-bilous-alberta- innovates-funding-ai-artificial-intelligence-calgary-amii-1.5017785. Clayton, P., Feldman, M., Lowe, N., 2018. Behind the Scenes: Intermediary Organizations that Facilitate Science Commercialization Through Entrepreneurship. Acad. Manage. Perspect. 32 (1), 104 – 124. https://doi.org/10.5465/amp.2016.0133. Clough, D., Fang, T., Vissa, B., Wu, A., 2019. Turning lead Into gold- How do entrepreneurs mobilize resources to exploit opportunities. Academy of Management Annals. Cooke, P., Gomez Uranga, M., Etxebarria, G., 1997. Regional innovation systems: Institutional and organisational dimensions. Res. Policy 26 (4 – 5), 475 – 491. https:// doi.org/10.1016/S0048-7333(97)00025-5. Croidieu, G., Kim, P.H., 2018. Labor of love: Amateurs and lay-expertise legitimation in the early U.S. radio field. Adm. Sci. Q. 63 (1), 1 – 42. Davenport, T., 2019. Learning From The Canadian Model Of AI. Forbes. https://www. forbes.com/sites/tomdavenport/2019/11/19/learning-from-the-canadian-model- of-ai/?sh = 40748b1c2300. De Marchi, G., Lucertini, G., Tsouki ` as, A., 2016. From evidence-based policy making to policy analytics. Ann. Oper. Res. 236 (1), 15 – 38. DiMaggio, P.J., 1997. Culture and cognition. Ann. Rev. Sociol. 23, 263 – 287. Edelmann, A., Wolff, T., Montagne, D., Bail, C.A., 2020. Computational Social Science and Sociology. Ann. Rev. Sociol. 46 (1), 61 – 81. Feld, B., 2020. Startup Communities: Building an Entrepreneurial Ecosystem in Your City. John Wiley   &   Sons. Feldman, M., Siegel, D.S., Wright, M., 2019. New developments in innovation and entrepreneurial ecosystems. Industr. Corp. Change 28 (4), 817 – 826. https://doi.org/ 10.1093/icc/dtz031. Felin, T., Kauffman, S., Koppl, R., Longo, G., 2014. Economic Opportunity and Evolution: Beyond Landscapes and Bounded Rationality. Strateg. Entrepreneurship J. 8 (4), 269 – 282. Ferraro, F., Etzion, D., Gehman, J., Al, O.S.E., 2015. Tackling Grand Challenges Pragmatically: Robust Action.   Revisited . Financial Post, 2017. Revolution AI: How Edmonton is gaining ground as a research and innovation hub for AI. Financial Post. https://financialpost.com/entrepreneur/revol ution-ai-how-edmonton-is-gaining-ground-as-a-research-and-innovation-hub-for-ai. Fischer, E., Reuber, A.R., 2011. Social interaction via new social media:(How) can interactions on Twitter affect effectual thinking and behavior? J. Bus. Venturing 26 (1), 1 – 18. Flagg, M.,   &   Harris, P. (2020, September).   September 2020 . How to Lead Innovation in a Changed World. https://intangibleeconomy.wordpress.com/2020/09/. Florida, R., Kenney, M., 1990. High-Technology Restructuring in the USA and Japan. Environ. Plann. A 22 (2), 233 – 252. https://doi.org/10.1068/a220233. Forbes, D.P., Kirsch, D.A., 2011. The study of emerging industries: Recognizing and responding to some central problems. J. Bus. Venturing 26 (5), 589 – 602. https://doi. org/10.1016/j.jbusvent.2010.01.004. Friedland, R., 2013. The gods of institutional life: Weber ’ s value spheres and the practice of polytheism. Crit. Res. Religion 1 (1), 15 – 24. https://doi.org/10.1177/ 2050303213476104. Giddens, A., 1984. The constitution of society: Outline of the structuration theory. Polity Press. Giorgi, S., Lockwood, C., Glynn, M.A., 2015. The Many Faces of Culture: Making Sense of 30 Years of Research on Culture in Organization Studies. Acad. Manage. Ann. 9 (1), 1 – 54. https://doi.org/10.1080/19416520.2015.1007645. Globe and Mail, 2017. It ’ s time to make the Canadian AI ecosystem bloom. https://www. theglobeandmail.com/report-on-business/rob-commentary/its-time-to-make-the- canadian-ai-ecosystem-bloom/article35559957/. GlobeNewsWire. (2020a). $57 million to Alberta researchers and innovators. . GlobeNewswire News Room. https://www.globenewswire.com/news-release/2020/ 05/25/2038229/0/en/57-million-to-Alberta-researchers-and-innovators.html. Goldberg, A., 2011. Mapping shared understandings using relational class analysis: The case of the cultural omnivore reexamined. Am. J. Sociol. 116 (5), 1397 – 1436. Goldenstein, J., Poschmann, P., 2019. Analyzing Meaning in Big Data: Performing a Map Analysis Using Grammatical Parsing and Topic Modeling. Sociol. Methodol. 49 (1), 83 – 131. https://doi.org/10.1177/0081175019852762. Hallett, T. (2003). Symbolic Power and Organizational Culture.   Sociological Theory , 21 (2), 128 – 149. https://doi.org/10.1111/1467-9558.00181. Hannah, D.P., Eisenhardt, K.M., 2018. How firms navigate cooperation and competition in nascent ecosystems. Strateg. Manage. J. 39 (12), 3163 – 3192. https://doi.org/ 10.1002/smj.2750. Hannigan, T.R., Casasnovas, G., 2020. New structuralism and field emergence: The co- constitution of meanings and actors in the early moments of social impact investing. Res. Sociol. Org. 68, 147 – 184. Hannigan, T.R., Haans, R.F.J., Vakili, K., Tchalian, H., Glaser, V.L., Wang, M.S., Kaplan, S., Jennings, P.D, 2019. Topic Modeling in Management Research: Rendering New Theory from Textual Data. Acad. Manage. Ann. 13 (2), 586 – 632. https://doi.org/10.5465/annals.2017.0099. Hardy, C., Maguire, S., 2010. Discourse, Field-Configuring Events, and Change in Organizations and Institutional Fields: Narratives of DDT and the Stockholm Convention. Acad. Manag. J. 53 (6), 1365 – 1392. Heaton, S., Siegel, D.S., Teece, D.J., 2019. Universities and innovation ecosystems: A dynamic capabilities perspective. Industr. Corp. Change 28 (4), 921 – 939. https:// doi.org/10.1093/icc/dtz038. Huff, A., 1990. Mapping strategic thought. John Wiley   &   Sons. Hwang, M.V.W., Horowitt, M.G, 2012. The Rainforest: The Secret to Building the Next Silicon Valley. Regenwald. Isenberg, D.J., 2010. How to Start an Entrepreneurial Revolution. Harv. Bus. Rev. 10. Jennings, P.D., Greenwood, R., Lounsbury, M.D., Suddaby, R., 2013. Institutions, entrepreneurs, and communities: A special issue on entrepreneurship. J. Bus. Venturing 28 (1), 1 – 9. https://doi.org/10.1016/j.jbusvent.2012.07.001. Joannou, A., 2020. Alberta government giving $9 million in funding to Edmonton AI company. Edmonton J. https://edmontonjournal.com/news/politics/alberta-govern ment-to-give-9-million-in-funding-to-edmonton-ai-company. Johnson, D., Bock, A.J., George, G., 2019. Entrepreneurial dynamism and the built environment in the evolution of university entrepreneurial ecosystems. Industr. Corp. Change 28 (4), 941 – 959. Jonsson, E., Stolee, J., 2019. An Evaluation of Topic Modelling Techniques for Twitter 11. Kaufman, J., 2004. Endogenous Explanation in the Sociology of Culture. Ann. Rev. Sociol. 30 (1), 335 – 357. https://doi.org/10.1146/annurev.soc.30.012703.110608. Kay, N.M., Leih, S., Teece, D.J., 2018. The role of emergence in dynamic capabilities: A restatement of the framework and some possibilities for future research. Industr. Corp. Change 27 (4), 623 – 638. https://doi.org/10.1093/icc/dty015. Kennedy, M.T., 2008. Getting Counted: Markets, Media, and Reality. Am. Sociol. Rev. 73 (2), 270 – 295. Kim, H., Hoskisson, R.E., Lee, S.H., 2015. Why strategic factor markets matter:   “ New ”  multinationals ’   geographic diversification and firm profitability. Strateg. Manage. J. 36 (4), 518 – 536. Kirsch, D., Moeen, M., Wadhwani, R.D, 2013. Historicism and Industry Emergence: Industry Knowledge from Pre-emergence to Stylized Fact. In: Bucheli, M., Wadhwani, R.D. (Eds.), Organizations in Time. Oxford University Press, pp. 217 – 240. https://doi.org/10.1093/acprof:oso/9780199646890.003.0009. Kuckertz, A., Br ¨ andle, L., Gaudig, A., Hinderer, S., Morales Reyes, C.A., Prochotta, A., Steinbrink, K.M., Berger, E.S.C, 2020. Startups in times of crisis   –   A rapid response to the COVID-19 pandemic. J. Bus. Venturing Insights 13, e00169. https://doi.org/ 10.1016/j.jbvi.2020.e00169. Lam, L., Seidel, M.-D.L., 2020. Hypergrowth Exit Mindset: Destroying Societal Wellbeing through Venture Capital Biased Social Construction of Value. J. Manage. Inquiry 29 (4), 471 – 474. https://doi.org/10.1177/1056492620929085. Lizardo, O., 2014. Omnivorousness as the bridging of cultural holes: A measurement strategy. Theory Soc. 43 (3 – 4), 395 – 419. https://doi.org/10.1007/s11186-014- 9220-9. Lounsbury, M., Glynn, M.A., 2019. Cultural Entrepreneurship: A New Agenda for the Study of Entrepreneurial Processes and Possibilities, 1st ed. Cambridge University Press. https://doi.org/10.1017/9781108539487. Lounsbury, M., Ventresca, M., 2002. Social structure and organizations revisited. JAI. Malecki, E.J., 2018. Entrepreneurship and entrepreneurial ecosystems. Geograph. Compass 12 (3), e12359. https://doi.org/10.1111/gec3.12359. Manning, C., Surdeanu, M., Bauer, J., Finkel, J., Bethard, S., McClosky, D., 2014. The Stanford CoreNLP Natural Language Processing Toolkit. In: Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 55 – 60. https://doi.org/10.3115/v1/P14-5010. Markides, C.C., Williamson, P.J., 1994. Related diversification, core competences and corporate performance. Strateg. Manage. J. 15 (S2), 149 – 165. Markman, G.D., Gianiodis, P.T., Phan, P.H., Balkin, D.B., 2005. Innovation speed: Transferring university technology to market. Res. Policy 34 (7), 1058 – 1075. https://doi.org/10.1016/j.respol.2005.05.007. Marsden, P., 2005. Recent developments in network measurement. Models and methods in social network analysis, p. 30.  T.R. Hannigan et al.\n\n--- Page 18 ---\n\nResearch Policy 51 (2022) 104315  18 Martin, J., 1992. Cultures in Organizations: Three Perspectives. Oxford University Press, USA. Shepherd, D.A., McMullen, J.S., Jennings, P.D., 2007. The formation of opportunity beliefs: Overcoming ignorance and reducing doubt. Strateg. Entrepreneurship J. 1 (1 – 2), 75 – 95. Mehra, A., Kilduff, M., Brass, D.J., 2001. The Social Networks of High and Low Self- Monitors: Implications for Workplace Performance. Adm. Sci. Q. 46 (1), 121 – 146. Mohr, J.W., Bail, C.A., Frye, M., Lena, J.C., Lizardo, O., McDonnell, T.E., Mische, A., Tavory, I., Wherry, F.F., 2020. Measuring Culture. Columbia University Press. Mohr, J.W., Bogdanov, P., 2013. Introduction — Topic models: What they are and why they matter. Poetics 41 (6), 545 – 569. https://doi.org/10.1016/j. poetic.2013.10.001. Mohr, J.W., Duquenne, V., 1997. The Duality of Culture and Practice: Poverty Relief in New York City, 1888-1917. Theory Soc. 26 (2/3), 305 – 356. Mohr, J.W., Rawlings, C., 2018. The Oxford Handbook of Cultural Sociology. Four Ways to Measure Culture: Social Science, Hermeneutics, and the Cultural Turn. Nambisan, S., Lyytinen, K., Yoo, Y., 2020. Digital innovation: Towards a transdisciplinary perspective. Handbook Digital Innovation. https://www.elgaronl ine.com/view/edcoll/9781788119979/9781788119979.00008.xml. Nambisan, S., Wright, M., Feldman, M., 2019. The digital transformation of innovation and entrepreneurship: Progress, challenges and key themes. Res. Policy 48 (8), 103773. https://doi.org/10.1016/j.respol.2019.03.018. Navis, C., Glynn, M.A., 2010. How New Market Categories Emerge: Temporal Dynamics of Legitimacy, Identity, and Entrepreneurship in Satellite Radio, 1990 – 2005. Adm. Sci. Q. 55 (3), 439 – 471. https://doi.org/10.2189/asqu.2010.55.3.439. Nelson, L.K., 2019. To Measure Meaning in Big Data, Don ’ t Give Me a Map, Give Me Transparency and Reproducibility. Sociol. Methodol. 49 (1), 139 – 143. Oh, D.-S., Phillips, F., Park, S., Lee, E., 2016. Innovation ecosystems: A critical examination. Technovation 54, 1 – 6. Obschonka, M., Audretsch, D.B., 2020. Artificial intelligence and big data in entrepreneurship: A new era has begun. Small Bus. Econ. 55 (3), 529 – 539. Obschonka, M., Lee, N., Rodríguez-Pose, A., Eichstaedt, J.C., Ebert, T., 2020. Big data methods, social media, and the psychology of entrepreneurial regions: Capturing cross-county personality traits and their impact on entrepreneurship in the USA. Small Bus. Econ. 55 (3), 567 – 588. O ’ Connor, A., Stam, E., Sussan, F., Audretsch, D.B., 2018. Entrepreneurial Ecosystems: The Foundations of Place-based Renewal. In: O ’ Connor, A., Stam, E., Sussan, F., Audretsch, D.B. (Eds.), Entrepreneurial Ecosystems. Springer International Publishing, pp. 1 – 21. https://doi.org/10.1007/978-3-319-63531-6_1. Vol. 38. Owen-Smith, J., Powell, W.W., 2004. Knowledge Networks as Channels and Conduits: The Effects of Spillovers in the Boston Biotechnology Community. Org. Sci. 15 (1), 5 – 21. Pachucki, M.A., Breiger, R.L., 2010. Cultural Holes: Beyond Relationality in Social Networks and Culture. Ann. Rev. Sociol. 36 (1), 205 – 224. https://doi.org/10.1146/ annurev.soc.012809.102615. Padgett, J.F., Ansell, C.K., 1993. Robust Action and the Rise of the Medici, 1400-1434. Am. J. Sociol. 98 (6), 1259 – 1319. https://doi.org/10.1086/230190. Pencheva, I., Esteve, M., Mikhaylov, S.J., 2020. Big Data and AI   –   A transformational shift for government: So, what next for research? Pub. Policy Admin. 35 (1), 24 – 44. Perkmann, M., Schildt, H., 2015. Open data partnerships between firms and universities: The role of boundary organizations. Res. Policy 44 (5), 1133 – 1143. Porac, J.F., Thomas, H., Wilson, F., Paton, D., Kanfer, A., 1995. Rivalry and the Industry Model of Scottish Knitwear Producers. Adm. Sci. Q. 40 (2), 203. https://doi.org/ 10.2307/2393636. Porter, M., 1998. Clusters and the New Economics of Competition. http://marasbiber.co m/wp-content/uploads/2018/05/Michael-E.-Porter-Cluster-Reading.pdf. Powell, W.W., Oberg, A., 2017. Networks and Institutions. In: Greenwood, R., Oliver, C., Lawrence, T.B., Meyer, R.E. (Eds.), The Sage Handbook of Organizational Institutionalism, 2nd ed. Sage, London, pp. 446 – 476. Powell, W.W., Snellman, K., 2004. The Knowledge Economy. Ann. Rev. Sociol. 30 (1), 199 – 220. https://doi.org/10.1146/annurev.soc.29.010202.100037. Powell, W.W., White, D.R., Koput, K.W., Owen-Smith, J., 2005. Network Dynamics and Field Evolution: The Growth of Interorganizational Collaboration in the Life Sciences. Am. J. Sociol. 110 (4), 1132 – 1205. https://doi.org/10.1086/421508. JSTOR. Rawlings, C.M., Childress, C., 2019. Emergent Meanings: Reconciling Dispositional and Situational Accounts of Meaning-Making from Cultural Objects. Am. J. Sociol. 124 (6), 1763 – 1809. https://doi.org/10.1086/703203. Reif, L.R., 2020. MIT President on US Competitiveness and the Endless Frontier Act. Issues Sci. Technol. https://issues.org/to-compete-with-china-america-needs-the-e ndless-frontier-act-mit-rafael-reif/. Saxenian, A., 1994. Regional Networks: Industrial Adaptation in Silicon Valley and Route 128. Saxenian, A., 1990. Regional Networks and the Resurgence of Silicon Valley. Calif. Manage. Rev. 33 (1), 89 – 112. https://doi.org/10.2307/41166640. Schiffer, E., Hauck, J., 2010. Net-Map: Collecting Social Network Data and Facilitating Network Learning through Participatory Influence Network Mapping. Field Methods 22 (3), 231 – 249. https://doi.org/10.1177/1525822x10374798. Schmiedel, T., Müller, O., vom Brocke, J., 2019. Topic Modeling as a Strategy of Inquiry in Organizational Research: A Tutorial With an Application Example on Organizational Culture. Org. Res. Methods 22 (4), 941 – 968. https://doi.org/ 10.1177/1094428118773858. Seidel, M.-D.L., 2018. The Role of Conferences in the Emergence of Developmental Professional Culture. J. Manage. Inquiry 27 (2), 149 – 153. Seidel, M.-D.L., Greve, H.R., 2017. Emergence: How Novelty, Growth, and Formation Shape Organizations and Their Ecosystems. In: Seidel, M.-D.L., Greve, H.R. (Eds.), Research in the Sociology of Organizations. Emerald Publishing Limited, pp. 1 – 27. https://doi.org/10.1108/S0733-558x20170000050020. Vol. 50. Seidel, M.-D.L., Schwartz, L.H., Jordan, G.B., 2020. Innovation Isn ’ t a Frontier — It ’ s an Ecosystem. Issues Sci. Technol. https://issues.org/the-endless-frontier-legislation-hi bar-research-innovation-ecosystem/. Shi, Z., Rui, H., Whinston, A.B., 2014. Content sharing in a social broadcasting environment: evidence from twitter. MIS Q. 38 (1), 123 – 142. Shipilov, A., Gawer, A., 2019. Integrating Research on Inter-Organizational Networks and Ecosystems. Acad. Manage. Ann. https://doi.org/10.5465/annals.2018.0121. Sievert, C.,   &   Shirley, K. (2014). LDAvis: A method for visualizing and interpreting topics.   Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces , 63 – 70. https://doi.org/10.3115/v1/W14-3110. Social Science Space. (2021). NSF Letter Frames Concept of ‘Broader Impact ’ . https:// www.socialsciencespace.com/2021/03/nsf-letter-frames-concept-of-broader-impa ct/. Soubli ` ere, J.-F., Gehman, J., 2019. The Legitimacy Threshold Revisited: How Prior Successes and Failures Spill Over to Other Endeavors on Kickstarter. Acad. Manag. J. https://doi.org/10.5465/amj.2017.1103 amj.2017.1103. Spigel, B., 2015. Edinburgh ’ s Entrepreneurial and Support Ecosystem, 11. Spigel, B., 2017. The Relational Organization of Entrepreneurial Ecosystems. Entrepreneurship Theory Pract. 41 (1), 49 – 72. https://doi.org/10.1111/etap.12167. Spigel, B., 2020. Entrepreneurial Ecosystems: Theory, Practice and Futures. Edward Elgar Publishing. Spigel, B., Vinodrai, T., 2020. Meeting its Waterloo? Recycling in entrepreneurial ecosystems after anchor firm collapse. Entrepreneurship Reg. Dev. 1 – 22. Stam, E., 2015. Entrepreneurial ecosystems and regional policy: a sympathetic critique. Eur. Plann. Stud. 23 (9), 1759 – 1769. Stam, E. (2017). Measuring the Entrepreneurial Ecosystem.   Utrecht School of Economics, Discussion Paper Series 17 – 11. . Stam, Erik, van de Ven, A., 2019. Entrepreneurial ecosystem elements. Small Bus. Econ. https://doi.org/10.1007/s11187-019-00270-6. Startup Genome. (2019).   Global Startup Ecosystem Report 2019 . https://startupgenome. com/reports/global-startup-ecosystem-report-2019. Swidler, A., 1986. Culture in Action: Symbols and Strategies. Am. Sociol. Rev. 51 (2), 273 https://doi.org/10.2307/2095521. Suarez, F. [@francissuarez]. (2020, December 4).   How can I help?   Twitter. https://twitter .com/francissuarez/status/1335037068108554241. Tasselli, S.,   &   Kilduff, M. (2020). Network Agency.   Academy of Management Annals , annals.2019.0037. https://doi.org/10.5465/annals.2019.0037. Teece, D., Fleming, L., Feldman, M., Heaton, S., Desai, S., 2019. Special Issue Call for Uncommon Methods and Metrics for Local Entrepreneurial Ecosystems. Res. Policy. Economist, The, 2014. All together now.   The Economist . https://www.economist.com/ special-report/2014/01/16/all-together-now. Thomas, L.D.W., Ritala, P, 2021. Ecosystem Legitimacy Emergence: A Collective Action View. J. Manag., 0149206320986617 Thompson, T.A., Purdy, J.M., Ventresca, M.J., 2018. How entrepreneurial ecosystems take form: Evidence from social impact initiatives in Seattle. Strateg. Entrepreneurship J. 12 (1), 96 – 116. https://doi.org/10.1002/sej.1285. Tierney, G., Bail, C.,   &   Volfovsky, A. (2021) Author Clustering and Topic Estimation for Short Texts.   Working paper. . Tracey, P., Dalpiaz, E., Phillips, N., 2018. Fish out of Water: Translation, Legitimation, and New Venture Creation. Acad. Manag. J. 61 (5), 1627 – 1666. https://doi.org/ 10.5465/amj.2015.0264. van Rijnsoever, F.J., 2020. Meeting, mating, and intermediating: How incubators can overcome weak network problems in entrepreneurial ecosystems. Res. Policy 49 (1), 103884. https://doi.org/10.1016/j.respol.2019.103884. Wang, F., Mack, E.A.,   &   Maciewjewski, R. (2017). Analyzing Entrepreneurial Social Networks with Big Data.   Annals of the American Association of Geographers , 107(1), 130 – 150. https://doi.org/10.1080/24694452.2016.1222263. Whitehead, L.A., Slovic, S.H., Nelson, J.E., 2020. Re-invigorating HIBAR research for the 21st century: Enhancing fundamental research excellence in service to society. Technol. Innovation 21 (2), 153 – 167. Wijffels, J., 2019. BTM: Biterm Topic Models for Short Text.   R package version 0.2.1. . https://cran.r-project.org/web/packages/BTM/. Wurth, B., Stam, E., Spigel, B., 2021. Toward an entrepreneurial ecosystem research program. Entrepreneurship Theory and Practice, 1042258721998948. Yan, X., Guo, J., Lan, Y., Cheng, X., 2013. A Biterm Topic Model Short Texts 11. Zhao, W.X., Jiang, J., Weng, J., He, J., Lim, E.-P., et al., 2011. Comparing Twitter and Traditional Media Using Topic Models. In: Clough, P., Foley, C., Gurrin, C., Jones, G. J.F., Kraaij, W., et al. (Eds.), Advances in Information Retrieval. Springer, Berlin, Heidelberg, pp. 338 – 349. Zilber, T.B., 2011. Institutional Multiplicity in Practice: A Tale of Two High-Tech Conferences in Israel. Org. Sci. 22 (6), 1539 – 1559. https://doi.org/10.1287/ orsc.1100.0611.  T.R. Hannigan et al.",
    "analysis": {
      "governance_power_accountability": "The algorithmic assemblage is positioned as a tool for policymakers, implying the power resides with governing institutions. The system's goals are defined by these policymakers with the aim of fostering economic growth. Interestingly, the accountability is somewhat obfuscated; while the system bears the potential risk of inaccurate or inappropriate policies, it also deflects responsibility to the 'visible markers' used to evaluate entrepreneurial ecosystems.",
      "plurality_inclusion_embodiment": "Despite its focus on mapping cultural possibilities, the system does not explicitly value diverse knowledge systems and embodied experiences. The document does not mention the inclusion of Indigenous, disability, or non-Western perspectives, nor does it challenge the 'default' user assumption. This could lead to a reinforcement of existing power dynamics, marginalizing underrepresented perspectives in the entrepreneurial ecosystem.",
      "agency_codesign_self_determination": "The system does not detail any mechanism for community agency, co-design, or the right to refuse. The use of the tool appears to be solely in the hands of policymakers, with no mention of input or feedback from the community it impacts. The absence of such mechanisms might undermine self-determination and impose external control.",
      "reflexivity_situated_praxis": "The document does not overtly display reflexivity or situated praxis. The authors do not examine their positionality, nor do they disclose their value assumptions or the history shaping their choices. However, they indicate an awareness of the structural inequities by suggesting the risk of policymakers inappropriately copying policies from other successful regions.",
      "legitimacy_claims": {
        "source": "Technocratic",
        "mechanisms": "Legitimacy is established by presenting the algorithmic assemblage as a solution to the 'difficult time making locally informed decisions' that policymakers face. It is maintained through its proposed utility in fostering economic growth.",
        "tensions": "The legitimacy claim is primarily technocratic, relying on the scientific authority and utility of the tool. There is a potential tension between this claim and a democratic legitimacy claim, which would require more participatory decision-making."
      },
      "key_insight": "The algorithmic assemblage, framed as a policymaking tool, centralizes power with technocratic institutions, potentially reinforcing existing inequalities and lacking in reflexivity.",
      "governance_scores": {
        "centralization": 80,
        "rights_focus": 30,
        "flexibility": 50,
        "market_power": 60,
        "procedurality": 70
      },
      "structural_pillars": {
        "risk": {
          "title": "Deferment of Accountability",
          "description": "The system acknowledges the risks of inappropriate policies but does not outline mitigation strategies.",
          "badge": "Deflective",
          "quote": "Consequently, policymakers may have a difficult time making informed decisions about incentives and regulations to foster economic growth through ecosystem emergence."
        },
        "enforcement": {
          "title": "Unilateral Decision-Making",
          "description": "The system enables policymaker-centered decision-making, potentially marginalizing community input.",
          "badge": "Authoritative",
          "quote": "We provide a new policy analytics tool and approach that helps address this issue for policymakers..."
        },
        "rights": {
          "title": "Neglect of Collective Rights",
          "description": "The system does not explicitly consider collective rights or varied cultural perspectives.",
          "badge": "Exclusionary",
          "quote": "Instead, policymakers are driven to inappropriately copy policies from other successful regions..."
        },
        "scope": {
          "title": "Broad but Unreflexive Scope",
          "description": "The system aims to address broad policy issues but lacks reflexivity about its own limitations and biases.",
          "badge": "Unexamined",
          "quote": "One reason emerging systems are difficult to study is that existing material resource and knowledge ties do not track closely with more radical innovations underpinning new systems..."
        }
      }
    },
    "cultural_framing": {
      "state_market_society": "The policy document assumes a governance philosophy where the state acts as a facilitator of economic growth and innovation, promoting the interests of the market and civil society. Policymakers are expected to provide the necessary incentives and regulations that foster the development and emergence of entrepreneurial ecosystems. The market, in this case, is seen as a dynamic field for innovation, entrepreneurship, and wealth creation, while civil society is portrayed as a potential beneficiary of the economic well-being and solutions to grand societal challenges that these ecosystems could deliver.",
      "technology_role": "In this document, technology is conceptualized as both a tool and infrastructure. As a tool, technology is seen as a means to enhance economic growth and societal well-being through artificial intelligence and digital technology. As infrastructure, technology provides the backbone for creating entrepreneurial ecosystems, which are seen as essential for systemic wealth creation, economic well-being, and addressing societal challenges.",
      "rights_conception": "The document does not directly address issues of rights conception. However, a subtle emphasis is placed on collective rights as it advocates for policies that foster economic growth, societal well-being, innovation, and entrepreneurship at the societal level. The rights conception appears to be more substantive, focusing on the outcomes of policy decisions.",
      "historical_context": "The document does not directly refer to any historical or colonial influences. However, the emphasis on entrepreneurial ecosystems, digital technology, and AI indicates its grounding in recent developments in global economic and technological trends. The policy document also reflects a response to the challenges of the digital age.",
      "epistemic_authority": "The epistemic authority in this policy document is largely academic and technocratic. The authors draw on existing research to make their arguments, suggesting an emphasis on scientific and scholarly knowledge. Policymakers are encouraged to use these academic insights to make informed decisions that foster the growth of entrepreneurial ecosystems. This establishes a legitimacy dynamic where technocratic and scientific knowledge is privileged over other forms of knowledge.",
      "cultural_distinctiveness_score": 0.2,
      "dominant_cultural_logic": "Technocratic universalism"
    },
    "institutional_logics": {
      "logics": {
        "market": {
          "strength": 0.7,
          "champions": [
            "Policymakers",
            "Entrepreneurs",
            "Startups"
          ],
          "material": "Visible material metrics like new products, patents, VC funding, jobs, and successful exits are used to evaluate and understand ecosystems.",
          "discursive": "Economic growth, innovation, and systemic wealth creation are emphasised, highlighting the market logic.",
          "key_tensions": [
            "Tensions with state and community logics due to lack of localized, democratic decision-making and participation"
          ]
        },
        "state": {
          "strength": 0.5,
          "champions": [
            "Policymakers"
          ],
          "material": "Discussion of incentives and regulations suggests the presence of state logic. Policymakers can directly shape the context, indicating regulatory control.",
          "discursive": "The state logic is evident in discussions of grand challenges facing society and public interest.",
          "key_tensions": [
            "Tensions with market logic due to different focus areas and goals"
          ]
        },
        "professional": {
          "strength": 0.6,
          "champions": [
            "Researchers"
          ],
          "material": "Use of technical methods like topic modeling of Twitter feeds, and focus on expertise like AI, suggests professional logic.",
          "discursive": "The paper emphasizes expertise, technical standards, and professional autonomy.",
          "key_tensions": [
            "Tensions with market and community logics due to different focus areas and goals"
          ]
        },
        "community": {
          "strength": 0.4,
          "champions": [
            "Local cultural possibilities"
          ],
          "material": "Community logic is reflected in discussions about local macro entrepreneurial context.",
          "discursive": "Refers to cultural mindsets of entrepreneurs and the importance of local knowledge.",
          "key_tensions": [
            "Tensions with market logic due to different focus areas and goals"
          ]
        }
      },
      "dominant_logic": "market",
      "logic_conflicts": [
        {
          "between": "market and state",
          "site_of_conflict": "Focus on economic growth versus public interest",
          "resolution_strategy": "Mostly unresolved in this text"
        },
        {
          "between": "market and community",
          "site_of_conflict": "Focus on growth and efficiency versus local knowledge and participation",
          "resolution_strategy": "Unresolved, but highlights the need for more localized approach"
        },
        {
          "between": "professional and market",
          "site_of_conflict": "Technical expertise and standards versus economic metrics and growth",
          "resolution_strategy": "Unresolved, but mentions the importance of both"
        }
      ],
      "overall_assessment": "This document exhibits a complex interplay of logics, with market logic being dominant. The text highlights tensions between market, state, community, and professional logics, with unresolved conflicts. The authors emphasize the need for a more integrated approach that takes into account cultural and material aspects of ecosystems."
    }
  },
  {
    "id": "1763873417256",
    "title": "Americas-AI-Action-Plan",
    "description": "Uploaded 11/22/2025",
    "type": "PDF",
    "addedDate": "11/22/2025",
    "status": "Active Case",
    "colorClass": "bg-purple-100",
    "iconClass": "text-purple-600",
    "extractedText": "--- Page 1 ---\n\nWinning   the   Race  AMERICA’S AI ACTION PLAN  JULY 2025  T H E   W H I T E   H O U S E\n\n--- Page 2 ---\n\nAMERICA’S AI ACTION PLAN  i  “Today, a new frontier of scientific discovery lies before us, defined by transformative technologies such as artificial intelligence… Breakthroughs in these fields have the potential to reshape the global balance of power, spark entirely new industries, and revolutionize the way we live and work. As our global competitors race to exploit these technologies, it is a national security imperative for the United States to achieve and   maintain   unquestioned   and   unchallenged   global technological dominance. To secure our future, we must harness the full power of American innovation.”  Donald J. Trump  45   th   and 47 th   President of the United States\n\n--- Page 3 ---\n\nAMERICA’S AI ACTION PLAN  ii  Table of Contents  Introduction   ................................ ................................ ................................ ..............................   1  Pillar I: Accelerate AI Innovation   ................................ ................................ .............................. 3  Remove Red Tape and Onerous Regulation ....................................................................................   3  Ensure that Frontier AI Protects Free Speech and American Values   .........................................   4  Encourage Open-Source and Open-Weight AI .............................................................................   4  Enable AI Adoption ...............................................................................................................................   5  Empower American Workers in the Age of AI.................................................................................   6  Support Next-Generation Manufacturing ....................................................................................... 7  Invest in AI-Enabled Science ..............................................................................................................   8  Build World-Class Scientific Datasets ..............................................................................................   8  Advance the Science of AI   ...................................................................................................................   9  Invest in AI Interpretability, Control, and Robustness Breakthroughs........................................   9  Build an AI Evaluations Ecosystem .................................................................................................. 10  Accelerate AI Adoption in Government ......................................................................................... 10  Drive Adoption of AI within the Department of Defense ............................................................. 11  Protect Commercial and Government AI Innovations................................................................. 12  Combat Synthetic Media in the Legal System .............................................................................. 12  Pillar II: Build American AI Infrastructure   ................................ ................................ ..............   14  Create Streamlined Permitting for Data Centers, Semiconductor Manufacturing Facilities, and Energy Infrastructure while Guaranteeing Security ..................................... 14  Develop a Grid to Match the Pace of AI Innovation ...................................................................... 15  Restore American Semiconductor Manufacturing ...................................................................... 16  Build High-Security Data Centers for Military and Intelligence Community Usage .............. 16  Train a Skilled Workforce for AI Infrastructure.............................................................................. 17  Bolster Critical Infrastructure Cybersecurity   ................................................................................. 18  Promote Secure-By-Design AI Technologies and Applications ............................................... 18  Promote Mature Federal Capacity for AI Incident Response ..................................................... 19  Pillar III: Lead in International AI Diplomacy and Security   ................................ ...................   20  Export American AI to Allies and Partners .................................................................................... 20  Counter Chinese Influence in International Governance Bodies   .............................................. 20  Strengthen AI Compute Export Control Enforcement ............................................................... 21  Plug Loopholes in Existing Semiconductor Manufacturing Export Controls ......................... 21  Align Protection Measures Globally ................................................................................................ 21  Ensure that the U.S. Government is at the Forefront of Evaluating National Security Risks in Frontier Models ...............................................................................................................   22  Invest in Biosecurity ............................................................................................................................   23\n\n--- Page 4 ---\n\nAMERICA’S AI ACTION PLAN  1  Introduction  The United States is in a race to achieve global dominance in artificial intelligence (AI). Whoever has the largest AI ecosystem will set global AI standards and reap broad economic and military benefits. Just like we won the space race, it is imperative that the United States and its allies win this race. President Trump took decisive steps toward achieving this goal during his first days in office by signing Executive Order 14179, “Removing Barriers to American Leadership in Artificial Intelligence,” calling for America to retain dominance in this global race and directing the creation of an AI Action Plan. 1  Winning the AI race will usher in a new golden age of human flourishing, economic competitiveness, and national security for the American people. AI will enable Americans to discover new materials, synthesize new chemicals, manufacture new drugs, and develop new methods to harness energy—an industrial revolution. It will enable radically new forms of education,   media,   and   communication—an   information   revolution.   And   it   will   enable altogether   new   intellectual   achievements:   unraveling   ancient   scrolls   once   thought unreadable, making breakthroughs in scientific and mathematical theory, and creating new kinds of digital and physical art—a   renaissance .  An industrial revolution, an information revolution, and a renaissance—all at once. This is the potential that AI presents. The opportunity that stands before us is both inspiring and humbling. And it is ours to seize, or to lose.  America’s AI Action Plan has three pillars: innovation, infrastructure, and international diplomacy   and   security.   The   United   States   needs   to   innovate   faster   and   more comprehensively than our competitors in the development and distribution of new AI technology across every field, and dismantle unnecessary regulatory barriers that hinder the private sector in doing so. As Vice President Vance remarked at the Paris AI Action Summit in February, restricting AI development with onerous regulation “would not only unfairly benefit incumbents… it would mean paralyzing one of the most promising technologies we have seen in   generations.” 2   That   is   why   President   Trump   rescinded   the   Biden   Administration’s dangerous actions on day one.  We need to build and maintain vast AI infrastructure and the energy to power it. To do that, we will continue to reject radical climate dogma and bureaucratic red tape, as the Administration has done since Inauguration Day. Simply put, we need to “Build, Baby, Build!”  We need to establish American AI—from our advanced semiconductors to our models to our applications—as the gold standard for AI worldwide and ensure our allies are building on American technology.  Several principles cut across each of these three pillars. First, American workers are central to the Trump Administration’s AI policy. The Administration will ensure that our Nation’s workers and their families gain from the opportunities created in this technological revolution. The AI infrastructure   buildout   will   create   high-paying   jobs   for   American   workers.   And   the  1   Executive Order 14179 of January 23, 2025, “Removing Barriers to American Leadership in Artificial Intelligence ,”   Federal Register 90 (20) 8741,   www.govinfo.gov/content/pkg/FR-2025-01-31/pdf/2025-02172.pdf .  2   J.D. Vance, “Remarks by the Vice President at the Artificial Intelligence Action Summit in Paris, France,” February 11, 2025,  www.presidency.ucsb.edu/documents/remarks-the-vice-president-the-artificial-intelligence-action-summit-paris-france .\n\n--- Page 5 ---\n\nAMERICA’S AI ACTION PLAN  2  breakthroughs in medicine, manufacturing, and many other fields that AI will make possible will increase the standard of living for all Americans. AI will improve the lives of Americans by complementing their work—not replacing it.  Second, our AI systems must be free from ideological bias and be designed to pursue objective truth rather than social engineering agendas when users seek factual information or analysis. AI systems are becoming essential tools, profoundly shaping how Americans consume information, but these tools must also be trustworthy.  Finally, we must prevent our advanced technologies from being misused or stolen by malicious actors as well as monitor for emerging and unforeseen risks from AI. Doing so will require constant vigilance.  This Action Plan sets forth clear policy goals for near-term execution by the Federal government. The Action Plan’s objective is to articulate policy recommendations that this Administration can deliver for the American people to achieve the President’s vision of global AI dominance. The AI race is America’s to win, and this Action Plan is our roadmap to victory.  Michael J. Kratsios Assistant to the President for Science and Technology  David O. Sacks Special Advisor for AI and Crypto  Marco A. Rubio Assistant to the President for National Security Affairs\n\n--- Page 6 ---\n\nAMERICA’S AI ACTION PLAN  3  Pillar I: Accelerate AI Innovation  America must have the most powerful AI systems in the world, but we must also lead the world in creative and transformative application of these systems. Achieving these goals requires the Federal government to create the conditions where private-sector-led innovation can flourish.  Remove Red Tape and Onerous Regulation  To maintain global leadership in AI, America’s private sector must be unencumbered by bureaucratic red tape. President Trump has already taken multiple steps toward this goal, including rescinding Biden Executive Order 14110 on AI that foreshadowed an onerous regulatory regime. 3   AI is far too important to smother in bureaucracy at this early stage, whether at the state or Federal level. The Federal government should not allow AI-related Federal funding to be directed toward states with burdensome AI regulations that waste these funds, but should also not interfere with states’ rights to pass prudent laws that are not unduly restrictive to innovation.  Recommended Policy Actions  •   Led by the Office of Science and Technology Policy (OSTP), launch a Request for Information from businesses and the public at large about current Federal regulations that hinder AI innovation and adoption, and work with relevant Federal agencies to take appropriate action.  •   Led by the Office of Management and Budget (OMB) and consistent with Executive Order 14192 of January 31, 2025, “Unleashing Prosperity Through Deregulation,” work with all Federal agencies to identify, revise, or repeal regulations, rules, memoranda, administrative   orders,   guidance   documents,   policy   statements,   and   interagency agreements that unnecessarily hinder AI development or deployment. 4  •   Led by OMB, work with Federal agencies that have AI-related discretionary funding programs to ensure, consistent with applicable law, that they consider a state’s AI regulatory climate when making funding decisions and limit funding if the state’s AI regulatory regimes may hinder the effectiveness of that funding or award.  •   Led by the Federal Communications Commission (FCC), evaluate whether state AI regulations interfere with the agency’s ability to carry out its obligations and authorities under the Communications Act of 1934. 5  •   Review all Federal Trade Commission (FTC) investigations commenced under the previous administration to ensure that they do not advance theories of liability that unduly burden AI innovation. Furthermore, review all FTC final orders, consent decrees,  3   Executive Order 14110 of October 30, 2023, “Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence,” Federal Register 88 (210) 75191,   www.govinfo.gov/content/pkg/FR-2023-11-01/pdf/2023-24283.pdf .  4   Executive Order 14192 of January 31, 2025, “Unleashing Prosperity Through Deregulation,” Federal Register 90 (24) 9065,  www.govinfo.gov/content/pkg/FR-2025-02-06/pdf/2025-02345.pdf .  5   Communications Act of 1934, 47 U.S.C. §§ 151-646.\n\n--- Page 7 ---\n\nAMERICA’S AI ACTION PLAN  4  and injunctions, and, where appropriate, seek to modify or set-aside any that unduly burden AI innovation.  Ensure that Frontier AI Protects Free Speech and American Values  AI systems will play a profound role in how we educate our children, do our jobs, and consume media. It is essential that these systems be built from the ground up with freedom of speech and expression in mind, and that U.S. government policy does not interfere with that objective. We must ensure that free speech flourishes in the era of AI and that AI procured by the Federal government objectively reflects truth rather than social engineering agendas.  Recommended Policy Actions  •   Led by the Department of Commerce (DOC) through the National Institute of Standards and Technology (NIST), revise the NIST AI Risk Management Framework to eliminate references to misinformation, Diversity, Equity, and Inclusion, and climate change.   6  •   Update Federal procurement guidelines to ensure that the government only contracts with frontier large language model (LLM) developers who ensure that their systems are objective and free from top-down ideological bias.  •   Led by DOC through NIST’s Center for AI Standards and Innovation (CAISI), conduct research and, as appropriate, publish evaluations of frontier models from the People’s Republic of China for alignment with Chinese Communist Party talking points and censorship.  Encourage Open-Source and Open-Weight AI  Open-source and open-weight AI models are made freely available by developers for anyone in the world to download and modify. Models distributed this way have unique value for innovation because startups can use them flexibly without being dependent on a closed model provider. They also benefit commercial and government adoption of AI because many businesses and governments have sensitive data that they cannot send to closed model vendors. And they are essential for academic research, which often relies on access to the weights and training data of a model to perform scientifically rigorous experiments.  We need to ensure America has leading open models founded on American values. Open- source and open-weight models could become global standards in some areas of business and in academic research worldwide. For that reason, they also have geostrategic value. While the decision of whether and how to release an open or closed model is fundamentally up to the developer, the Federal government should create a supportive environment for open models.  Recommended Policy Actions  •   Ensure   access   to   large-scale   computing   power   for   startups   and   academics   by improving the financial market for compute. Currently, a company seeking to use large- scale compute must often sign long-term contracts with hyperscalers—far beyond the  6   National Institute of Standards and Technology, “Artificial Intelligence Risk Management Framework (AI RMF 1.0),” (Gaithersburg, MD: National Institute of Standards and Technology, 2023),   www.doi.org/10.6028/NIST.AI.100-1 .\n\n--- Page 8 ---\n\nAMERICA’S AI ACTION PLAN  5  budgetary reach of most academics and many startups. America has solved this problem before with other goods through financial markets, such as spot and forward markets for commodities. Through collaboration with industry, NIST at DOC, OSTP, and the National Science Foundation’s (NSF) National AI Research Resource (NAIRR) pilot, the Federal government can accelerate the maturation of a healthy financial market for compute.  •   Partner with leading technology companies to increase the research community’s access to world-class private sector computing, models, data, and software resources as part of the NAIRR pilot.  •   Build the foundations for a lean and sustainable NAIRR operations capability that can connect an increasing number of researchers and educators across the country to critical AI resources.  •   Continue to foster the next generation of AI breakthroughs by publishing a new National AI Research and Development (R&D) Strategic Plan, led by OSTP, to guide Federal AI research investments.  •   Led by DOC through the National Telecommunications and Information Administration (NTIA), convene stakeholders to help drive adoption of open-source and open-weight models by small and medium-sized businesses.  Enable AI Adoption  Today, the bottleneck to harnessing AI’s full potential is not necessarily the availability of models, tools, or applications. Rather, it is the limited and slow adoption of AI, particularly within large, established organizations. Many of America’s most critical sectors, such as healthcare, are especially slow to adopt due to a variety of factors, including distrust or lack of understanding of the technology, a complex regulatory landscape, and a lack of clear governance and risk mitigation standards. A coordinated Federal effort would be beneficial in establishing a dynamic, “try-first” culture for AI across American industry.  Recommended Policy Actions  •   Establish regulatory sandboxes or AI Centers of Excellence around the country where researchers, startups, and established enterprises can rapidly deploy and test AI tools while committing to open sharing of data and results. These efforts would be enabled by regulatory agencies such as the Food and Drug Administration (FDA) and the Securities and Exchange Commission (SEC), with support from DOC through its AI evaluation initiatives at NIST.  •   Launch several domain-specific efforts (e.g., in healthcare, energy, and agriculture), led by NIST at DOC, to convene a broad range of public, private, and academic stakeholders to accelerate the development and adoption of national standards for AI systems and to measure how much AI increases productivity at realistic tasks in those domains.  •   Led by the Department of Defense (DOD) in coordination with the Office of the Director of National Intelligence (ODNI), regularly update joint DOD-Intelligence Community (IC) assessments of the comparative level of adoption of AI tools by the United States, its competitors, and its adversaries’ national security establishments, and establish an\n\n--- Page 9 ---\n\nAMERICA’S AI ACTION PLAN  6  approach for continuous adaptation of the DOD and IC’s respective AI adoption initiatives based on these AI net assessments.  •   Prioritize, collect, and distribute intelligence on foreign frontier AI projects that may have national security implications, via collaboration between the IC, the Department of Energy (DOE), CAISI at DOC, the National Security Council (NSC), and OSTP.  Empower American Workers in the Age of AI  The Trump Administration supports a worker-first AI agenda. By accelerating productivity and creating entirely new industries, AI can help America build an economy that delivers more pathways to economic opportunity for American workers. But it will also transform how work gets done across all industries and occupations, demanding a serious workforce response to help workers navigate that transition. The Trump Administration has already taken significant steps to lead on this front, including the April 2025 Executive Orders 14277 and 14278, “Advancing Artificial Intelligence Education for American Youth” and “Preparing Americans for High-Paying Skilled Trade Jobs of the Future.” 7 ,   8   To continue delivering on this vision, the Trump Administration will advance a priority set of actions to expand AI literacy and skills development,   continuously   evaluate   AI’s   impact   on   the   labor   market,   and   pilot   new innovations to rapidly retrain and help workers thrive in an AI-driven economy.  Recommended Policy Actions  •   Led by the Department of Labor (DOL), the Department of Education (ED), NSF, and DOC, prioritize AI skill development as a core objective of relevant education and workforce funding streams. This should include promoting the integration of AI skill development into relevant programs, including career and technical education (CTE), workforce training, apprenticeships, and other federally supported skills initiatives.  •   Led by the Department of the Treasury, issue guidance clarifying that many AI literacy and AI skill development programs may qualify as eligible educational assistance under Section 132 of the Internal Revenue Code, given AI’s widespread impact reshaping the tasks and skills required across industries and occupations. 9   In applicable situations, this will enable employers to offer tax-free reimbursement for AI-related training and help scale private-sector investment in AI skill development, preserving jobs for American workers.  •   Led by the Bureau of Labor Statistics (BLS) and DOC through the Census Bureau and the Bureau of Economic Analysis (BEA), study AI’s impact on the labor market by using data they already collect on these topics, such as the firm-level AI adoption trends the Census Bureau tracks in its Business Trends and Outlook Survey. These agencies could then provide analysis of AI adoption, job creation, displacement, and wage effects.  •   Establish the AI Workforce Research Hub under DOL to lead a sustained Federal effort to evaluate the impact of AI on the labor market and the experience of the American  7   Executive Order 14277 of April 23, 2025: “Advancing Artificial Intelligence Education for American Youth,” Federal Register 90 (80) 17519,   www.govinfo.gov/content/pkg/FR-2025-04-28/pdf/2025-07368.pdf .  8   Executive Order 14278 of April 23, 2025: “Preparing Americans for High-Paying Skilled Trade Jobs of the Future,” Federal Register 90 (80) 17525,   www.govinfo.gov/content/pkg/FR-2025-04-28/pdf/2025-07369.pdf .  9   Revenue Act of 1978, 26 U.S.C. § 132.\n\n--- Page 10 ---\n\nAMERICA’S AI ACTION PLAN  7  worker, in collaboration with BLS and DOC through the Census Bureau and BEA. The Hub would produce recurring analyses, conduct scenario planning for a range of potential AI impact levels, and generate actionable insights to inform workforce and education policy.  •   Led by DOL, leverage available discretionary funding, where appropriate, to fund rapid retraining for individuals impacted by AI-related job displacement. Issue clarifying guidance to help states identify eligible dislocated workers in sectors undergoing significant structural change tied to AI adoption, as well as guidance clarifying how state Rapid Response funds can be used to proactively upskill workers at risk of future displacement.  •   At DOL and DOC, rapidly pilot new approaches to workforce challenges created by AI, which may include areas such as rapid retraining needs driven by worker displacement and shifting skill requirements for entry-level roles. These pilots should be carried out by states and workforce intermediaries using existing authorities under the Workforce Innovation and Opportunity Act and the Public Works and Economic Development Act, and should be designed to identify surface scalable, performance-driven strategies that help the workforce system adapt to the speed and complexity of AI-driven labor market change.   10 ,   11  Support Next-Generation Manufacturing  AI will enable a wide range of new innovations in the physical world: autonomous drones, self- driving cars, robotics, and other inventions for which terminology does not yet exist. It is crucial that America and our trusted allies be world-class manufacturers of these next-generation technologies. AI, robotics, and related technologies create opportunities for novel capabilities in manufacturing and logistics, including ones with applications to defense and national security.   The   Federal   government   should   prioritize   investment   in   these   emerging technologies and usher in a new industrial renaissance.  Recommended Policy Actions  •   Invest   in   developing   and   scaling   foundational   and   translational   manufacturing technologies via DOD, DOC, DOE, NSF, and other Federal agencies using the Small Business Innovation Research program, the Small Business Technology Transfer program, research grants, CHIPS R&D programs, Stevenson-Wydler Technology Innovation Act authorities, Title III of the Defense Production Act, Other Transaction Authority, and other authorities. 12 ,   13 ,   14 ,   15  •   Led by DOC through NTIA, convene industry and government stakeholders to identify supply chain challenges to American robotics and drone manufacturing.  10   Workforce Innovation and Opportunity Act of 2014, 29 U.S.C. §§ 3101-3361 .  11   Public Works and Economic Development Act of 1965,   42 U.S.C. §§ 3121-3233.  12   William M. (Mac) Thornberry National Defense Authorization Act for Fiscal Year 2021, 15 U.S.C. § 4656.  13   Stevenson-Wydler Technology Innovation Act of 1980, Pub. L. No. 96-480, 94 Stat. 2311 (codified as amended in scattered sections of 15 U.S.C.).  14   Defense Production Act of 1950, 50 U.S.C. §§ 4551-4568.  15   National Defense Authorization Act for Fiscal years 1990 and 1991, 10 U.S.C. §§ 4021-4022.\n\n--- Page 11 ---\n\nAMERICA’S AI ACTION PLAN  8  Invest in AI-Enabled Science  Like many other domains, science itself will be transformed by AI. AI systems can already generate models of protein structures, novel materials, and much else. Increasingly powerful general-purpose   models   show   promise   in   formulating   hypotheses   and   designing experiments. These nascent capabilities promise to accelerate scientific advancement. They will only do so, however, with critical changes in the way science is conducted, including the enabling scientific infrastructure. AI-enabled predictions are of little use if scientists cannot also increase the scale of experimentation. Basic science today is often a labor-intensive process; the AI era will require more scientific and engineering research to transform theories into industrial-scale enterprises. This, in turn, will necessitate new infrastructure and support of new kinds of scientific organizations.  Recommended Policy Actions  •   Through NSF, DOE, NIST at DOC, and other Federal partners, invest in automated cloud-enabled labs for a range of scientific fields, including engineering, materials science, chemistry, biology, and neuroscience, built by, as appropriate, the private sector, Federal agencies, and research institutions in coordination and collaboration with DOE National Laboratories.  •   Use long-term agreements to support Focused-Research Organizations or other similar entities using AI and other emerging technologies to make fundamental scientific advancements.  •   Incentivize researchers to release more high-quality datasets publicly by considering the impact of scientific and engineering datasets from a researchers’ prior funded efforts in the review of proposals for new projects.  •   Require   federally   funded   researchers   to   disclose   non-proprietary,   non-sensitive datasets that are used by AI models during the course of research and experimentation.  Build World-Class Scientific Datasets  High-quality data has become a national strategic asset as governments pursue AI innovation goals and capitalize on the technology’s economic benefits. Other countries, including our adversaries, have raced ahead of us in amassing vast troves of scientific data. The United States must lead the creation of the world’s largest and highest quality AI-ready scientific datasets, while maintaining respect for individual rights and ensuring civil liberties, privacy, and confidentiality protections.  Recommended Policy Actions  •   Direct the National Science and Technology Council (NSTC) Machine Learning and AI Subcommittee to make recommendations on minimum data quality standards for the use of biological, materials science, chemical, physical, and other scientific data modalities in AI model training.  •   Promulgate the OMB regulations required in the Confidential Information Protection and Statistical Efficiency Act of 2018 on presumption of accessibility and expanding secure access, which will lower barriers and break down silos to accessing Federal data,\n\n--- Page 12 ---\n\nAMERICA’S AI ACTION PLAN  9  ultimately facilitating the improved use of AI for evidence building by statistical agencies while protecting confidential data from inappropriate access and use. 16  •   Establish secure compute environments within NSF and DOE to enable secure AI use- cases for controlled access to restricted Federal data.  •   Create an online portal for NSF’s National Secure Data Service (NSDS) demonstration project to provide the public and Federal agencies with a front door to AI use-cases involving controlled access to restricted Federal data.  •   Explore the creation of a whole-genome sequencing program for life on Federal lands, led by the NSTC and including members of the U.S. Department of Agriculture, DOE, NIH, NSF, the Department of Interior, and Cooperative Ecosystem Studies Units to collaborate on the development of an initiative to establish a whole genome sequencing program for life on Federal lands (to include all biological domains). This new data would be a valuable resource in training future biological foundation models.  Advance the Science of AI  Just as LLMs and generative AI systems represented a paradigm shift in the science of AI, future breakthroughs may similarly transform what is possible with AI. It is imperative that the United States remain the leading pioneer of such breakthroughs, and this begins with strategic, targeted investment in the most promising paths at the frontier.  Recommended Policy Actions  •   Prioritize investment in theoretical, computational, and experimental research to preserve America’s leadership in discovering new and transformative paradigms that advance the capabilities of AI, reflecting this priority in the forthcoming National AI R&D Strategic Plan.  Invest in AI Interpretability, Control, and Robustness Breakthroughs  Today, the inner workings of frontier AI systems are poorly understood. Technologists know how LLMs work at a high level, but often cannot explain why a model produced a specific output. This can make it hard to predict the behavior of any specific AI system. This lack of predictability, in turn, can make it challenging to use advanced AI in defense, national security, or other applications where lives are at stake. The United States will be better able to use AI systems to their fullest potential in high-stakes national security domains if we make fundamental breakthroughs on these research problems.  Recommended Policy Actions  •   Launch a technology development program led by the Defense Advanced Research Projects   Agency   in   collaboration   with   CAISI   at   DOC   and   NSF,   to   advance   AI interpretability, AI control systems, and adversarial robustness.  16   Confidential Information Protection and Statistical Efficiency Act of 2018, 44 U.S.C. §§ 3561-3583.\n\n--- Page 13 ---\n\nAMERICA’S AI ACTION PLAN  10  •   Prioritize fundamental advancements in AI interpretability, control, and robustness as part of the forthcoming National AI R&D Strategic Plan.  •   The DOD, DOE, CAISI at DOC, the Department of Homeland Security (DHS), NSF, and academic partners should coordinate an AI hackathon initiative to solicit the best and brightest from U.S. academia to test AI systems for transparency, effectiveness, use control, and security vulnerabilities.  Build an AI Evaluations Ecosystem  Evaluations are how the AI industry assesses the performance and reliability of AI systems. Rigorous evaluations can be a critical tool in defining and measuring AI reliability and performance in regulated industries. Over time, regulators should explore the use of evaluations in their application of existing law to AI systems.  Recommended Policy Actions  •   Publish guidelines and resources through NIST at DOC, including CAISI, for Federal agencies to conduct their own evaluations of AI systems for their distinct missions and operations and for compliance with existing law.  •   Support the development of the science of measuring and evaluating AI models, led by NIST at DOC, DOE, NSF, and other Federal science agencies.  •   Convene meetings at least twice per year under the auspices of CAISI at DOC for Federal agencies and the research community to share learnings and best practices on building AI evaluations.  •   Invest, via DOE and NSF, in the development of AI testbeds for piloting AI systems in secure, real-world settings, allowing researchers to prototype new AI systems and translate them to the market. Such testbeds would encourage participation by broad multistakeholder teams and span a wide variety of economic verticals touched by AI, including agriculture, transportation, and healthcare delivery.  •   Led   by   DOC,   convene   the   NIST   AI   Consortium   to   empower   the   collaborative establishment of new measurement science that will enable the identification of proven,   scalable,   and   interoperable   techniques   and   metrics   to   promote   the development of AI.  Accelerate AI Adoption in Government  With AI tools in use, the Federal government can serve the public with far greater efficiency and effectiveness. Use cases include accelerating slow and often manual internal processes, streamlining public interactions, and many others. Taken together, transformative use of AI can help deliver the highly responsive government the American people expect and deserve.\n\n--- Page 14 ---\n\nAMERICA’S AI ACTION PLAN  11  OMB has already advanced AI adoption in government by reducing onerous rules imposed by the Biden Administration. 17 ,   18   Now is the time to build on this success.  Recommended Policy Actions  •   Formalize the Chief Artificial Intelligence Officer Council (CAIOC) as the primary venue for interagency coordination and collaboration on AI adoption. Through the CAIOC, initiate   strategic   coordination   and   collaboration   with   relevant   Federal   executive councils, to include: the President’s Management Council, Chief Data Officer Council, Chief Information Officer Council, Interagency Council on Statistical Policy, Chief Human Capital Officer Council, and Federal Privacy Council.  •   Create a talent-exchange program designed to allow rapid details of Federal staff to other agencies in need of specialized AI talent (e.g., data scientists and software engineers), with input from the Office of Personnel Management.  •   Create an AI procurement toolbox managed by the General Services Administration (GSA), in coordination with OMB, that   facilitates   uniformity across   the Federal enterprise to the greatest extent practicable. This system would allow any Federal agency to easily choose among multiple models in a manner compliant with relevant privacy, data governance, and transparency laws. Agencies should also have ample flexibility to customize models to their own ends, as well as to see a catalog of other agency AI uses (based on OMB’s pre-existing AI Use Case Inventory).  •   Implement an Advanced Technology Transfer and Capability Sharing Program with GSA to quickly transfer advanced AI capabilities and use cases between agencies.  •   Mandate that all Federal agencies ensure—to the maximum extent practicable—that all employees whose work could benefit from access to frontier language models have access to, and appropriate training for, such tools.  •   Convene, under the auspices of OMB, a cohort of agencies with High Impact Service Providers to pilot and increase the use of AI to improve the delivery of services to the public.  Drive Adoption of AI within the Department of Defense  AI has the potential to transform both the warfighting and back-office operations of the DOD. The United States must aggressively adopt AI within its Armed Forces if it is to maintain its global military preeminence while also ensuring, as outlined throughout this Action Plan, that its use of AI is secure and reliable. Because the DOD has unique operational needs within the Federal government, it merits specific policy actions to drive AI adoption.  17   Office of Management and Budget, “Accelerating Federal Use of AI through Innovation, Governance, and Public Trust (M-25- 21),” (Washington, DC: Executive Office of the President, 2025),   www.whitehouse.gov/wp-content/uploads/2025/02/M-25- 21-Accelerating-Federal-Use-of-AI-through-Innovation-Governance-and-Public-Trust.pdf .  18   Office of Management and Budget, “Driving Efficient Acquisition of Artificial Intelligence in Government (M-25 22),” (Washington, DC: Executive Office of the President, 2025),   www.whitehouse.gov/wp-content/uploads/2025/02/M-25-22- Driving-Efficient-Acquisition-of-Artificial-Intelligence-in-Government.pdf .\n\n--- Page 15 ---\n\nAMERICA’S AI ACTION PLAN  12  Recommended Policy Actions  •   Identify the talent and skills DOD’s workforce requires to leverage AI at scale. Based on this identification, implement talent development programs to meet AI workforce requirements and drive the effective employment of AI-enabled capabilities.  •   Establish an AI & Autonomous Systems Virtual Proving Ground at DOD, beginning with scoping the technical, geographic, security, and resourcing requirements necessary for such a facility.  •   Develop a streamlined process within DOD for classifying, evaluating, and optimizing workflows involved in its major operational and enabling functions, aiming to develop a list of priority workflows for automation with AI. When a workflow is successfully automated, DOD should strive to permanently transition that workflow to the AI-based implementation as quickly as practicable.  •   Prioritize DOD-led agreements with cloud service providers, operators of computing infrastructure, and other relevant private sector entities to codify priority access to computing resources in the event of a national emergency so that DOD is prepared to fully leverage these technologies during a significant conflict.  •   Grow our Senior Military Colleges into hubs of AI research, development, and talent building, teaching core AI skills and literacy to future generations. Foster AI-specific curriculum, including in AI use, development, and infrastructure management, in the Senior Military Colleges throughout majors.  Protect Commercial and Government AI Innovations  Maintaining American leadership in AI necessitates that the U.S. government work closely with industry to appropriately balance the dissemination of cutting-edge AI technologies with national security concerns. It is also essential for the U.S. government to effectively address security risks to American AI companies, talent, intellectual property, and systems.  Recommended Policy Actions  •   Led by DOD, DHS, CAISI at DOC, and other appropriate members of the IC, collaborate with leading American AI developers to enable the private sector to actively protect AI innovations from security risks, including malicious cyber actors, insider threats, and others.  Combat Synthetic Media in the Legal System  One risk of AI that has become apparent to many Americans is malicious deepfakes, whether they be audio recordings, videos, or photos. While President Trump has already signed the TAKE IT DOWN Act, which was championed by First Lady Melania Trump and intended to protect against sexually explicit, non-consensual deepfakes, additional action is needed.   19   In particular, AI-generated media may present novel challenges to the legal system. For example, fake evidence could be used to attempt to deny justice to both plaintiffs and  19   TAKE IT DOWN Act, Pub. L. No. 119-12, 139 Stat. 55 (2025) (codified as 47 U.S.C. § 223(h)).\n\n--- Page 16 ---\n\nAMERICA’S AI ACTION PLAN  13  defendants. The Administration must give the courts and law enforcement the tools they need to overcome these new challenges.  Recommended Policy Actions  •   Led by NIST at DOC, consider developing NIST’s   Guardians of Forensic Evidence  deepfake evaluation program into a formal guideline and a companion voluntary forensic benchmark. 20  •   Led by the Department of Justice (DOJ), issue guidance to agencies that engage in adjudications to explore adopting a deepfake standard similar to the proposed Federal Rules of Evidence Rule 901(c) under consideration by the Advisory Committee on Evidence Rules.  •   Led by DOJ’s Office of Legal Policy, file formal comments on any proposed deepfake- related additions to the Federal Rules of Evidence.  20   Haiying Guan, James Horan, and Andrew Zhang, “Guardians of Forensic Evidence: Evaluating Analytic Systems Against AI- Generated Deepfakes,” (Gaithersburg, MD: National Institute of Standards and Technology, January 27, 2025),  www.nist.gov/publications/guardians-forensic-evidence-evaluating-analytic-systems-against-ai-generated-deepfakes .\n\n--- Page 17 ---\n\nAMERICA’S AI ACTION PLAN  14  Pillar II: Build American AI Infrastructure  AI is the first digital service in modern life that challenges America to build vastly greater energy generation than we have today. American energy capacity has stagnated since the 1970s while China has rapidly built out their grid. America’s path to AI dominance depends on changing this troubling trend.  Create Streamlined Permitting for Data Centers, Semiconductor Manufacturing Facilities, and Energy Infrastructure while Guaranteeing Security  Like most general-purpose technologies of the past, AI will require new infrastructure— factories to produce chips, data centers to run those chips, and new sources of energy to power it all. America’s environmental permitting system and other regulations make it almost impossible to build this infrastructure in the United States with the speed that is required. Additionally, this infrastructure must also not be built with any adversarial technology that could undermine U.S. AI dominance.  Fortunately, the Trump Administration has made unprecedented progress in reforming this system. Since taking office, President Trump has already reformed National Environmental Policy Act (NEPA) regulations across almost every relevant Federal agency, jumpstarted a permitting technology modernization program, created the National Energy Dominance Council (NEDC), and launched the United States Investment Accelerator. 21 ,   22 ,   23 ,   24   Now is the time to build on that momentum.  Recommended Policy Actions  •   Establish new Categorical Exclusions under NEPA to cover data center-related actions that normally do not have a significant effect on the environment. Where possible, adopt Categorical Exclusions already established by other agencies so that each relevant agency can proceed with maximum efficiency.  •   Expand the use of the FAST-41 process to cover all data center and data center energy projects eligible under the Fixing America’s Surface Transportation Act of 2015. 25  •   Explore the need for a nationwide Clean Water Act Section 404 permit for data centers, and, if adopted, ensure that this permit does not require a Pre-Construction Notification and covers development sites consistent with the size of a modern AI data center.   26  •   Expedite   environmental   permitting   by   streamlining   or   reducing   regulations promulgated under the Clean Air Act, the Clean Water Act, the Comprehensive  21   Executive Order 14156 of January 20, 2025, “Declaring a National Energy Emergency,” Federal Register 90 (18) 8433,  www.govinfo.gov/content/pkg/FR-2025-01-29/pdf/2025-02003.pdf .  22   Presidential Memorandum of April 15, 2025, “Updating Permitting Technology for the 21 st   Century,”  www.whitehouse.gov/presidential-actions/2025/04/updating-permitting-technology-for-the-21st-century/ .  23   Executive Order 14213 of February 14, 2025, “Establishing the National Energy Dominance Council,” Federal Register 90 (33) 9945,   www.govinfo.gov/content/pkg/FR-2025-02-20/pdf/2025-02928.pdf .  24   Executive Order 14255 of March 31, 2025, “Establishing the United States Investment Accelerator,” Federal Register 90 (63) 14701,   www.govinfo.gov/content/pkg/FR-2025-04-03/pdf/2025-05908.pdf .  25   Fixing America’s Surface Transportation Act, 42 U.S.C. §§ 4370m-4370m-11.  26   Clean Water Act of 1972, 33 U.S.C. § 1344.\n\n--- Page 18 ---\n\nAMERICA’S AI ACTION PLAN  15  Environmental Response, Compensation, and Liability Act, and other relevant related laws. 27 ,   28  •   Make Federal lands available for data center construction and the construction of power generation infrastructure for those data centers by directing agencies with significant land portfolios to identify sites suited to large-scale development.  •   Maintain security guardrails to prohibit adversaries from inserting sensitive inputs to this infrastructure. Ensure that the domestic AI computing stack is built on American products and that the infrastructure that supports AI development such as energy and telecommunications are free from foreign adversary information and communications technology and services (ICTS)—including software and relevant hardware.  •   Expand efforts to apply AI to accelerate and improve environmental reviews, such as through expanding the number of agencies participating in DOE’s PermitAI project. 29  Develop a Grid to Match the Pace of AI Innovation  The U.S. electric grid is one of the largest and most complex machines on Earth. It, too, will need to be upgraded to support data centers and other energy-intensive industries of the future. The power grid is the lifeblood of the modern economy and a cornerstone of national security, but it is facing a confluence of challenges that demand strategic foresight and decisive   action.   Escalating   demand   driven   by   electrification   and   the   technological advancements of AI are increasing pressures on the grid. The United States must develop a comprehensive strategy to enhance and expand the power grid designed not just to weather these challenges, but to ensure the grid’s continued strength and capacity for future growth.  Recommended Policy Actions  •   Stabilize the grid of today as much as possible. This initial phase acknowledges the need to safeguard existing assets and ensures an uninterrupted and affordable supply of power. The United States must prevent the premature decommissioning of critical power generation resources and explore innovative ways to harness existing capacity, such as leveraging extant backup power sources to bolster grid reliability during peak demand. A key element of this stabilization is to ensure every corner of the electric grid is in compliance with nationwide standards for resource adequacy and sufficient power generation capacity is consistently available across the country.  •   Optimize existing grid resources as much as possible. This involves implementing strategies to enhance the efficiency and performance of the transmission system. The United States must explore solutions like advanced grid management technologies and upgrades to power lines that can increase the amount of electricity transmitted along existing routes. Furthermore, the United States should investigate new and novel ways for large power consumers to manage their power consumption during critical grid periods to enhance reliability and unlock additional power on the system.  27   Clean Air Act of 1963, 42 U.S.C. §§ 7401-7671q.  28   Comprehensive Environmental Response, Compensation, and Liability Act of 1980.   42 U.S.C. §§ 9601-9675.  29   Office of Policy, U.S. Department of Energy, “Faster, Better Permitting with PermitAI,” (Washington, D.C., July 10, 2025),  www.energy.gov/policy/articles/faster-better-permitting-permitai .\n\n--- Page 19 ---\n\nAMERICA’S AI ACTION PLAN  16  •   Prioritize the interconnection of reliable, dispatchable power sources as quickly as possible and embrace new energy generation sources at the technological frontier (e.g., enhanced geothermal, nuclear fission, and nuclear fusion). Reform power markets to align financial incentives with the goal of grid stability, ensuring that investment in power generation reflects the system’s needs.  •   Create a strategic blueprint for navigating the complex energy landscape of the 21st century. By stabilizing the grid of today, optimizing existing grid resources, and growing the grid for the future, the United States can rise to the challenge of winning the AI race while also delivering a reliable and affordable power grid for all Americans.  Restore American Semiconductor Manufacturing  America jump-started modern technology with the invention of the semiconductor. Now America must bring semiconductor manufacturing back to U.S. soil. A revitalized U.S. chip industry will generate thousands of high-paying jobs, reinforce our technological leadership, and protect our supply chains from disruption by foreign rivals. The Trump Administration will lead that revitalization without making bad deals for the American taxpayer or saddling companies with sweeping ideological agendas.  Recommended Policy Actions  •   Led by DOC’s revamped CHIPS Program Office, continue focusing on delivering a strong return on investment for the American taxpayer and removing all extraneous policy requirements for CHIPS-funded semiconductor manufacturing projects. DOC and other relevant Federal agencies should also collaborate to streamline regulations that slow semiconductor manufacturing efforts.  •   Led by DOC, review semiconductor grant and research programs to ensure that they accelerate the integration of advanced AI tools into semiconductor manufacturing.  Build High-Security Data Centers for Military and Intelligence Community Usage  Because AI systems are particularly well-suited to processing raw intelligence data today, and because of the vastly expanded capabilities AI systems could have in the future, it is likely that AI will be used with some of the U.S. government’s most sensitive data. The data centers where these models are deployed must be resistant to attacks by the most determined and capable nation-state actors.  Recommended Policy Actions  •   Create new technical standards for high-security AI data centers, led by DOD, the IC, NSC, and NIST at DOC, including CAISI, in collaboration with industry and, as appropriate, relevant Federally Funded Research and Development Centers.  •   Advance agency adoption of classified compute environments to support scalable and secure AI workloads.\n\n--- Page 20 ---\n\nAMERICA’S AI ACTION PLAN  17  Train a Skilled Workforce for AI Infrastructure  To build the infrastructure needed to power America’s AI future, we must also invest in the workforce that will build, operate, and maintain it—including roles such as electricians, advanced HVAC technicians, and a host of other high-paying occupations. To address the shortages in many of these critical jobs, the Trump Administration should identify the priority roles that underpin AI infrastructure, develop modern skills frameworks, support industry- driven training, and expand early pipelines through general education, CTE, and Registered Apprenticeships to fuel American AI leadership.  Recommended Policy Actions  •   Led by DOL and DOC, create a national initiative to identify high-priority occupations essential to the buildout of AI-related infrastructure. This effort would convene employers, industry groups, and other workforce stakeholders to develop or identify national skill frameworks and competency models for these roles. These frameworks would provide voluntary guidance that may inform curriculum design, credential development, and alignment of workforce investments.  •   Through DOL, DOE, ED, NSF, and DOC, partner with state and local governments and workforce system stakeholders to support the creation of industry-driven training programs that address workforce needs tied to priority AI infrastructure occupations. These programs should be co-developed by employers and training partners to ensure individuals who complete the program are job-ready and directly connected to the hiring process. Models could also be explored that incentivize employer upskilling of incumbent workers into priority occupations. DOC should integrate these training models as a core workforce component of its infrastructure investment programs. Funding for this strategy will be prioritized based on a program’s ability to address identified pipeline gaps and deliver talent outcomes aligned to employer demand.  •   Led by DOL, ED, and NSF, partner with education and workforce system stakeholders to expand early career exposure programs and pre-apprenticeships that engage middle and high school students in priority AI infrastructure occupations. These efforts should focus on creating awareness and excitement about these jobs, aligning with local employer needs, and providing on-ramps into high-quality training and Registered Apprenticeship programs.  •   Through the ED Office of Career, Technical, and Adult Education, provide guidance to state and local CTE systems about how to update programs of study to align with priority AI infrastructure occupations. This includes refreshing curriculum, expanding dual enrollment options, and strengthening connections between CTE programs, employers, and training providers serving AI infrastructure occupations.  •   Led by DOL, expand the use of Registered Apprenticeships in occupations critical to AI infrastructure. Efforts should focus on streamlining the launch of new programs in priority industries and occupations and removing barriers to employer adoption, including simplifying registration, supporting intermediaries, and aligning program design with employer needs.  •   Led by DOE, expand the hands-on research training and development opportunities for undergraduate,   graduate,   and   postgraduate   students   and   educators,   leveraging\n\n--- Page 21 ---\n\nAMERICA’S AI ACTION PLAN  18  expertise and capabilities in AI across its national laboratories. This should include partnering with community colleges and technical/career colleges to prepare new workers and help transition the existing workforce to fill critical AI roles.  Bolster Critical Infrastructure Cybersecurity  As AI systems advance in coding and software engineering capabilities, their utility as tools of both cyber offense and defense will expand. Maintaining a robust defensive posture will be especially important for owners of critical infrastructure, many of whom operate with limited financial resources. Fortunately, AI systems themselves can be excellent defensive tools. With continued adoption of AI-enabled cyberdefensive tools, providers of critical infrastructure can stay ahead of emerging threats.  However, the use of AI in cyber and critical infrastructure exposes those AI systems to adversarial threats. All use of AI in safety-critical or homeland security applications should entail the use of secure-by-design, robust, and resilient AI systems that are instrumented to detect performance shifts, and alert to potential malicious activities like data poisoning or adversarial example attacks.  Recommended Policy Actions  •   Establish an AI Information Sharing and Analysis Center (AI-ISAC), led by DHS, in collaboration with CAISI at DOC and the Office of the National Cyber Director, to promote the sharing of AI-security threat information and intelligence across U.S. critical infrastructure sectors.  •   Led by DHS, issue and maintain guidance to private sector entities on remediating and responding to AI-specific vulnerabilities and threats.  •   Ensure collaborative and consolidated sharing of known AI vulnerabilities from within Federal agencies to the private sector as appropriate. This process should take advantage of existing cyber vulnerability sharing mechanisms.  Promote Secure-By-Design AI Technologies and Applications  AI systems are susceptible to some classes of adversarial inputs (e.g., data poisoning and privacy   attacks),   which   puts   their   performance   at   risk.   The   U.S.   government   has   a responsibility   to   ensure   the   AI   systems   it   relies   on—particularly   for   national   security applications—are protected against spurious or malicious inputs. While much work has been done to advance the field of AI Assurance, promoting resilient and secure AI development and deployment should be a core activity of the U.S. government.  Recommended Policy Actions  •   Led by DOD in collaboration with NIST at DOC and ODNI, continue to refine DOD’s Responsible AI and Generative AI Frameworks, Roadmaps, and Toolkits.  •   Led by ODNI in consultation with DOD and CAISI at DOC, publish an IC Standard on AI Assurance under the auspices of Intelligence Community Directive 505 on Artificial Intelligence.\n\n--- Page 22 ---\n\nAMERICA’S AI ACTION PLAN  19  Promote Mature Federal Capacity for AI Incident Response  The proliferation of AI technologies means that prudent planning is required to ensure that, if systems fail, the impacts to critical services or infrastructure are minimized and response is imminent. To prepare for such an eventuality, the U.S. government should promote the development and incorporation of AI Incident Response actions into existing incident response doctrine and best-practices for both the public and private sectors.  Recommended Policy Actions  •   Led by NIST at DOC, including CAISI, partner with the AI and cybersecurity industries to ensure AI is included in the establishment of standards, response frameworks, best- practices, and technical capabilities (e.g., fly-away kits) of incident response teams.  •   Modify the Cybersecurity and Infrastructure Security Agency’s Cybersecurity Incident & Vulnerability Response Playbooks to incorporate considerations for AI systems and to include requirements for Chief Information Security Officers to consult with Chief AI Officers, Senior Agency Officials for Privacy, CAISI at DOC, and other agency officials as appropriate. Agencies should update their subordinate playbooks accordingly.  •   Led by DOD, DHS, and ODNI, in coordination with OSTP, NSC, OMB, and the Office of the National Cyber Director, encourage the responsible sharing of AI vulnerability information as part of ongoing efforts to implement Executive Order 14306, “Sustaining Select Efforts to Strengthen the Nation’s Cybersecurity and Amending Executive Order 13694 and Executive Order 14144.” 30  30   Executive Order 14306 of June 6, 2025, “Sustaining Select Efforts To Strengthen the Nation’s Cybersecurity and Amending Executive Order 13694 and Executive Order 14144,” Federal Register 90 (111) 24723,   www.govinfo.gov/content/pkg/FR- 2025-06-11/pdf/2025-10804.pdf .\n\n--- Page 23 ---\n\nAMERICA’S AI ACTION PLAN  20  Pillar III: Lead in International AI Diplomacy and  Security  To succeed in the global AI competition, America must do more than promote AI within its own borders. The United States must also drive adoption of American AI systems, computing hardware, and standards throughout the world. America currently is the global leader on data center construction, computing hardware performance, and models. It is imperative that the United States leverage this advantage into an enduring global alliance, while preventing our adversaries from free-riding on our innovation and investment.  Export American AI to Allies and Partners  The United States must meet global demand for AI by exporting its full AI technology stack— hardware, models, software, applications, and standards—to all countries willing to join America’s AI alliance. A failure to meet this demand would be an unforced error, causing these countries to turn to our rivals. The distribution and diffusion of American technology will stop our strategic rivals from making our allies dependent on foreign adversary technology.  Recommended Policy Actions  •   Establish and operationalize a program within DOC aimed at gathering proposals from industry consortia for full-stack AI export packages. Once consortia are selected by DOC, the Economic Diplomacy Action Group, the U.S. Trade and Development Agency, the Export-Import Bank, the U.S. International Development Finance Corporation, and the Department of State (DOS) should coordinate with DOC to facilitate deals that meet U.S.-approved security requirements and standards.  Counter Chinese Influence in International Governance Bodies  A large number of international bodies, including the United Nations, the Organisation for Economic Co-operation and Development, G7, G20, International Telecommunication Union, Internet Corporation for Assigned Names and Numbers, and others have proposed AI governance frameworks and AI development strategies. The United States supports like- minded nations working together to encourage the development of AI in line with our shared values. But too many of these efforts have advocated for burdensome regulations, vague “codes of conduct” that promote cultural agendas that do not align with American values, or have been influenced by Chinese companies attempting to shape standards for facial recognition and surveillance.  Recommended Policy Actions  •   Led by DOS and DOC, leverage the U.S. position in international diplomatic and standard-setting   bodies   to   vigorously   advocate   for   international   AI   governance approaches   that   promote   innovation,   reflect   American   values,   and   counter authoritarian influence.\n\n--- Page 24 ---\n\nAMERICA’S AI ACTION PLAN  21  Strengthen AI Compute Export Control Enforcement  Advanced AI compute is essential to the AI era, enabling both economic dynamism and novel military capabilities. Denying our foreign adversaries access to this resource, then, is a matter of both geostrategic competition and national security. Therefore, we should pursue creative approaches to export control enforcement.  Recommended Policy Actions  •   Led by DOC, OSTP, and NSC in collaboration with industry, explore leveraging new and existing location verification features on advanced AI compute to ensure that the chips are not in countries of concern.  •   Establish a new effort led by DOC to collaborate with IC officials on global chip export control   enforcement.   This   would   include   monitoring   emerging   technology developments in AI compute to ensure full coverage of possible countries or regions where chips are being diverted. This enhanced monitoring could then be used to expand and increase end-use monitoring in countries where there is a high risk of diversion of advanced, U.S.-origin AI compute, especially where there is not a Bureau of Industry and Security Export Control Officer present in-country.  Plug Loopholes in Existing Semiconductor Manufacturing Export Controls  Semiconductors are among the most complex inventions ever conceived by man. America and its close allies hold near-monopolies on many critical components and processes in the semiconductor   manufacturing   pipeline.   We   must   continue   to   lead   the   world   with pathbreaking research and new inventions in semiconductor manufacturing, but the United States must also prevent our adversaries from using our innovations to their own ends in ways that undermine our national security. This requires new measures to address gaps in semiconductor manufacturing export controls, coupled with enhanced enforcement.  Recommended Policy Actions  •   Led by DOC, develop new export controls on semiconductor manufacturing sub- systems. Currently, the United States and its allies impose export controls on major systems necessary for semiconductor manufacturing, but do not control many of the component sub-systems.  Align Protection Measures Globally  America must impose strong export controls on sensitive technologies. We should encourage partners and allies to follow U.S. controls, and not backfill. If they do, America should use tools such as the Foreign Direct Product Rule and secondary tariffs to achieve greater international alignment.  Recommended Policy Actions  •   Led by DOC and DOS and in coordination with NSC, DOE, and NSF, develop, implement, and share information on complementary technology protection measures, including in basic research and higher education, to mitigate risks from strategic adversaries and\n\n--- Page 25 ---\n\nAMERICA’S AI ACTION PLAN  22  concerning entities. This work should build on existing efforts underway at DOS and DOC, or, where necessary, involve new diplomatic campaigns.  •   Develop a technology diplomacy strategic plan for an AI global alliance to align incentives   and   policy   levers   across   government   to   induce   key   allies   to   adopt complementary AI protection systems and export controls across the supply chain, led by DOS in coordination with DOC, DOD, and DOE. This plan should aim to ensure that American allies do not supply adversaries with technologies on which the U.S. is seeking to impose export controls.  •   Expand new initiatives for promoting plurilateral controls for the AI tech stack, avoiding the sole reliance on multilateral treaty bodies to accomplish this objective, while also encompassing existing U.S. controls and all future controls to level the playing field between U.S. and allied controls.  •   Led by DOC and DOD, coordinate with allies to ensure that they adopt U.S. export controls, work together with the U.S to develop new controls, and prohibit U.S. adversaries from supplying their defense-industrial base or acquiring controlling stakes in defense suppliers.  Ensure that the U.S. Government is at the Forefront of Evaluating National Security Risks in Frontier Models  The most powerful AI systems may pose novel national security risks in the near future in areas such as cyberattacks and the development of chemical, biological, radiological, nuclear, or explosives (CBRNE) weapons, as well as novel security vulnerabilities. Because America currently leads on AI capabilities, the risks present in American frontier models are likely to be a preview for what foreign adversaries will possess in the near future. Understanding the nature of these risks as they emerge is vital for national defense and homeland security.  Recommended Policy Actions  •   Evaluate frontier AI systems for national security risks in partnership with frontier AI developers, led by CAISI at DOC in collaboration with other agencies with relevant expertise in CBRNE and cyber risks.  •   Led by CAISI at DOC in collaboration with national security agencies, evaluate and assess potential security vulnerabilities and malign foreign influence arising from the use of adversaries’ AI systems in critical infrastructure and elsewhere in the American economy, including the possibility of backdoors and other malicious behavior. These evaluations should include assessments of the capabilities of U.S. and adversary AI systems, the adoption of foreign AI systems, and the state of   international AI competition.  •   Prioritize the recruitment of leading AI researchers at Federal agencies, including NIST and CAISI within DOC, DOE, DOD, and the IC, to ensure that the Federal government can continue to offer cutting-edge evaluations and analysis of AI systems.  •   Build, maintain, and update as necessary national security-related AI evaluations through collaboration between CAISI at DOC, national security agencies, and relevant research institutions.\n\n--- Page 26 ---\n\nAMERICA’S AI ACTION PLAN  23  Invest in Biosecurity  AI will unlock nearly limitless potential in biology: cures for new diseases, novel industrial use cases, and more. At the same time, it could create new pathways for malicious actors to synthesize harmful pathogens and other biomolecules. The solution to this problem is a multi- tiered   approach   designed   to   screen   for   malicious   actors,   along   with   new   tools   and infrastructure for more effective screening. As these tools, policies, and enforcement mechanisms mature, it will be essential to work with allies and partners to ensure international adoption.  Recommended Policy Actions  •   Require all institutions receiving Federal funding for scientific research to use nucleic acid synthesis tools and synthesis providers that have robust nucleic acid sequence screening and customer verification procedures. Create enforcement mechanisms for this requirement rather than relying on voluntary attestation.  •   Led by OSTP, convene government and industry actors to develop a mechanism to facilitate data sharing between nucleic acid synthesis providers to screen for potentially fraudulent or malicious customers.  •   Build, maintain, and update as necessary national security-related AI evaluations through collaboration between CAISI at DOC, national security agencies, and relevant research institutions.\n\n--- Page 27 ---\n\nAMERICA’S AI ACTION PLAN  24  This page intentionally left blank.\n\n--- Page 28 ---\n\nAMERICA’S AI ACTION PLAN  25",
    "analysis": {
      "governance_power_accountability": "The document reveals clear centralization of power, with the United States government defining the goals and expected outcomes of AI development. Accountability mechanisms are not explicitly stated, and there is an apparent lack of a comprehensive risk management strategy, potentially shifting the burden and risk to the public. Furthermore, the language used implies a competitive and militaristic approach to AI development, suggesting a focus on power dynamics at the global level.",
      "plurality_inclusion_embodiment": "There is a strong reinforcement of a 'default' user assumption, with little explicit consideration for plurality and inclusion. The plan does not show evidence of valuing various knowledge systems, Indigenous, disability, or non-Western perspectives, instead centering primarily on an assumption of Western dominance and the competitive advantages of American innovation.",
      "agency_codesign_self_determination": "There is a lack of discussion regarding community agency and co-design in the AI development process. The language suggests a top-down approach, with the government leading the direction of AI innovation. There are no provisions for self-determination or the right to refuse, with the priority being unquestioned and unchallenged global technological dominance.",
      "reflexivity_situated_praxis": "The document does not exhibit reflexivity or a critical examination of its own positionality, history, and underlying assumptions. There is a lack of awareness of the structural inequities that could be perpetuated or exacerbated through this approach to AI.",
      "legitimacy_claims": {
        "source": "Technocratic",
        "mechanisms": "The document establishes legitimacy through a technocratic rationale, invoking the authority of the White House and emphasizing the potential of AI to reshape power dynamics globally.",
        "tensions": "There is a tension between technocratic and democratic legitimacy claims, with the emphasis on technological dominance potentially sidelining democratic ideals of participation and diversity."
      },
      "key_insight": "The AI Action Plan is characterized by centralization of power, lack of reflexivity, and a dominant technocratic legitimacy claim, potentially sidelining democratic values, and diverse perspectives.",
      "governance_scores": {
        "centralization": 90,
        "rights_focus": 20,
        "flexibility": 40,
        "market_power": 60,
        "procedurality": 75
      },
      "structural_pillars": {
        "risk": {
          "title": "Risk Negligence",
          "description": "Lack of comprehensive risk management strategy",
          "badge": "Risk-Blind"
        },
        "enforcement": {
          "title": "Centralized Control",
          "description": "Top-down approach to AI innovation",
          "badge": "Authoritarian"
        },
        "rights": {
          "title": "Narrow Rights Focus",
          "description": "Little consideration for community agency and diverse perspectives",
          "badge": "Exclusionary"
        },
        "scope": {
          "title": "Technocratic Dominance",
          "description": "Strong focus on technological advancement and global dominance",
          "badge": "Dominant"
        }
      }
    }
  },
  {
    "id": "1763924708640",
    "title": "EBSCO-FullText-11_23_2025",
    "description": "Uploaded 11/23/2025",
    "type": "PDF",
    "addedDate": "11/23/2025",
    "status": "Active Case",
    "colorClass": "bg-purple-100",
    "iconClass": "text-purple-600",
    "extractedText": "--- Page 1 ---\n\nR ESEARCH   A RTICLE  DOI: 10.25300/MISQ/202 2 / 15736   MIS   Quarterly   V o l .   4 6   No.   3   pp.   1421 - 1452   /   September   202 2   1421  D ISCURSIVE   F IELDS AND THE   D IVERSITY - C OHERENCE  P ARADOX :   A N   E COLOGICAL   P ERSPECTIVE ON THE  B LOCKCHAIN   C OMMUNITY   D ISCOURSE 1  Shaila M. Miranda  Division of MIS, Price College of Business ,   University of Oklahoma ,  Norman, OK, U.S.A. { shailamiranda@ou.edu }  Dawei (David) Wang  Department of Business and Information Technology ,   Missouri University of Science and Technology  Rolla, MO, U.S.A. { dwang@mst.edu }  Chuan (Annie) Tian  Department of Information Systems, Statistics and Management   Science  Culverhouse College of Business ,   University of Alabama  Tuscaloosa, AL, U.S.A. { actian@ua.edu }  Innovation breakthroughs prompt sensemaking discourses that promote community learning and socially  construct the innovation. Through this   discourse, interested actors advance diverse frames, appealing to  consumers with disparate preferences but raising concerns for the coherence of that discourse. We  unpack this diversity - coherence paradox by recasting coherence as the relatedness of innovat ion frames  and spotlighting the role of discursive fields that circumscribe meaning. Our empirical context is the first  six years of blockchain discourse across seven discursive fields. Our research offers three insights in  furtherance of an ecological per spective on innovation discourse. First, framing diversity emanates from  discursive fields rather than from actors. Second, fields play differentiated roles in the framing process.  Enactment fields comprised of actors with direct experience with the techno logy limit diversity. They do  so by erecting walls that circumscribe discourse through imprinting on their original frame and retracting  from or abandoning frames learned from other fields. In contrast, mediated fields ,   in which actors lack  direct experien ce with the technology ,   enhance diversity. They do so by imitating or learning from other  fields and foreshadowing or anticipating   the   frames used by other fields, thereby building bridges. Third,  rather than oppos ing   each other, diversity and coherence co evolve as the diversity induced by mediated  fields increases framing redundancies, synthesizing frames into a coherent community understanding of  the innovation. Our research signals to the actors who serve as innovation ambassadors and gatekeepers  that di verse views of an innovation are not only inevitable, given the many discourse fields in which those  views are formulated, but   can   also be coherent and desirable .  Keywords:   Framing, diversity, coherence, discursive field, innovation, diffusion, blockchain  Introduction  Innovation breakthroughs prompt the set of interested actors  that   constitute   an   innovation   community   to   engage   in  sensemaking and sensegiving about the innovation. The  1   Andrew Burton - Jones   was the accepting senior editor for this paper.  Ni cholas Berente   served as the associate editor.  ensuing talk   is known as discourse   (Heracleous, 2016) .  Beyond fostering community learni ng   (Wang & Ramiller,  2009) ,   discourse   is   an   institutionalization   process   that  recursively shap es   actions that give rise to further discourse  (Phillips & Malhotra, 2017) . Consequently, understanding\n\n--- Page 2 ---\n\nMiranda   et   al.   /   Discursive Fields and the Diversity - Coherence Paradox  1422   MIS   Quarterly   Vo l.   4 6   No.   3   /   September   202 2  innovation discourse is important   for   understanding diffus ion.  Through discourse, an innovation community collectively  interprets the innovation, legitimates it, and mobilizes the  collective action necessary for its diffusion   (Barrett et al.,  2013; Swanson & Ramiller, 1997; Wang & Ramiller, 2009) .  The community t hus   socially constructs the innovation that is  the discourse object   (Foucault, 1977) .  As they engage in discourse, interested actors advance diverse  frames   (Barrett et al., 2013) .   Framing   is a sensegiving   process  of selectively accentuating innovation facets while masking or  glossing over others   (Entman, 1993) . It sets expectations for  innovation   (Lin & Silva, 2005; Orlikowski & Gash, 1994) ,  shaping actors’ categorization of the innovation and the  criteria   used to evaluate it   (Kaplan & Tripsas, 2008) . Consider  the following statements about blockchain:  Blockchain is a   foundational   technology: It has the  potential to create new foundations for our economic  and social systems .   ( Iansiti   &   Lakhani, 2017 ,   p. 4 )  This   extraordinary   technology   may   be   stalled,  sidetracked,   captured   or   otherwise   suboptimized  depending on how all the stakeholders behave in  stewarding   this   set   of   resources — i.e.,   how   it   is  governed .   ( Tapscott   &   Tapscott, 2017 )  You can even   download an out - of - the - box blockchain  app   for   your   local   babysitting   circle — or   your  prostitution ring   ( Harford ,   2015 ) .  Each   frame   highlights   a   different   aspect   of   blockchain,  collectively, offering a more holistic picture. Such diversity can  be beneficial.   Diverse frames   attract   the   attention of consumers  with   disparate   preferences   (Hsu,   2006) .   They   enable  entrepreneurial engagement and novel deployment   (Stark,  2009) .   Different   frames   induce   variation   in   the   material  innovation, from which a dominant design emerges   (Kaplan &  Tripsas, 2008) . Recombination of the knowledge in discursive  frames can foster   the   perception and creation of novel solutions  (Nambisan et al., 2 017) , a key pathway to further innovation  and economic value   (Davis & Aggarwal, 2020; Hargadon &  Sutton, 1997) . Expressions of diverse views were found to  enhance social media diffusion   (Miranda et al., 2015) .  The   conundrum   facing   organizational   and   infor mation  systems (IS) scholars studying innovation discourse is that  framing diversity seems both conducive and inimical to  coherence. Swanson and Ramiller   (1997, p. 464)   noted,  “diversity in the community’s contributions to the discourse  tend   to   make   it   ric her,   but   probably   more   prone   to  contradiction, ambiguity, and   low   coherence.”   Similarly,  Barrett et al.   (2013)   argued that while struggles over meaning  are conducive to diffusion, they also can create conflict and  reduce diffusion. These views, positioning   diversity and  coherence at   the   ends of a continuum, are consistent with one  definition of coherence: “A text can be said to be coherent if a  single imagined world is compatible with all parts of the text”  (Fillmore, 1975, p. 136) . Consistent with this per spective,  Wang and Ramiller   (2009)   related coherence in community  discourse about ERP to its consensus on the innovation.  Coherence as consensus has been linked to diffusion. For  example, Currie   (2004)   found diffusion of the application  service provider in novation stalled   because of   an incoherent  vision for it. Likewise, Bunduchi et al.   (2015)   attributed   the  failure of a telehealth innovation to an incoherent vision for it.  Wang   and   Swanson   (2007)   described   how   institutional  entrepreneurs struggled to produ ce a coherent vision in order  to launch professional services automation.   Jarrett   (1984)  challenged   Fillmore’s   consensus   view   of   coherence   as  ethnocentric, privileging one world view over others.  An alternate view of   coherence is of ties or relationships  a mong texts that render them a “unified discourse”   (Tannen,  1984, p. xiv) . In this view, coherence emanates from the  emergent organization of frames based on their linkages.  Research   showing that “chunking” — i.e., encoding   small  information   units   into   larger   units — fosters   learning  substantiates this view   (Thalmann et al., 2019) . Consistent  with this perspective, Wang and Swanson   (2008)   viewed  coherence of the CRM vision in terms of “ties” between the  special advertising sections about CRM in   Business Week  sections covering the innovation. Miranda et al.   (2015)   viewed  the coherence of social media discourse as the relationality  among texts describing different social media initiatives. We  confront   the   apparent   diversity - coherence   paradox   by  addressing the re search question:  RQ :   How can an innovation community’s framing discourse  be both diverse and coherent?  To understand how a community might enjoy   simultaneously  diverse   and   coherent   framing   of   an   IT   innovation,   we  introduce the concept of a discursive fiel d.   A   discursive field  is a   symbolic space that circumscribes meaning   (Hardy &  Maguire, 2010; Weedon, 1994) . It is the social context within  which discourses take place, producing texts   (Phillips &  Malhotra, 2017) . Different fields have access to different  cultural   resources,   i.e.,   values,   beliefs   and   ideologies,  symbols, and relationships with material artifacts   (Spillman,  1995) . Innovation communities are differentiated into such  discursive   fields,   producing   competing   meanings   of   the  innovation   (Weedon, 19 94) . To understand the dynamics of  discursive fields within an innovation community, we draw  upon four population ecology principles — two that address the  organization of actors within the ecosystem and two that  address the relationship of innovation variants to each other.\n\n--- Page 3 ---\n\nMiranda et al. /   Discursive Fields and the Diversity - Coherence Paradox  MIS Quarterly Vo l . 4 6   No.   3   /   September   202 2   1423  We   tackled   the   paradox   of   framing   diversity   and  coherence by studying blockchain, an innovation that has  proved   it s   staying   power   despite   diverse   frames.  Blockchain   is   a   tamper - resistant   distributed   database  system   (Rossi   et   al.,   2019) .   We   studied   blockchain  discourse from its inception through the end of 2017,  when,   as   per   Google   Trends,   “blockchain”   searches  peaked,   and   technology   bellwethers   such   as   IBM,  Microsoft, and Oracle offered platforms and services,  auguring broad diffusion. We adopted a computational ly  intensive grounded theory approach   (Berente et al., 2019) .  Drawing   on   cultural   perspectives   on   organizing,   we  theoretically sampled discursive fields of governments,  newspapers,   science,   idea   evangelists,   corporations,  crowdfunding projects, and social influencers based on  seven ideal types of cultural ly   specific logics of action  (DiMaggio, 1997; Weber & Dacin , 2011) . This yielded a  corpus of 4,925 texts. Following   DiMaggio et al. (2013) ,  we used topic modeling to surface 23 blockchain frames,  compiling a dataset of frames across the six study years.  Through our study of blockchain framing by discursive fields,  we offer three insights into innovation discourse. First, we  highlight discursive fields as an intermediate level between  community and actors in a discourse ecosystem, showing that  participatio n by diverse fields, rather than participation by  diverse   actors   per   se,   contributes   to   diversity.   Ongoing  framing diversity   that   suggests a dynamic of mutualism, rather  than   competitiveness,   prevails   within   the   discourse  ecosystem. Second, typifying field s as mediated, hybrid, and  enactment fields, representing low, moderate, and high levels  of participant material engagement, i.e., direct experience  with use or production of the focal IT   (Kaplan & Tripsas,  2008; Wang, 2010) , we highlight the differentiate d roles they  play vis - à - vis four framing processes. Enactment fields use  processes of imprinting, i.e., sticking with their original frame,  and retracting, i.e., abandoning frames learned from other  fields. Thus, rather than permit the knowledge in discour se to  flow freely   (Wang & Ramiller, 2009) , enactment fields erect  walls that circumscribe their actors’ discourse. In contrast,  mediated fields use processes of imitating — i.e., learning from  other fields — and foreshadowing, anticipating frames used by  other   fields. Through these processes, mediated fields enhance  diversity, thereby bridging frames and fields. Third, rather  than opposing each other, we show that states of framing  diversity   and   coherence   coevolve   as   diversity   increases  framing redundancies, sy nthesizing frames into a coherent  community understanding of the innovation. Our research  signals   to   actors who serve as innovation ambassadors and  gatekeepers that diverse views of an innovation are inevitable,  given the many discourse fields in which tho se views are  formulated. But it also shows that such diversity is desirable —  rather than foreclosing on coherence, diversity enhances it.  Background  Discourse is an important pathway to vicarious learning in  organizations   (Myers, 2018)   and in innovation communities  (Wang & Ramiller, 2009) . Organizational learning scholars  have noted the advantages of vicarious learning ,   relative to  experiential learnin g   (Ingram & Baum, 1997) . Vicarious  learning   permits   intentional   and   serendipitous   learning  (Myers, 2018) . Within innovation communities, discourse  thus enhances opportunities for diffusion of the innovation.   To  situate our study, we overview the roles of d iscourse and  discursive fields in innovation framing and review ecological  perspectives on innovation and discourse.  The Role of Discourse in Innovation Framing  Discourse   also   socially   constructs   its   object,   i.e.,   the  innovation   (Parker, 2014) . It enables   certain ways of talking  about the innovation, engendering a “mutually constitutive  relationship”   between   the   innovation   and   the   discourse  (Phillips et al., 2004, p. 639) .   D iscourse not only produce s  specific knowledge, but also institutional mechanisms tha t  circumscribe whether a statement about the focal object  should be deemed true or false   (Hardy & Maguire, 2016) ,  constituting a “regime of truth”   (Foucault, 1980, p. 131) . In  this way,   the   discourse becomes material ,   as what matters to  actors at a point i n time is embodied in the discourse   (Cooren  et al., 2012) ,   and   the   discourse enacts the innovation through  the “truths” it presents   (Orlikowski & Scott, 2014) .  Discourses are constituted by frames   (Fiss & Hirsch, 2005) .  Earlier, we saw three blockchain frames. Iansiti and Lakhani’s  frame   cast blockchain   in   a   positive   light ,   Tapscott and  Tapscott   qualified   the   value   of   blockchain   based   on  implemented   governance structures ,   and the   Financial Times  article spotlighted its   socially desirable and undesirable use  cases.   Frames are rhetorical devices that spotlight some  aspects   of   a   situation,   promoting   “a   particular   problem  definition, causal interpretation, moral evaluation, and/or  treatment recommendation”   (Entman, 1993, p.   52) . They  draw upon cultural resources, e.g., values and experiences,  making the spotlighted aspects of the situation meaningful to  the   actors   who   share   those   resources   (Williams,   1995) .  F rames supply templates for action and attention   (Kaplan &  Tripsas, 2 008) . Because frames guide subsequent action,  scholars researching technological frames have noted that by  imbuing technologies with specific meanings, frames socially  construct them   (Essén & Värlander, 2019; Sahay et al., 1994) .  Framing diversity emerges   from engagement by diverse actors  with diverse “assumptions, expectations, and knowledge”  (Orlikowski   &   Gash,   1994,   p.   178) .   These   differences\n\n--- Page 4 ---\n\nMiranda   et   al.   /   Discursive Fields and the Diversity - Coherence Paradox  1424   MIS   Quarterly   Vo l.   4 6   No.   3   /   September   202 2  emanate from actors’ prior history   (Kaplan & Tripsas, 2008,  p. 791) . Snow   (2008, p. 8)   therefore posited that “framing can  be best understood within the context of a discursive field.”  Discursive   Fields   a s   Discourse - Structuring  Spaces  Discursive fields are symbolic spaces that circumscribe salient  meanings   (Hauser, 1999; Weedon, 1994; Wuth now, 2009) ,  offering actors the distinct toolkits or cultural resources that  are deployed in framing   (King, 2007) .   Whereas private  discourse occurs among acquaintances, discursive field s   are  the   locale s   of   public   discourse   and   are   inhabited   by  “strangers”   (Hauser,   1999,   p.   64) .   Examples   include  governments, media, and the family   (Weedon, 1994) .   Fields  embed discourses within “historical and material contexts”  (Fiss & Hirsch, 2005, p. 30)   and shape participants’ goals and  values, not just the vocabulary and   other resources used in the  discourse   (Hauser, 1999) .   Thus, Fayard et al.   (2016)   found  that different cultural contexts shaped organizations’ framing  of crowdsourcing as either algorithmic or humanistic.   Fiss and  Hirsch   (2005)   showed that news media and fi rms framed  globalization differently over time.  Discourse must   be understood within the context of discursive  fields because the knowledge that informs discourse is tied to  the field   (Lok & Willmott, 2006) . Within these fields,   the  knowledge   of   objects   co nstituted   through   discourse   is  incomplete for two reasons. First, from a realist perspective,  knowledge is incomplete because only some features of the  object are salient and visible to a discursive field   (Sewell,  2005) . Second, from a constructivist persp ective, knowledge  is incomplete because fields social ly   construct the objects ,  which do not exist   prior to   the   discourse,   differently   (Foucault,  1972; Lok & Willmott, 2006) . Th e fact that   knowledge within  discursive fields is incomplete and knowledge across fields  differs suggests that discursive fields foster diverse frames.  Ho wever,   this   does not address how such framing can also be  coherent. Rather, like the proverbial blind men around   the  elephant, actors within different discursive   frames   are able to  perceive the innovation only partially, yielding divergent  views of it.  An Ecological Perspective on Discourse  Our analysis of the dynamics of discursive fields that permits  both diverse a nd coherent framing within the innovation  community reflects and builds on four population ecology  principles — two that address the organization of actors within  the   ecosystem   and   two   that   address the   relationship   of  innovation   variants   to   each   other.   First ,   ecosystems are  comprised of multiple hierarchical ly   organized levels   (Low et  al., 2003; Wang, 2021) , integrating the capabilities of different  actors   (Rao, 2017) . This perspective resonates with views of  discursive fields wherein the “individualized, loc al associative  space is potentially included in larger, more polyphonous  exchanges”   (Hauser, 1999, p. 62) .   In our study, we consider  not only the nesting of actors within fields, but also of  discursive fields within the innovation community. Second,  actors   within   an   ecosystem   are   both   autonomous   and  interdependent   (Seidl, 2007; Wang, 2021) . An ecological  perspective therefore emphasizes the need to study ecosystem  components in conjunction with each other   (Seidl, 2007) .  Third, interactions among entities i n an ecosystem can be  competitive , resulting in an innovation variant “winning” and  a “dominant design”   (Boucher et al., 1982)   or   mutualistic ,  permitting multiple variants to survive   (Barnett & Carroll,  1987) . Whereas a competitive perspective on discourse   would  anticipate   the   “survival” of a dominant frame, a mutualistic  perspective would allow   for   multiple frames   to   coexist and  remain viable. Fourth, diversity in ecosystems engenders  redundancy   (Low et al., 2003) . Such redundancy is believed  to promote resilience of variants within the ecosystem — and  consequently, of the ecosystem itself — because   redundancy   in  the form   of   exact   copies increases   the   exposure and signaling  strength of variants within the ecosystem and redundancy   in  the form of   minor   variations improves variants’ fit with  evolving environments   (Low et al., 2003) .  Understanding the discursive spaces within   which innovations  are socially constructed and the nature of innovation discourse  is imperative for those who wish to lead innovation efforts.  Doing so will help them   better   manage, monitor, and interpret  such discourses.  Methods  To address our questi on of how discursive fields manage the  diversity - coherence   paradox,   we   used   a   computational ly  intensive theory discovery (CITD) approach, which blends  qualitative analyses with computational discovery   (Berente et  al., 2019) . We studied framing in discourse , i.e., interrelated  texts   (Phillips et al., 2004) .   At each stage in our CITD (see  Figure   1 ), we used two standard inductive practices to surface  and reconcile insights within our research team. First, we used  memo - writing   to   encourage   team   members   to   enga ge  independently with the data to elicit parallel and   competing  insights   (Birks et al., 2013) . Second, we shared, discussed, and  reconciled   observations   (Tracy,   2013) .   We   successively  compiled, transformed, and reduced the dataset to frames  advanced and es poused by the seven discursive fields across  six study years.\n\n--- Page 5 ---\n\nMiranda et al. /   Discursive Fields and the Diversity - Coherence Paradox  MIS Quarterly Vo l . 4 6   No.   3   /   September   202 2   1425  Theoretical Sampling  Understanding the social construction of an innovation object  requires us to broadly sample across discursive fields. This was a  key   consideration   in   our   theoretical   sampling   approach,  summarized in Table   1 .   Following Chiapello and Fairclough  (2002,   p.   194 ) ,   who   termed   discursive   fields   “orders   of  discourse , ” we developed our discourse sample based on the  seven archetypical orders of worth identified by Boltanski and  colleagues   (Boltanski & Chiapello, 2005; Boltanski & Thévenot,  2006) . Orders of worth/disco urse are cultural categories similar  to institutional logics — both are posited   as   supply ing   higher - order  principles that legitimate action   (DiMaggio, 1997; Thornton &  Ocasio,   2008) .   While   the   institutional   logics   perspective  privileges   the   constraints   on   ac tion,   the   orders   of   worth  perspective recognizes both constraint and choice by cognizant  actors   (Cloutier & Langley, 2013) .   According to   this perspective,  al though actors are influenced by their predominant order of  worth, they flexibly draw upon repertoir es from other orders.  Such cognizant, flexible action is the basis for diverse framing  within orders and similar framing across orders.  For each order of worth, we sought fields that met two criteria.  First, their primary justification principle had to resemble that of  one of Boltanski and colleagues’ orders. Second, the field’s  discourse had to be public. Not only is private discourse  ina ccessible to researchers, it also is inaccessible to members of  the community trying to make sense of an innovation. Our  sampling choices mirror those   of   prior researchers. For example,  Cloutier and Langley (2013) also associated science with the  industria l order of worth ,   and Durand and Thornton (2018)  viewed corporate producers, consumers, intermediaries, and their  evaluators as market actors.  Because Boltanski   and colleagues   emphasize actor competence  in tailoring justifications to the situation at hand,   we did not  assume that the sampled texts would reflect the principle of the  order for which the texts were sampled. Rather, we validated our  sample by mining field guiding texts, i.e., texts specifying field  ideals.   For   example,   the   U . S .   Constitution   spec ifies   U . S .  government ideals. We found guiding texts t hat   emphasize  vocabulary   that   Boltanski   and colleagues   used to characterize the  orders. Appendix A details this validation process and the results.  Discourse Sample  We constructed our discourse sample by searching focal  discourse spaces for “blockchain” — from its first appearance  until December 2017, when as p er Figure   2 , “blockchain”  searches peaked and the numbers of sample texts were  highest. By the end of 2017, technology bellwethers like IBM,  Microsoft, and Oracle were offering blockchain platforms and  services, auguring broad diffusion.  Discourse Authors  Actors’   discourse   may   be   informed   by   their   material  engagement,   i.e.,   direct   experience,   with   the   innovation  (Wang, 2010) . To understand the basis of discourse, we  searched for information on the authors of texts in each  discursive field. We relied primari ly on LinkedIn, but also  searched newspaper, university, and other live websites, as  well as the Internet Archive (archive.org).  Synchronic Analysis  Here,   we   used   machine   and   manual   coding   to   surface  discourse and discursive field patterns. We used two tex t  mining procedures to analyze the corpus of 4,925 texts to  discover the patterning of the discourse. We coded author  experience to discover   the   patterning of the discursive fields.  Text Mining to Surface Blockchain Frames  Following DiMaggio et al.   (2013) , who used topic modeling  via   l atent Dirichlet   a nalysis (LDA) to surface frames about  public support for the arts from   10   years of newspaper  coverage, we subjected our corpus to LDA. We followed this  with   multidimensional   scaling   (MDS),   which   surfaces  unde rlying meaning structures by positioning related topics  next to each other in two - dimensional space   (Mohr, 1998) .  (See Appendix B for details of our topic modeling procedure).  From the seven alternate models surfaced, we selected a 23 -  topics model spanning   four thematic quadrants. All topics had  emerged by 2015 ,   al though almost 90% of sample texts were  from subsequent years. This suggests theoretical saturation at  the synchronic analysis stage, i.e., that additional data reiterate  existing   codes   and   reveal   no   new   codes   (Urquhart   &  Fernández, 2013) .  Coding   Author   Experience   to   Estimate   Material  Engagement in Fields  To understand the basis of discourse across the discursive  fields, we coded bios for all authors from government and  idea evangelist fields and ra ndom samples of authors from  news media, social influencer, science, and corporate fields.  Our coding approach and results are summarized in Table   2 .  For news media, which engage authors differently and  accounted for a large proportion of texts in our   corpus and  authors in our sample, we stratified the sample by newspaper  and year to ensure a representative sample. Across all fields,  consistent   with   theoretical   sampling   principles,   we  iteratively sampled and coded until our material engagement  estimates   remained unchanged.\n\n--- Page 6 ---\n\nMiranda   et   al.   /   Discursive Fields and the Diversity - Coherence Paradox  1426   MIS   Quarterly   Vo l.   4 6   No.   3   /   September   202 2  Figure   1 . Summary of Methods  Table   1 .   Theoretical Sampling  Order of   w orth*   Sampled field   Rationale for field selection   Sample description  Civic   Governments   In the prototypical civic world, the predominant  mode of justification is the well - being of the  collective, and action governed by   laws and rules  is valued. This mode of justification is epitomized  in government policy - setting documents.  42 government documents from  8 U . S .   Federal and European  Union agencies, averaging  1,458 words per document.  Domestic   News media   The prototypical do mestic world values  relationships between guardians and protectees.  News media exemplifies this principle in public  discourse, playing the role of watchdog   (Hachten,  2005) .  1,178 newspaper articles from  14 newspapers, averaging  1,294 words per article.  Industrial   Science   The prototypical industrial world values  “technological objects and scientific methods”  (Boltanski & Thévenot, 2006, p. 203) , efficiency,  measurement, expertise, and progress. This focus  is best represented in scholarly journal   articles.  65 articles indexed by ABI -  Inform and PubMed from 48  journals, averaging 7,023 words  per article.  Inspiration   Idea  evangelists  The prototypical inspiration world values  uniqueness,   passion, and creative energy. These  values are   idealized in the evangelism promoted  by TED talks, which aim to “make great ideas  accessible and spark conversation” (TED website).  26 TED talks from 25 speakers,  averaging 2,265 words per  speech.  Market   Corporations   The prototypical market world values competition  and   resource accrual. This focus is epitomized in  the public communications   of   public ly   traded firms.  222 annual report sections and  press releases from 90  corporations, averaging 1,229  words per document.\n\n--- Page 7 ---\n\nMiranda et al. /   Discursive Fields and the Diversity - Coherence Paradox  MIS Quarterly Vo l . 4 6   No.   3   /   September   202 2   1427  Network   Projects   The prototypical network world is concerned with  projects and short - term relationships. This is  epitomized in crowdfunding projects, in which  there is little ongoing contact between investors  and project owners.  158 Kickstarter and GoFundMe  project descri ptions from 157  funding seekers, averaging 657  words per description.  Renown   Social  influencers  The prototypical renown world values actors who  command widespread recognition, opinion  leaders, and influencers. Twitter influencers  represent such microcelebrities, able to shape  opinion via their discourse   (Khamis et al., 2017) .  3,230 tweet digests (wee kly  aggregations of an account’s  blockchain posts) from 176  influencers, averaging 159  words per digest.  Note:   *Correspond s   to orders of worth identified by Boltanski   and Chiapello   (2005)   and   Boltanski and Thévenot (2006) .  Sample Texts   Note:  Google Trends depicts  the volume of user  searches relative to the  maximum volume.This  occurred in Dec 2017 and  is represented as  “blockchain   100 ”.  2012   2013   2014   2015   2016   2017  Civic   1   7   34  Domestic   1   18   51   153   444   514  Industrial   13   24   29  Inspiration   1   1   1   10   13  Market   12   39   171  Network   2   2   12   142  Renown   3   2   26   211   896   2,092  Figure   2 . Google Trends for \" B lockchain\" Searches and Sample Texts  Table   2 . Types of Discursive Fields  Field   Approach to estimating material engagement in   the   field   Material  e ngagement  News media   Randomly sampled 25% of texts (300 articles). Searched newspaper sites for  authors’ bios revealing blockchain - related experience.  2%  Social  influencers  Randomly sampled 110 authors, representing 62% of tweets. Searched their LinkedIn  profiles for evidence of blockchain - related work experience.  14%  Science   Randomly sampled 24% of texts (16 articles). Examined authors’ websites and  LinkedIn   profiles for evidence of authors’ blockchain - related work experience.  44%  Idea  evangelists  Examined LinkedIn profiles of all speakers for evidence of work experience related to  blockchain.  57%  Projects   Examined LinkedIn profiles for authors of 27% of texts (37 proposals) for evidence of  blockchain - related work experience.  73%  Governments   Searched group websites for evidence of regulatory or compliance authority.  Examined individuals’ LinkedIn profiles   for evidence of blockchain - related work  experience.  88%  Corporations   Randomly sampled 34% of texts (76 PRs). Searched Google, LinkedIn, and  Bloomberg for evidence of source firms’ production (e.g., patenting or other  development) or use (i.e., adoption or   deployment) of blockchain.  96%\n\n--- Page 8 ---\n\nMiranda   et   al.   /   Discursive Fields and the Diversity - Coherence Paradox  1428   MIS   Quarterly   Vo l.   4 6   No.   3   /   September   202 2  In the government field, we discerned two types of actors:  government groups and individuals briefing governments. We  searched   group   websites   for   evidence   of   regulatory   and  compliance   authority. For example, legislative branches of the  U . S .   government such as Congress and executive branches such  as the Department of Treasury were coded as materially  engaged because they set and execute policy for technology use.  Individual authors showed up in government documents when  a nongovernment employ ee briefed a government entity or  when a government employee delivered a speech that did not  directly set or represent government policy. In these cases, we  examined individuals’ LinkedIn profiles for evidence of work  experience related to blockchain.  Most   actors in the projects field were not prominent enough to  match with their LinkedIn profiles or used a pseudonym on their  crowdfunding proposal, precluding a match. Therefore, instead  of coding a random sample of authors from this field, we coded  all 37 a uthors we could definitively match to a LinkedIn profile.  Since we did not draw a random sample for this field, we also  coded all proposals   according to   whether the project objective  was to develop or use blockchain - related technologies or simply  acquire o r disseminate knowledge about them. In researching  these proposals, we investigated existing websites referenced in  the proposals for their use of blockchain as well as defunct  websites archived by the Internet Archive. This second set of  codes   revealed   a   slightly   higher   estimate   of   material  engagement than that obtained from coding author experience.  Lexical Framing  At   this   stage,   we   interpreted   and   conceptualized   the  previously surfaced patterns. From the seven candidate topic  models surfaced, we selecte d the 23 - topics model as described  in   Appendix   B.   To   do   so,   our   team   considered   the  interpretability of frames surfaced by the LDA procedure and  of the underlying dimensions surfaced by the MDS procedure.  We next tagged each corpus document with its focal   frame,  i.e., the number of the topic with the highest density.  Core Concepts and Operationalization  Via iterative study of this dataset, we augmented the extant  framing lexicon with the concepts depicted in Table   3 . Based  on the observed material engagemen t reported in Table   2 , we  designated discursive fields as mediated (low engagement),  hybrid   (moderate   engagement) ,   and   enactment   (high  engagement) fields. Next, we identified two concepts that  describe states of the blockchain community discourse and  four   concepts describing field processes contributing to these  states. We quantified these concepts, as per Glaser   (2008) ,  offering alternate indices of each.  Our two state concepts are framing diversity and framing  coherence. Following Miranda et al.   (2015) , w e quantified  diversity   using   the   Herfindahl - Hirschman   Index,   which  captures diversity as variety of frames   (Harrison & Klein,  2007) . Our second diversity metric took advantage of the  MDS results positioning similar topics together. Capturing  diversity as   a   separation of frames, our second diversity index  was   the   average   distance   between   frames   in   Figure   3  (Harrison   &   Klein,   2007) .   Whereas   diversity - as - variety  accounted for quantitative differences in frame use, diversity -  as - separation captured qualitative d ifferences, i.e., frames  evincing different themes. Diversity was computed for two  units of analysis within the community — temporal and field.  The temporal indices permitted diachronic analysis of sources  of framing diversity and its relationship with coher ence. The  field indices permitted its association with field - specific  framing processes.  In keeping with Miranda et al.   (2015) , we measured coherence  as the density of edge - weighted framing networks. In the  frame - frame network, an edge or tie was constitut ed by a field  discussing a frame pair; in the field - field network, an edge was  constituted by a frame being shared by fields. Densities were  divided by the number of participating fields in a year in  frame - frame networks and   by   the number of available fram es  in field - field networks to correct for network size in a year.  Our   four   process   concepts   are   imprinting,   imitating,  retracting,   and   foreshadowing.   Following   Hsu   and   Lim  (2014) , we quantified imprinting as a later recurrence of initial  behaviors. Following   Gaba and Terlaak’s (2013)   view of  imitation as copying, we quantified imitation as the speed with  which   a   field   copied   frames from   other   fields and   the  proportion of frames co pied. We quantified   retracting   as the  decrease in   the   proportion of field texts focusing on a frame.  To assess foreshadowing, we used an approach similar to that  used by   Piezunka and Dahlander (2015)   to assess content  similarity. Specifically, we computed   the average cosine  similarity   between   a   field’s   frame   portfolio   (vector   of  proportions of documents using each frame) in one year and  the community’s frame portfolio in subsequent years. Since  the   evolution of framing across discourse years would   have  undu ly   penalize d   early   participants,   our   primary  foreshadowing index assesse d   the similarity between fields’  frames   from   2015   onward   and   subsequent   community  framing. Our secondary index includes all six years. While  imitating captures vicarious learning, fore shadowing captures  fields’ anticipation of framing that other fields subsequently  find desirable. Based on the metrics in Table   3 , we reduced  the frames in the 4,925 texts to metrics of the two community  state concepts over time and the five process concep ts for the  seven discursive fields.\n\n--- Page 9 ---\n\nMiranda et al. /   Discursive Fields and the Diversity - Coherence Paradox  MIS Quarterly Vo l . 4 6   No.   3   /   September   202 2   1429  Table   3 . Core Concepts and Metrics  Concept   Definition   Indices  Discursive  field type  Extent to which field participants are  materially engaged with innovation  Categorized low engagement fields as   mediated , high  engagement fields as   enactment , and moderate engagement  fields as   hybrid   fields  Community   discourse states  Framing  diversity  Extent to which the community used  disparate frames over time and  across fields  ▪   Variety of frames (1   -   Herfindahl - Hirschman index)  ▪   Separation of frames (Average linear distance between frames)  Framing  coherence  Extent to which the community  converged in its framing   over time  ▪   Edge - weighted density of   frame - frame network, tied by fields  ▪   Edge - weighted density of field - field network, tied by frames  Field   discursive proc esses  Frame  imprinting  Extent to which fields used their  initial frame  ▪   Proportion of 2017 texts using field’s initial frame  ▪   Proportion of annual texts using initial frame  Frame  imitating  Extent to which fields copied frames  originating in other fields  ▪   Imitation rate, i.e., n umber of copied   frame s / number of   frame s  available for copying  ▪   Imitation speed, i.e., a verage speed   of copying frames  Frame  retracting  Extent to which fields abandoned  frames copied from other fields  ▪   Average   year - over - year   loss   in frame use   for   all years  ▪   Average loss in 2017  Frame  foreshadowing  Extent to which a field anticipated  frames used by   the community  ▪   Average cosine similarity between one field’s frame portfolio in one  year and the community’s frame portfolio in subsequent years f rom  2015 onwards (when all fields were participating)  ▪   Average cosine similarity between one field’s framing in one year  and the community’s frame portfolio in subsequent years for all  participating years  Legend:  Frames  1.   Regulatory oversight   2.   Transitioning financial systems   3.   Socio p olitical   solutions  4.   Technological innovation   5.   Virtual payment challenges   6.   The bitcoin promise  7.   Technology   monetization   8.   Healthcare applications   9.   Cryptocurrency exchanges  10.   Transaction settlement   11.   European banking   12.   Startup funding  13.   Infrastructure for Finance   14.   Cultural productions   15.   Cryptocurrency investment  16.   Community   engagement   17.   Smart contract networks   18.   Crypto - entrepreneurship  19.   Country sovereignty   20.   Cryptocurrency watch   21.   Promoting blockchain  22.   Companies exploring blockchain   23.   Blockchain resources  Themes  I: Social ly   embedded   t echnology - in - use   II: Social ly   disembedded   t echnology - in - use  III:Social ly   disembedded   t echnology - in - production   IV: Social ly   embedded   t echnology - in - production  Figure   3 . Blockchain Frames and   Theme s\n\n--- Page 10 ---\n\nMiranda   et   al.   /   Discursive Fields and the Diversity - Coherence Paradox  1430   MIS   Quarterly   Vo l.   4 6   No.   3   /   September   202 2  Types of Discursive Fields  Based on our coding of authors’ material engagement in Table  2 ,   we   characterized   the   discursive   fields   as   mediated,  enactment, and   hybrid. We describe these categories below.  Mediated fields:   We characterized news media and social  influencers as mediated fields. This label highlights that these  field participants’ experience with blockchain was mediated by  others   who   engaged   directly   with   the   technology.   The  “mediated” label also invokes a critical view of modernity as a  condition of mediated experience, i.e., of a normative reality  produced by mass media   (Marcuse, 1964) . Most participants in  these fields were reporters, correspondents, editors, or social  influencers   that   did   not   engage   directly   with   blockchain  technology.   Their   discourse   therefore   was   based   on   the  secondhand   knowledge   acquired   from   participants   directly  engaged with the technology in corporate and government fields,  as well as thirdhand knowledge obtained from other participants  in their own and the other mediated field. Exceptions were  articles authored by materially engaged individuals such as  Coinbase   founder, Fred Ehrsam, that appeared in the   Wall Street  Journal , and tweets by   Roberto Ferrari, chief digital and  innovation officer of the Mediobanca Group.  Enactment fields:   We characterized projects, corporations,  and governments as enactment fields, as   their participants are  doers not just talkers. The “enactment” label invokes the social  constructionist perspective, in which the material and cognitive  worlds are mutually constitutive   (Powell & Colyvas, 2008) .  The knowledge held by field participants de rives from their  firsthand experience with blockchain. “Enactment refers to the  process … through which shared meaning systems are brought  into reality through action”   (Suddaby & Foster, 2017, p. 28) . In  the projects field, blockchain - related funding reque sts entailed  proposals for developing or using blockchain; a few proposals  requested funding for purely informational purposes, such as  participating in training or creating informational materials such  as a billboard or blog about blockchain. In the corpo rate field,  press releases were almost exclusively from firms materially  engaged with blockchain. The exception was press releases  from firms such as FinancialBuzz.com, a financial news  curator, or Research and Markets, a market research firm. In the  gover nment   field,   texts   mainly   represented   either  standardization workshops or knowledge transfer sessions,  precursors to regulations and policies shaping the material  landscape of blockchain. The few texts representing solely  informational engagement in this f ield were briefings and  speeches with passing references to blockchain.  Hybrid fields:   Two discursive fields — idea evangelists and  science — had some participants who were materially engaged  with blockchain and others who were not. We therefore  characterize t hese as hybrid fields.  Diachronic Analyses  Here, we examined patterning of the core process concepts  across the seven discursive fields and six study years. We  performed three types of constant comparisons to assess  theoretical saturation   (Glaser, 2008) .   First, we triangulated  visual, quantitative, and qualitative observations of patterns.  In visualizing relationships among variables, we use d   z - scores  to   facilitate   interpretability.   Second,   we   compared  observations across alternate indices depicted in Tabl e   3   to  assess the consistency of our observations. Because these  assessments showed us that coherence and our four process  concept indices could reliably be considered interchangeable,  we report   the   results of our primary index for each of these  concepts b elow   only   and the other s   in the   A ppendix. Because  Harrison and Klein   (2007)   and our own analyses suggested  that our two diversity indices are related but distinct concepts,  we   report both   indices   below.   Third,   when   considering  correlations among concepts,   we used a jackknifing approach  to establish pseudo - confidence limits on correlation s   (Chin,  1998) . To do so, we recomputed correlations excluding each  observation used to calculate the correlation. Inferences were  drawn only when correlations did not change in direction with  the   exclusion of any observation. This ensured   that   no data  point was a “leve rage” point in determining a correlation  (Rousseeuw & Van Zomeren, 1990) . Appendix C details our  constant   comparisons,   depicting   correlations   between  alternate indices for a concept and among indices for the  different concepts.  Findings:   Toward   an   Eco logical  Perspective   on   Discourse  We now develop an ecological perspective on innovation  discourse, highlighting the hierarchy of and interdependence  among   community   actors   and   mutualism   rather   than  competition, engendering an essential redundancy in framin g.  We summarize our findings in two figures. Figure   3   depicts  the   results of our synchronic analysis that surfaced and  organized frames. Appendix D (and the   O nline   S upplement at  https://osf.io/75yq3/)   provides   illustrative   quotes   for   the  frames.  The spatial proximity of frames in Figure   3   reveals their  (dis)similarity. T he most similar frames are 5 ( VIRTUAL  PAYMENT   CHALLENGES )   and   7   ( TECHNOLOGY  MONETIZATION ) .   These occur within a single quadrant in  Figure   3 — Q uadrant   II.   T he   most   dissimilar   frames — 1  ( REGULATORY   OVERSIGHT )   and   21   ( PROMOTING  BLOCKCHAIN ) — occur   in   diagonally   opposed   quadrants :  Q uadrants I and III . We interpreted the dimensions underlying  Figure   3   by   considering terms constituting frames at the\n\n--- Page 11 ---\n\nMiranda et al. /   Discursive Fields and the Diversity - Coherence Paradox  MIS Quarterly Vo l . 4 6   No.   3   /   September   202 2   1431  extremes. Frames 12 ( STARTUP FUNDING ), 14 ( CULTURAL  PRODUCTIONS ), and   16 ( COMMUNITY ENGAGEMENT ) at the  bottom of the vertical axis, constituted by terms such as  human, music, share, and help, connote aspects of   the social  world   that are   missing from frames such as 10 ( TRANSACTION  SETTLEMENT ) and 13 ( INFRASTRUCTURE FOR FINANCE ) at the  top.   We   therefore   characterized   the   vertical   axis   as   a  continuum   of   social - embeddedness ,   i.e.,   a   social  contextualization   of   frames.   Similarly,   F rames   1   and   2  ( REGULATORY INFRASTRUCTURE ) at the left of horizontal axis,  are constituted by such terms as use,   B itcoin, pay, economic,  market, finance, and risk, signifying blockchain applications.  Frames   21   ( PROMO TING   BLOCKCHAIN ) ,   22   ( COMPANIES  EXPLORING BLOCKCHAIN ) ,   and 23 ( BLOCKCHAIN RESOURCES )  on the right concern blockchain promoters such as   C oin D esk  and   the   Tapscotts,   firms   investigating   and   developing  blockchain applications, and resources such as the   Z cash  p latform. We therefore characterized the horizontal axis as   a  technology - in - use   to   technology - in - production   continuum .  Clockwise   from   the   bottom   left,   the   four   overarching  innovation   themes   thus identified are :   Quadrant   I :   SOCIAL LY  EMBEDDED   TECHNOLOGY - IN - USE ,   Quadrant   II:   SOCIAL LY  DISEMBEDDED TECHNOLOGY - IN - USE ,   Quadrant   III:   SOCIAL LY  DISEMBEDDED TECHNOLOGY - IN - PRODUCTION ,   and   Quadrant  IV:   SOCIAL LY   EMBEDDED TECHNOLOGY - IN - PRODUCTION .  Figure   4   depicts   the   results of our diachronic analyses, i.e.,  how different types of discursive fields framed blockchain  between 2012 and 2017. Each panel (square) in Figure   4  reproduces Figure   3 , depicting fields’ framing across the four  themes. At the top of the figure, we zoom out framing by  social influencers in   2014, showing frames used in at least 5%  of social influencer texts in color. These include the   SOCIAL LY  EMBEDDED   TECHNOLOGY - IN - USE   f rame   6   ( THE   BITCOIN  PROMISE ),   the   SOCIAL LY   DISEMBEDDED   TECHNOLOGY - IN -  PRODUCTION   f rame s   17 ( SMART CONTRACT NETWORKS ) and  21 ( PROMOTING BLOCKCHAIN ), and   the   SOCIAL LY   EMBEDDED  TECHNOLOGY - IN - PRODUCTION   frame   23   ( BLOCKCHAIN  RESOURCES ). Infrequently used frames — appearing in less  than 5% of texts — are depicted in white. These include   the  SOCIAL LY   EMBEDDED TECHNOLOGY - IN - USE   frames 1 , 3, 9,  and 12;   the   SOCIAL LY   DISEMBEDDED TECHNOLOGY - IN - USE  frames 7 and 8;   the   SOCIALLY DISEMBEDDED TECHNOLOGY -  IN - PRODUCTION   frame 15; and   the   SOCIAL LY   EMBEDDED  TECHNOLOGY - IN - PRODUCTION   frames 14,   16, 20, and 22.  Infrequently used frames are omitted in the visualization of  the complete framing ecology of the seven discursive fields.  Each row in the visualization of the framing ecology combines  the frames used by fields of different types; each column  depicts   framin g   within   a   year.   Each   resulting   square  represents the frames used in at least 5% of their texts by  discursive fields of a particular type in a year. The grids within  each   square   depict   framing   across   the   four   themes,  highlighting the qualitative diversity   of frames pursued by  fields across the four themes in addition to the quantitative  diversity of frames pursued. For example, in the left - most  square in the top row of the main figure, we see frame 6   ( THE  BITCOIN PROMISE )   used by both mediated fields and fr ame 9  ( CRYPTOCURRENCY EXCHANGES )   used   by social influencers.  We see these two frames repeated in 2013 (second square in  top row),   al though social influencers dropped frame 6 and  news media added frame 9. In 2014 (third square in top row),  we   see   news   media   adding   frame   3   ( SOCIOPOLITICAL  SOLUTIONS ) and social influencers adding   the   TECHNOLOGY -  IN - PRODUCTION   frames 17, 21, and 23. In 2015 (fourth square  in top row), news media added   the   SOCIALLY DISEMBEDDED  frames   1   ( REGULATORY   OVERSIGHT ),   2   ( TRANSITIONING  FINANCIAL SYSTEMS ), 4 ( TECHNOLOGICAL INNOVATION ), 11  ( EUROPEAN   BANKING ),   12   ( STARTUP   FUNDING ),   and   14  ( CULTURAL PRODUCTIONS ); both mediated fields added frame  10 ( TRANSACTION SETTLEMENT ); and social influencers added  more   TECHNOLOGY - IN - PRODUCTION   frames, i .e., frames 15  ( CRYPTOCURRENCY   INVESTMENT )   and   22   ( COMPANIES  EXPLORING BLOCKCHAIN ). In 2016 (fifth square in top row),  news   media   dropped   frames   2,   9,   and   14 ,   and   social  influencers dropped 15, 22, and 23. In 2017 (sixth square in  top row), news media resu scitated frames 2 and 9 , and   social  influencers dropped frame 10 and resuscitated frames 9, 15,  and 23. Thus, we see increasing diversification of frames in  mediated   fields — in   terms   of   number   as   well   as   type  (representing the gamut of the   SOCIAL - EMBEDDEDNE SS   and  TECHNOLOGY - IN - USE / PRODUCTION   themes) — until   2015,  retrenchment in 2016, and increased diversity in 2017.  In Figure   4 , we also depict the initial frame in each field in the  field’s initial participation year and in 2017 with a gray box.  For example, f rame 6 was the initial frame for news media and  frame 9 for social influencers in 2012. News media retained  frame 6 every year; social influencers dropped frame 9  between 2014 and 2016 but revisited it in 2017. In the fourth  square in   the second r ow, we se e that the initial frame of the  science field, which began participating in 2015, was frame 5.  The science field retained its focus on this frame in the two  subsequent years.  From   Figure   4 ,   we   see   that   news   media   and   social  influencers — discourse   front - run ners — originated   six   and  sixteen frames respectively. Idea evangelists, entering the  discourse in 2013, advanced one new frame. Subsequent  discourse entrants — i.e., projects, governments, science, and  corporations — contributed no new frames but appropriated  f rames originated by the mediated fields and idea evangelists.  We depict frame appropriations with dotted lines (colored red  when the source was news media, gold when the source was  social   influencers,   and   blue   when   the   source   was   idea  evangelists) from the   source to the appropriating field.\n\n--- Page 12 ---\n\nMiranda   et   al.   /   Discursive Fields and the Diversity - Coherence Paradox  1432   MIS   Quarterly   Vo l.   4 6   No.   3   /   September   202 2  Reading Note 1:  Rows in the graphic below combine frame use by  field type — mediated, hybrid, and enactment.  Columns depict fields’ frame use in each study  year — 2012   through 2017. Dotted colored lines  flowing from the mediated and idea evangelist  fields depict frame foreshadowing and imitation.  Reading Note 2:  The graphic to the left zooms out the   2014   framing by  social influencers . Colored frames (i.e., 6, 17,   21, and  23) are used in more than 5% of field texts in 2014.  Uncolored frames were used in fewer than 5% of 2014  texts. Though ommitted from the ecology below, their  status as frame source is captured in the lines flowing  from the 2014 grid to those of oth er fields.  Figure 4. Framing by Discursive Fields over Time  For example, frame 6, which originated with news media in  2012, was appropriated by social influencers the same year;  by science, corporations, and projects in   2015 ,   and by idea  evangelists in 2016. Notably, several frames that appeared in  less than 5% of an originating field’s frames for the year —  especially   in   2014 — were   subsequently   picked   up   and  emphasized by other fields. Examples include frames 2  (picked up b y corporations, governments, and projects in  2016) ;   5 (picked up by science in 2015) ;   8 (picked up by  corporations in 2015, projects in 2016, and governments in  2017) ;   16 (picked up by projects in the same year) ;   and 20  (picked up by corporations in 2015 a nd projects in 2016).  Figure s   3   and   4   reflect the intermediary theorizing devices we  used to understand the framing dynamics.   W e   now   address  2   Alt hough our choice of the   23 - topics model from the six candidate models  was subjective, the fewest topics in an inductively surfaced model was ten  (see Appendix B).  our   research   question   of   how   fields   enable   community  discourse to be both diverse and coherent. In doing so, we  complement   the   visuals in Figure   3   and Figure   4   with  quantitative and qualitative analyses of framing. We first  consider states of community dis course over time and then the  field processes giving rise to these states.  Fields   as   the   Source   of   Diversity   in  Community Discourse (Proposition 1)  Figure   3   depicted the diverse frames advanced in terms of  variety (23 different frames) and separation (four   different  themes) . 2   Moving from left to right in Figure   4 , we see  framing diversity increased over time as the number of\n\n--- Page 13 ---\n\nMiranda et al. /   Discursive Fields and the Diversity - Coherence Paradox  MIS Quarterly Vo l . 4 6   No.   3   /   September   202 2   1433  participating fields increased. This is visible not only in the  increasing number of framing icons for the seven fields but  also in t he disparate positioning of frames. From Figure   5 , we  see that diversity - as - variety and diversity - as - separation both  peaked in 2015, along with the number of participating fields,  while participating actors continued to increase, suggesting  that   new actors   were constrained to us ing   existing frames.  Table   4   quantifies field and actor participation and framing  diversity   and   coherence.   Figure   6   depicts   the   positive  relationships between participation patterns over time and  diversity.  While the slopes in   Figure   6   appear similar at first glance, the  correlations show that the relationship is much stronger for  fields than it is for actors.   The corr elations between framing  diversity and the number of fields engaged in the discourse  across the six years were:   r (variety, fields)   =   0.95 (range of   r   =   0.92  to   r   =   1.00); r (separation, fields)   =   0.88 (range of   r   =   0.79 to   r   =   1.00).  The correlations for th e number of actors were:   r (variety, actors)   =  0.65 (range of   r   =   0.59 to r   =   0.77);   r (separation, actors)   =   0.61 (range  of   r   =   0.51 to   r   =   0.76). The consistently lower correlations  between diversity and actors vis - à - vis diversity and fields  suggest   that   f ields are more important to the diversity in  community discourse than are actors. Further, by 2015, when  all fields were participating in the discourse, the relationship  between diversity and actors became negative:   r (variety, actors)   =  - 0.98 (excluding any of the three annual data points,   r   =  - 1.00) 3 ,   suggesting   that   additional   actors   entering   fields  suppress   the   emergence of novel frames , once all fields have  entered the discourse.  Proposition 1 :   Framing diversity emanates primarily   from  disparate discursive fields, rather than from disparate actors.  The continued prevalence of diverse frames over time shows  that mutualism, rather than competition, prevails in community  framing discourse. That diversity emanates primarily from  fields   highlights the hierarchical ordering of the innovation  community, with discursive fields nested in the community and  actors nested within fields that constrain their autonomy.  Discursive   Field   Processes   Contributing   to  Diversity (Proposition 2)  Turning bac k to Figure   4 , we see notable differences in the  number and types of frames used by different types of  discursive fields. Mediated fields used more frames and  themes   than   enactment   fields.   The   two   mediated   fields  maximized frames used at 16 frames in 2015,   covering all four  themes. In contrast, the most frames used by the three  enactment fields was 9 frames in 2017 ,   and their framing  consistently omitted   SOCIALLY EMBEDDED TECHNOLOGY - IN -  PRODUCTION   frames.  Table   5   quantifies framing diversity within fields a nd four  field processes   that   we will discuss. (Secondary indices for the  processes are in Appendix C.) From Table   5 , we see   that  framing diversity (as variety and separation) within mediated  fields exceeded that of hybrid and enactment fields.  To understand   how   discursive fields contribute to diversity,  we visualize relationships between framing diversity and the  field processes in   Figure   7 . The slopes of the imprinting and  retracting lines show these two processes to be diversity  reducing.   In   contrast,   the   slopes   for   imitating   and  foreshadowing depict them as diversity - inducing processes.  We now consider each field process.  Imprint ing   refers to   the   extent to which actors in a field  persist in using their field’s initial frame .   Imprinting occurs  because actors gravitate toward the familiar   (Phillips et al.,  2004) .   In biology and within organizations, imprinting is  considered learning   that occurs early in the life of an organism  and persistently influences that organism   (Marquis & Tilcsik,  2013) .   To see the imprinting process in action, we return to  Figure   4 . Figure   4   highlights the focal frame in each field’s  first text and its recurr ence as a dominant frame through 2017.  News media discourse began with an April 2012   New York  Times   article focusing on frame 6 ( THE BITCOIN PROMISE ) .   A  Financial Times   article (10/2014) on the role of blockchain in  making “transactions viable for the   poor who cannot afford to  open an account”   i llustrat es   this frame. Figure   4   shows that  this frame garnered consistent field attention across the six  years. Social influencer discourse began with Roger Ver’s  April 2012 frame 9 ( CRYPTOCURRENCY EXCHANGE )   twee t.  Alt hough this frame faded from field attention between 2014  and 2016, Figure   4   shows that   it resurged in 2017, as  illustrated   in   a   December   2017   post by   Chris Skinner:  “Blockchain went mainstream in 2017: Australia is now home  to the world’s first block chain - based stock exchange.”  Science entered the blockchain discourse in 2015 with an  article in   Business Law International ,   focusing on frame 5  ( VIRTUAL PAYMENTS ) . This frame maintained field attention  through 2017, illustrated in an article by Abboushi   (2017)  highlighting the advantages and risks of virtual payments  relative to other payment systems.  3   The separation - actors correlation is not computable for 2015 through 2017  as separation was invariant for the three   years.\n\n--- Page 14 ---\n\nMiranda   et   al.   /   Discursive Fields and the Diversity - Coherence Paradox  1434   MIS   Quarterly   Vo l.   4 6   No.   3   /   September   202 2  Number of authors  835  150  28  132  194  20  154  Figure   5 .   Dynamics of Community Framing Diversity  Table   4 .   Field Participation   and   Community   Framing Diversity   and Coherence   over Time  2012   2013   2014   2015   2016   2017  Participating f ields   2   3   4   7   7   7  Participating actor s   3   14   50   196   545   929  Framing   diversity (variety)   0.39   0.55   0.83   0.97   0.96   0.95  Framing diversity (separation)   0.37   0.43   0.90   0.93   0.93   0.93  Framing coherence (d ensity of frame network )   0.01   0.00   0.14   0.21   0.27   0.32  A: Diversity as Variety   B: Diversity as Separation  Figure   6 .   The Relationship between Field and Actor Participation and Diversity\n\n--- Page 15 ---\n\nMiranda et al. /   Discursive Fields and the Diversity - Coherence Paradox  MIS Quarterly Vo l . 4 6   No.   3   /   September   202 2   1435  Table   5 .   Framing Diversity and   Processes   Across   Fields  Type   Discursive   f ield   Variety   Separation   Imprinting   Imitating   Retracting   Foreshadowing  Mediated   News media   0.94   0.89   10%   0.53   0.04   0.32  Social influencers   0.90   0.93   5%   0.86   0.03   0.37  Hybrid   Science   0.84   0.66   10%   0.43   0.15   0.14  Idea evangelists   0.21   0.47   85%   0.05   0.38   0.17  Enactment   Corporations   0.65   0.82   35%   0.50   0.10   0.23  Projects   0.57   0.85   63%   0.52   0.29   0.18  Governments   0.82   0.55   29%   0.30   0.43   0.19  Note:  Imprinting :   proportion of 2017 texts using   the   field’s initial frame  Imitating:   proportion of available frames a field copied  Retracting:   average   year - over - year loss in frame use  Foreshadowing:   average cosine similarity between field’s frame portfolio in one year and community’s frame portfolio in subsequent  years from 2015 onward  A: Diversity as   V ariety   B: Diversity as   S eparation  Figure   7 . Field Processes Related to Diversity  Idea evangelist discourse began   with   a 2013 TED Talk by  Paul   Kemp - Robertson,   cofounder   of   advertising   firm  Contagious Communication, focusing on frame 3 ( SOCIAL  CONNECTIVITY ) . This frame continued to dominate field texts,  as illustrated by Lorne Lantz’s TED talk (12/2016) touting  blockchain’ s ability to re store   public trust in banks after the  2009 financial crisis and fight counterfeit drugs by enhancing  supp ly   chain transparency.  Corporate discourse commenced with a 2015 press release  from JP Morgan Chase focusing on frame 4 ( TECHNOLOGICAL  INNOVATION ) . This frame commanded field attention   for   the  next two years. An example is a May 2017 press release by  Skuchain,   a   blockchain - based   supply   chain   startup,  announcing its partnership with DCG Connect “to contribute  to the growth of the blockcha in ecosystem.” Project discourse  began with a 2014 Kickstarter proposal by the Europe Crypto  4   One possible concern with our designation of the focal frame in a field’s  first text as the initial frame is that a field might “imprint” on a set of initial  Group for a Bitcoin documentary, focusing on frame 16  ( COMMUNITY ENGAGEMENT ) . This frame commanded field  attention   across   the   next   three   years,   illustrated   by   an  I nvisibleIsrael   (8/2017)   proposal   to   revolutionize   K - 12  education via blockchain - based online courses. Government  discourse commenced in 2015 with a European Commission  webpage describing a workshop on blockchain and digital  currencies   and   focusing   on   frame   4   ( TECHNOLOGICAL  INNOVATION ) . Illustrating this frame is a December 2017  European Union notice of a contest with five €1 million  awards for blockchain solutions “to tackle social innovation  challenges.” This frame continued to command field attention  in t he next two years . 4  Fields imprinted to varying degrees. From Table   5 , we see that  imprinting by mediated fields was substantially lower than  that   by   enactment   fields.   This   stands   to   reason,   given  cognitions rather than on a single frame. See Appendix C for how we  exclude d   this possib ility.\n\n--- Page 16 ---\n\nMiranda   et   al.   /   Discursive Fields and the Diversity - Coherence Paradox  1436   MIS   Quarterly   Vo l.   4 6   No.   3   /   September   202 2  enactment fields’ material investments in a technological  direction. From Figure   7 , we also see an inverse relationship  between field imprinting and diversity:   r (imprinting, variety)   =   ˗ 0.97  (bounds:   r   =   ˗ 0.94 to   r   =   ˗ 0.99);   r (imprinting, separation)   =   ˗ 0.51  (bounds:   r   =   ˗ 0.04   to   r   =   ˗ 0.99).   This   relationship   is  substantially   lower   for   diversity - as - separation   than   for  diversity - as - variety,   suggesting   imprinting   compromises  framing   across   the   different   themes   less   so   than   it  compromises varied framing in general. Nonetheless, the  relationship remains consistent acr oss the two indices. We  therefore propose:  Proposition 2a :   Enactment fields consistently imprint more  than mediated fields, thereby limiting diversity.  With imperfect imprinting even in enactment fields, fields  clearly   learned   from   or   imitated   each   other’s   frames.  Imitating   refers to   a field copying frames originating in other  fields . It is a form of vicarious learning   (Wang & Ramiller,  2009) . Returning to Figure   4 , the dotted lines depict extensive  framing   imitation.   For   example,   frame   1   ( REGULATORY  OVERSIGHT ) was advanced by a social influencer in 2014   and  was   taken up by news media the same year, by science and  governments in 2016, and   by   corporations in 2017. Frame 6  ( THE BITCOIN PROMISE )   emerged in news media in 2012   and  was immediately imitated   by social influencers ; it was   taken  up by science, corporations, and projects in 2015 and by idea  evangelists in 2016. Projects, corporations, science, news  media, governments, social influencers, and idea evangelists  copied 12, 11, 10, 9, 7, 6, and 1 fram e(s) respectively.  Nonetheless, mediated fields imitated a greater proportion of  the frames they did not originate than did any other field and  did so more quickly. Table   5   thus shows consistently more  imitating by mediated fields than by enactment and hybrid  fields. Again, this stands to reason ,   as mediated fields lack  firsthand   knowledge   of   the   innovation   and   must   learn  vicariously. Figure   7   also shows field diversity incr eased with  imitation:   r (imitation, variety)   =   0.70 (bounds:   r   =   0.23 to   r   =   0.81);  r (imitation, separation)   =   0.91 (bounds:   r   =   0.83 to   r   =   0.93). We  therefore propose:  Proposition 2b :   Mediated fields consistently imitate more  than enactment and hybrid fie lds, thereby enhancing diversity .  Because enactment fields also imitated, albeit to a lesser  extent, we investigated what they then did with imitated  frames.   Retracting   refers to   the extent to which actors in  discursive fields abandoned frames previously a ppropriated  from other fields . For example, from Figure   4 , we see that  science and corporations picked up frame 6 ( THE BITCOIN  PROMISE ) in 2015 but jettisoned it thereafter. Governments  imitated   the   idea   evangelist   frame   3   ( SOCIOPOLITICAL  SOLUTIONS ) in 2016 but retracted from it in 2017. The projects  field   also   imitated this frame in 2015   and   retracted from it in  2016, but   then   revisited it in 2017. Thus, frame imitating,  especially by enactment fields,   was   often followed by them  retracting   from these acquired frames.  Table   5   depicts consistently lower numeric retracting by  mediated than by enactment and hybrid fields. Forays into  novel framing areas are less effortful for those in mediated  fields and less risky. Figure   7   shows that field di versity  decreased with retracting:   r (retracting, variety)   =   ˗ 0.59 (bounds:   r   =  ˗ 0.38 to   r   =   ˗ 0.92);   r (retracting, separation)   =   ˗ 0.82 (bounds:   r   =   ˗ 0.76  to   r   =   ˗ 0.94). We therefore propose:  Proposition 2c :   Enactment and hybrid fields   consistently  retract more than mediated fields, thereby limiting diversity.  Foreshadowing   is   the extent to which a field anticipates  frames used by other fields . From Figure   4 , we see that  mediated fields appeared to anticipate enactment and hybrid  field f rames. For example, news media first advanced frame 5  ( VIRTUAL PAYMENT CHALLENGES ) in 2014. This was the  science field’s initial frame in 2015, when nine published  articles used this frame. The focus of the November 6, 2014 ,  Wall Street Journal   article adv ancing the frame was the arrest  of a man for operating a Silk Road 2.0 site, using   B itcoin to  preserve transaction anonymity. All four articles appearing in  April   2015,   the   month   of   the   earliest   science   articles,  referenced the Silk Road. Three of the acad emic articles  directly referenced the   Wall Street Journal   article or facts  cited by the   Wall Street Journal   article in framing blockchain  as   enabling   underworld   commerce.   Foreshadowing   a  dominant projects field frame, social influencer, Roger Ver,  tweeted   his arrival at the #bitcoinmiami conference in January  2014, along with members of his blockchain team.   F rame 16  ( COMMUNITY ENGAGEMENT ) , which was   advanced in the  tweet ,   was taken up later that year in two project field  proposals — to make a   B itcoin document ary and to attend a  conference. Consistent with the originating field, the focus of  this frame in the project field   was   initially on community  learning; later, the project field focus with this frame shifted  to resource production by and for communities.  From Table   5 , we see that mediated fields foreshadowed other  fields   the   most.   Less   anchored   by   their   own   material  engagement,   mediated   fields   necessarily   observe   and  articulate the tacit knowledge performed within enactment  fields. Figure   7   shows that field diversity increased with  foreshadowing:   r (foreshadowing, variety)   =   0.52 (bounds:   r   =   0.41 to  r   =   0.73);   r (foreshadowing, separation)   =   0.70 (bounds:   r   =   0.60 to   r   =  0.83). We therefore propose:  Proposition 2d :   Mediated   fields   foreshadow   more   than  enactment and hybrid fields, thereby increasing diversity.\n\n--- Page 17 ---\n\nMiranda et al. /   Discursive Fields and the Diversity - Coherence Paradox  MIS Quarterly Vo l . 4 6   No.   3   /   September   202 2   1437  Earlier, we noted that actor autonomy is constrained by  discursive fields. Our analyses of the field framing processes  reveal fields and actors   and   shed further light on autonomy  constraints   across   fields.   On   the   one   hand,   their   direct  innovation experienc e permits enactment fields independence  in framing, requiring less imitating and foreshadowing of  frames   emanating   from   other   fields.   Yet,   with   higher  imprinting and retracting, field actors evince lower framing  autonomy. In contrast, without an experienti al basis for their  framing, mediated fields evince greater dependence on other  discursive fields, foreshadowing and imitating their framing.  Yet with lower imprinting and retracting, field actors evince  higher framing autonomy than those in enactment field s.  Coevolution   of   Community   Discourse  Diversity and Coherence (Proposition 3)  Next, we   sought   to understand the relationship between  diversity and coherence. Both diversity and coherence trended  upward across the six study years, both relating positively   t o  time:   r (time, variety)   =   0.91 (bounds:   r   =   0.83 to   r   =   0.96);   r (time,  separation)   =   0.85 (bounds:   r   =   0.74 to   r   =   0.90);   r (time, coherence)   =  0.98 (bounds:   r   =   0.97 to   r   =   1.00). Figure   8   depicts this  coevolution across the six years.  How did this happen? Within the heavy gray line designating  the evolution of coherence in Figure   8 , we see the annual  networks   of   frames.   These   frame - frame   networks   were  constituted   by   field — especially   media ted   field — use   of  multiple frames. Returning to Figure   4 , we see   that the   social  influencer use of frames 6 and 9 in 2012 provided the tie  visible in Figure   8   for that year. By 2017, we see many more  ties forged as fields expanded their framing repertoires.  Redundant frames across fields then cinched individual fields’  framing networks into a coherent whole. For example, frame  10, redundant across news media, science, and governments,  cinched together the framing networks of those three fields,  connecting 11   frames; redundancy of frame 9 between news  media and social influencers further expanded this connected  network to 16 frames, while redundancy of frame 4 with  science and projects expanded the connected network further  to 17 frames. Redundancy in fields’   use of frame pairs  contributed redundant ties, further strengthening this network.  For   example,   news   media,   science,   and   governments  combined   the   use of frames 4 and 10 in 2017.  To better understand how diversity contributes to coherence,  consider the fram ing by two actors in 2015. Izabella Kaminska  5   We focused solely on the focal frame within a text. But from Appendix B,  we know that texts used multiple frames. Coherence based on analysis that  accounts for linkages forged within texts could potentially be higher.  authored   three   Financial   Times   articles   using   frame   2  ( TRANSITIONING FINANCIAL SYSTEMS ), two using frame 3  ( SOCIOPOLITICAL   SOLUTIONS ),   one   using   frame   10  ( TRANSACTION SETTLEMENT ), and two other articles using tw o  more frames. Her frame 2 articles described efforts by  financial sector leaders to deploy blockchain, her frame 10  article noted the technology’s disruption of “longstanding  financial bookkeeping practices , ” and her frame 3 articles  considered sociopolitical threats   posed by   the technology. Ms.  Kaminska   thus   revealed   her   own   understanding   of   the  relatedness of these frames and promoted this understanding  to her readers. IBM   blockchain business development le ader  and social influencer, Dave Maddox, also tweeted repeatedly  using   frame   3 ,   frame   10,   and   six   other   frames,   thus  communicating his synthesized understanding of the eight  frames — or of blockchain as simultaneously   being   eight  different things . 5   From the   overlap in Ms. Kaminska’s articles  and Mr. Maddox’s posts through frames 3 and 10, their  readers can potentially synthesize their eleven different views  of blockchain.  Thus, as blockchain framing   became   more diverse over time,  it also got more coherent.   Belying the belief that coherence  requires   singular   framing,   our   data   shows   a   positive  relationship between   diversity and coherence:   r (variety, coherence)   =  0.93 (bounds:   r   =   0.91 to   r   =   0.95), r (separation, coherence)   =   0.91  (bounds:   r   =   0.87 to   r   =   0.96) 6 . As framing diversity increases,  ties among frames become denser, permitting coherence to  evolve not just alongside but through diversity. We therefore  propose:  Proposition 3 :   Framing diversity and framing coherence  coevolve over time.  Discussion  The   study of innovation discourse   has been beleaguered by the  implicit paradox of how innovation communities can permit  and benefit from states of diversity   and   coherence. Figure   9  summarizes   insights   gleaned   from   our   study   of   early  blockchain   discourse.   It   de picts   how   differentiated   field  framing processes within the discourse ecosystem contribute  concurrently to discourse diversity and coherence. First, we  find   that   discursive fields matter to the diversity of frames that  proliferate within an innovation comm unity. Specifically, we  found diversity to be a function of the number of participating  fields, rather than the number of participating actors.  6   The very high correlations between   diversity and coherence may raise a  question   of whether the indices are assessing   distinct concepts. See  Appendix E for an explanation.\n\n--- Page 18 ---\n\nMiranda   et   al.   /   Discursive Fields and the Diversity - Coherence Paradox  1438   MIS   Quarterly   Vo l.   4 6   No.   3   /   September   202 2  Figure   8 . Diversity and Coherence over Time  Figure   9 .   The Ecology of   Innovation Community Discourse  Second, enactment and mediated fields play different roles in  the innovation discourse. Enactment fields erect discourse  walls through imprinting and retracting, thereby constricting  their discourse arenas. In contrast, mediated fields build  bridges by for eshadowing and imitating other fields, thereby  expanding   their   discourse   arenas.   Third,   through   the  expansive, redundancy - inducing diversity of mediated fields,  coherence coevolves with diversity. Thus, while enactment  fields are indisputably the source of   tacit knowledge about an  innovation, our research points to   the   explication of this  knowledge by mediated fields as salient in resolving the  diversity - coherence paradox.  Implications for Discursive Perspectives on  Innovation Diffusion  We now consider   the   implications of our work for three  research streams.\n\n--- Page 19 ---\n\nMiranda et al. /   Discursive Fields and the Diversity - Coherence Paradox  MIS Quarterly Vo l . 4 6   No.   3   /   September   202 2   1439  Perspectives on Diversity and Coherence  Prior research has attributed framing diversity to diverse  discourse   participants   and   stakeholders   (Barrett,   1999;  Davidson, 2002; Gallivan, 2001;   Orlikowski & Gash, 1994) .  Alt hough some of these scholars have alluded to stakeholder  groups   and   organizational   subcultures,   we   clarify   and  elaborate on discursive fields as the basis for these disparate  groups and subcultures.  The fundamental paradox moti vating this research was that  states of framing diversity and coherence, both necessary for  innovation diffusion, seemed to stand in opposition to each  other. Innovation diffusion research has noted the importance  of both diversity and coherence to diffusi on   (Currie, 2004;  Hsu, 2006; Kaplan & Tripsas, 2008; Miranda et al., 2015;  Swanson & Ramiller, 1997) . Here we explored the interplay  between framing diversity and coherence, invoking a view of  coherence as relatedness   (Miranda et al., 2015; Wang &  Swanson,   2008) . This view resonates with research showing  that learning is enhanced by chunking   (e.g., Thalmann et al.,  2019) . In other words, a diversity of frames can be coherent if  perceived as related.  By recasting what is meant by coherence and spotlighting t he  unique role of mediated fields in the framing discourse, we  show that diversity and coherence coevolve. Our findings  revealed   that   the   collection   of   fields   in   an   innovation  community advance s   diverse frames in accordance with their  disparate preferences . Thus, as the number of participating  fields increased, so too did the diversity of frames across the  community.   Enactment   fields   managed   coherence  conventionally — by reducing within - field diversity through  processes of imprinting and retracting. In doing   so, they acted  as framing walls. However, mediated fields acted as bridges  among discursive fields, promoting community discourse  coherence to increase even as its diversity increased. Through  processes of imitating and foreshadowing, mediated fields  enhan ce other fields’ ability to see their frames as widely held.  Such   a   perception   of   shared   frames   enhances   actors’  confidence in their choices   (Heath & Gonzalez, 1995)   and  therefore   would be expected to   promote diffusion.  Community Learning Perspectives  Wang   and Ramiller   (2009)   proposed public discourse as a  vehicle for learning about innovations. However, learning is  not unfettered. Rather, actors are cognitive misers   (Fiske &  Taylor, 1991) , susceptible to bounded rationality   (Simon,  1997) . Learning, therefo re, tends to be localized to the  neighborhood   of   actors’   existing   knowledge   (Stuart   &  Podolny,   1996) .   Further,   cross - field   learning   encounters  cultural barriers ,   as fields have distinct repertoires, i.e., jargon  and values   (Lamont & Thévenot, 2000) , and th e material  purposes/values of fields, which inform and are shaped by  their discourse, differ   (Pentland & Singh, 2012) . Our research  shows that this is particularly true for enactment fields, which  erect walls circumscribing their actors’ learning. Our stud y  extends community learning research   (Wang & Ramiller,  2009)   in   two   ways:   by   noting   that   learning   is   not  unconstrained and showing which fields are more active  cross - field learners.  Ecological Perspectives on Innovations  Our conceptualization of the discu rsive ecosystem highlights  the value of extending an ecology perspective from   material  innovation   ecosystems   to   also   include   discursive  ecosystems. It extends that of Wang   (2021, p. 398) , for  whom focal actors include “giants like Apple and Google …  application   developers,   accessory   vendors,   service  providers, and users.” In our work, we view noncorporate  actors   such   as   news   media   and   social   influencers,  governments, scientists ,   and idea evangelists,   as well as  individuals   pursuing   low - stakes   crowdfun ding   as   also  playing   germane   roles   in   the   discursive   ecosystem,  contributing   to   diverse   yet   coherent   framing   of   focal  innovation s .  Our   work   depicts   the   hierarchical   ordering   of   actors,  discursive fields, and the innovation community comprising  the ecosyst em   (Rao, 2017; Wang, 2021) . It elaborates on the  ecological perspective of autonomy and interdependence  (Wang, 2021) , showing that material engagement by actors  within   a   discursive   field   impinges   on   the   autonomy -  interdependence dynamic. Specifically, we sa w that while  enactment fields enjoy a level of autonomy in their pursuit  of frames, actors within the fields experience constraint. In  contrast, while discursive fields depend on enactment fields  for their framing, their actors are less constrained in thei r  pursuit of alternate frames.  Though we followed only the first six years of blockchain  discourse,   which   may   be   characterized   as   the   “era   of  ferment” that permits variation and precedes a shakeout  (Anderson & Tushman, 1990) , our research opens up the  pos sibility that mutualism, rather than competition, may  characterize the innovation discourse s   that social ly   construct  digital innovations. Consistent with Low et al.   (2003) , who  noted that diversity and redundancy go together, our study  demonstrates that di versity and coherence, emanating from  redundancies   in   framing   across   fields,   coevolve.  Alternatively   put,   discourses   emanating   from   different  discursive fields coevolve as one resonates with another,  each crafted based on its own discursive logic   (Seidl, 2 007) .\n\n--- Page 20 ---\n\nMiranda   et   al.   /   Discursive Fields and the Diversity - Coherence Paradox  1440   MIS   Quarterly   Vo l.   4 6   No.   3   /   September   202 2  Limitations   and   Suggestions   for   Future  Research  Three limitations constrain our contributions to the innovation  discourse literature. First,   al though we used rigorous topic  modeling techniques, our choice among topic models was  subjective. Thus, des pite our alternate indices, we cannot  exclude the possibility that choosing the 23 - topics model may  have   shaped   our   later   observations   of   framing   across  discursive fields. Second, we limited our investigation to  blockchain.   It   is   p ossibl e   that   more   tangibl e,   discrete  technologies can cohere quickly with limited diversity. Third,  we focused on what Hardy and Maguire   (2016, p. 85)   term  “grand   muscular   ‘big   D’   Discourses”,   rather   than   the  “localized   ‘little   d’   discourses”   that   happen   in   private  conversations.   This constrained our ability to fully understand  how imprinting and foreshadowing come about. It is possible,  for example, that the imprinting we observed was not so much  because the initial text in our dataset formed the subsequent  cognitions for a discur sive field but rather because the initial  text reflected a field frame previously formulated through  private   discourse.   Similarly,   the   exact   nature   of  foreshadowing of enactment fields by mediated fields was  hard to discern. Did mediated fields interpret a ctions by  enactment fields and supply the vocabulary with which they  subsequently described their own actions? Or did enactment  fields possess the linguistic resources to describe their own  actions but were simply heralded by mediated fields?  Alt hough we   have advanced understanding of framing by  discursive   fields,   our   work   leaves   several   unanswered  questions. First, studies of the role of social media in  diffusion currently focus mainly on Twitter   (Vaast et al.,  2013) . Our work highlights the heterogeneity   of actors on  social media. The three categories of actors we studied on  social media — idea evangelists, project proposers, and social  influencers — used   different   platforms   and   undertook  framing differently. Innovation and diffusion research will  benefit fro m disentangling the   relative   contributions of  disparate actors from social media platforms to discourse  and diffusion. Relatedly,   o n e   unanswered question is:   O n  which platforms and by which fields does framing matter  most to innovation and diffusion?  Second, though we noted the tendency for mediated fields  to foreshadow framing by enactment and hybrid fields, we  do not know how this happened. Future research should  investigate whether foreshadowing by mediated fields is  a sensegiving process, in which   enactment and hybrid  fields assume frames crafted by mediated fields,   or   a  sensetaking process, whereby mediated fields vicariously  learn from the materially engaged but supply them with a  discursive vocabulary.  Third, in our analyses, we focused on the do minant frames  emanating   from   the   disparate   discursive   fields.   Future  research may wish to attend to the qualities of framing  outliers,   especially   in   enactment   fields,   and   the  consequences of their marginalized frames. Additionally,  initial texts that impri nted frames on fields did not always  originate with the most pedigreed source — for example, the  science field’s first blockchain article in   Business Law  International . Future research should consider the relevance  of source pedigree in shaping innovation di scourse. Finally,  one of the reasons why empirical work on concepts related  to meaning has lagged is the difficulty   in   assessing these  concepts   (Zilber, 2017) . Our work shows that computational  techniques such as topic modeling provide an effective way  of   assessing shared meaning.  Implications for Practice  Our study suggests that a singular view of innovation is both  misleading and “bad for practice”   (Ghoshal & Moran, 1996) .  It is misleading because   while   a singular, “dominant design”  view of innovations ma y have been necessary when the focal  innovation was a videotape and only a VHS or Betamax  standard   could   survive,   today’s   digital   era,   in   which  complementary   digital   innovations   are   dematerialized,  permits   competing   ideas   about   and   enactments   of   an  innovat ion to coexist. Failing to recognize this is bad for  practice because it could lead the vendors and lead users who  serve as innovation ambassadors and gatekeepers to squelch  viable   ideas   about   an   innovation   and   future   innovation  trajectories.   Further, when   executives consider different ways  of engaging with an innovation, they may feel compelled to  orient themselves toward a dominant frame because they  believe it will resonate best with their stakeholders and will be  the surest way for them to obtain buy - in . But such an approach  could limit future options. Executives   should   therefore be  open to diversity too.   Failing to embrace a mutualistic view  also is more likely to focus scholarly and practitioner attention  on   privileged   actors   such   as   large   firms,   which   have  historically been viewed as the legitimate actors in framing  innovations   and   possess   the   resources   to   articulate   and  promote their visions for innovations. In contrast, peripheral  actors such as startups, often the source of radical innovation,  lack the legitimacy and resources to promote a dominant  vision for an innovation   (Sgoure v, 2013) . We therefore  suggest   that   a   mutualistic   view   is   ethical   because   it  enfranchises all actors; it is   also   competitive   because it draws  attention to alternate innovation frames, from which further  innovation may emanate. It is thus good for practice.  Our study also suggests prospective adopters and developers  monitor   mediated   fields   for   emerging   innovations   and  enactment fields for more focused competitive intelligence.\n\n--- Page 21 ---\n\nMiranda et al. /   Discursive Fields and the Diversity - Coherence Paradox  MIS Quarterly Vo l . 4 6   No.   3   /   September   202 2   1441  Further, developers may be well - served   to   monitor social  influencers for technolo gy - in - production frames. Lastly, our  study suggests   that   policymakers attend to the range of frames  that emerge across an innovation community to provide more  comprehensive regulatory support. Failure to do so risks  overlooking minority   use cases for — or co ncerns about — the  innovation that   may disadvantage a   specific group. For  example,   compared   to   frame   9   ( CRYPTOCURRENCY  EXCHANGES ) , frame 15 ( CRYPTOCURRENCY INVESTMENT ) ,  and frame 17 ( SMART CONTRACT NETWORKS ) , which were the  focus of 282, 299, and 343 documen ts ,   respectively, frame 14  ( CULTURAL   PRODUCTIONS )   was the focus of a mere 54  documents. Coincidentally, while the U . S .   Congress has  introduced 18 bills on cryptocurrencies and smart contracts in  2021   (Brett, 2021) , little regulatory attention has been paid   to  blockchain - based NFTs as an art form. This despite one piece  of digital art having commanded $69 million at a Christie’s  auction, suddenly defunct hosting sites making patrons’ art  purchases unavailable, and extensive teen participation in the  NFT art   market   (Kastrenakes, 2021; Kurutz, 2021) .  To obtain a practitioner perspective on the relevance of our  study, we performed the applicability checks recommended  by   Rosemann and Vessey (2008)   using three focus groups  comprising 10 panelists with diverse bac kgrounds. (See   the  O nline   S upplement   at   https://osf.io/75yq3/   for   details .)  Panelists appreciated the need to monitor innovations across  diverse fields, noting that they primarily monitored activities  by firms in their own industries, Gartner, news media, social  influencers on Twitter, and Reddit technology communities.  However ,   l ike   other   ecological   work   that   is   rich   in  descriptions of complex arenas but eludes facile prescriptions,  collaborative work between academics and practitioners is  needed to ascertain how decision   makers can leverage insights  about   coherence   via   diversi ty   toward   more   effective  technology choices.  Conclusion  We set out to disentangle the paradox of diversity and  coherence in community discourse about IT innovations to  answer   the question :   H ow can an innovation community’s  framing   discourse   be   both   diverse   and   coherent?   We  addressed this question in the context of the multifaceted  blockchain technology. In doing so, we highlighted the key  role   of   discursive   fields   in   the   ecology   of   innovation  discourses. Fields are the source of framing diversity across  the   community. Enactment fields, however, erected walls,  pursuing   homogeneous   framing   through   processes   of  imprinting and retracting. In contrast, mediated fields pursued  diverse   framing   through   processes   of   imitating   and  foreshadowing.   Their   diverse   framing   engendered  redundancies — or bridges — among frames and   with   other  fields. This redundancy enabled the blockchain community’s  discourse to simultaneously become more diverse and more  coherent over time.  Acknowledgments  We are deeply grateful to the   senior edito r, Andrew Burton - Jones,  and   associate ed itor, Nick Berente, for their generosity with their  insights and patient guidance as we developed this paper. We also  are indebted to the four anonymous reviewers. Together,   our review  team epitomized the dialectical process at its very best. Special  thanks to Ping Wang, Malmi Amadoru, Elizabeth Davidson, Carol  Saunders, and Inchan Kim for their commentary on earlier versions  of the paper, and to faculty and Ph . D .   students at   the University of  Oklahoma’s MIS seminar series and informal brown bags for their  time and critique. We are much obliged to Pascal Nitiema for  downloading the Twitter data and introducing us to the   R   LDAvis  package,   and   to   Surya   Miranda   for   her   help   gather ing  crowdfunding data and developing some interim visualizations.  References  Abboushi, S. (2017). Global   virtual currency :   Brief   o verview.  Journal of Applied Business & Economics ,   19 (6).  Anderson,   P.,   &   Tushman,   M.   L.   (1990).   Technological  discontinuitie s and dominant designs: A cyclical model of  technological change.   Administrative Science Quarterly ,   35 (4),  604 - 633.  Barnett, W. P., & Carroll, G. R. (1987). Competition and mutualism  among   early   telephone companies.   Administrative Science  Quarterly ,   32 (3) , 400 - 421.  Barrett, M., Heracleous, L., & Walsham, G. (2013). A rhetorical  approach to IT diffusion: Reconceptualizing the ideology -  framing   relationship   in   computerization   movements.   M IS  Quarterly ,   37 (1), 201 - 220.  Barrett, M. I. (1999). Challenges of EDI   adoption for electronic  trading in the London Insurance Market.   European Journal of  Information Systems ,   8 (1), 1 - 15.  Berente,   N.,   Seidel,   S.,   &   Safadi,   H.   (2019).   Data - driven  computationally intensive theory development.   Information  Systems Research ,   30 ( 1), 50 - 64.  Birks, D. F., Fernandez, W., Levina, N., & Nasirin, S. (2013).  Grounded theory method in information systems research: its  nature,   diversity   and   opportunities.   European   Journal   of  Information Systems ,   22 (1), 1 - 8.  Blei,   D.   (2009).   Topic   m odels .   Available   at  http://videolectures.net/mlss09uk_blei_tm/  Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent   D irichlet  allocation.   T he   Journal of   M achine Learning   R esearch ,   3 , 993 -  1022.  Boltanski, L., & Chiapello, E. (2005 ).   The   new spirit of c apitalism .  Verso.  Boltanski, L., & Thévenot, L. (1999). The sociology of critical  capacity.   European   J ournal of   Social Theo ry ,   2 (3), 359 - 377.  Boltanski, L., & Thévenot, L. (2006).   On   j ustification: Economies  of wor th . Princeton Univ ersity Press.\n\n--- Page 22 ---\n\nMiranda   et   al.   /   Discursive Fields and the Diversity - Coherence Paradox  1442   MIS   Quarterly   Vo l.   4 6   No.   3   /   September   202 2  Boucher, D. H., James, S., & Keeler, K. H. (1982). The ecology of  mutualism.   Annual Review of Ecology and Systematics ,   13 (1),  315 - 347.  Brett, J. (2021). Congress   has introduced 18 bills on crypt o   a nd  Blockchain   i n   2021.   Forbes .   https://www. forbes.com/sites/  jasonbrett/2021/08/22/congress - has - introduced - 18 - new - bills -  on - crypto - and - blockchain - in - 2021/?sh   =   1c8703d9263b  Bunduchi, R., Smart, A., Charles, K., McKee, L., & Azuara -  Blanco, A. (2015). When innovation fails: An institutional  perspectiv e of the (non)adoption of boundary spanning IT  innovation.   Information & Management ,   52 (5), 563 - 576.  Chiapello, E., & Fairclough, N. (2002). Understanding the new  management ideology: A transdisciplinary contribution from  critical discourse analysis and n ew sociology of capitalism.  Discourse & Society ,   13 (2), 185 - 208.  Chin, W. W. (1998). The partial least squares approach to structural  equation   modeling.   In   G.   A.   Marcoulides   (Ed.),   Modern  methods   for   business   research   (pp.   295 - 336).   Lawrence  Erlbaum Associates.  Cloutier,   C.,   &   Langley,   A.   (2013).   What   makes   a   process  theoretical   contribution?   Journal   of   Management   Inquiry ,  22 (4), 360 - 380.  Cooren, F., Fairhurst, G., & Huët, R. (2012). Why matter always  matters in (org anizational) communication. In P. M. Leonardi,  B. A. Nardi, & J. Kallinikos (Eds.),   Materiality and organizing:  Social interaction in a technological world   (pp. 296 - 314).  Oxford University Press.  Currie, W. L. (2004). The organizing vision of application   service  provision:   a   process - oriented   analysis.   Information   and  Organization ,   14 (4), 237 - 267.  Davidson, E. J. (2002). Technology frames and framing: A socio -  cognitive investigation of requirements determination.   M IS  Quarterly ,   26 (4), 329 - 358.  Davis, J. P ., & Aggarwal, V. A. (2020). Knowledge mobilization  in   the   face   of   imitation:   Microfoundations   of   knowledge  aggregation and firm‐level innovation.   Strategic Management  Journal ,   41 (11), 1983 - 2014.  DiMaggio, P. (1997). Culture and cognition.   Annual   R eview o f  S ociology ,   23 , 263 - 287.  DiMaggio, P., Nag, M., & Blei, D. (2013). Exploiting affinities  between topic modeling and the sociological perspective on  culture:   Application   to   newspaper   coverage   of   U . S .  government arts funding.   Poetics ,   41 (6), 570 - 606.  Durand, R., & Thornton, P. H. (2018). Categorizing institutional  logics,   institutionalizing   categories:   A   review   of   two  literatures.   Academy of Management Annals ,   12 (2), 631 - 658.  Entman, R. M. (1993). Framing: Toward clarification of a frac tured  paradigm.   Journal of   C ommunication ,   43 (4), 51 - 58.  Essén, A., & Värlander, S. W. (2019). How technology - afforded  practices at the micro - level can generate change at the field  level:   Theorizing   the   recursive   mechanism   actualized   in  Swedish   rheumatolog y   2000 - 2014.   M IS   Quarterly ,   43 (4),  1155 - 1176.  Fayard, A. L., Gkeredakis, E., & Levina, N. (2016). Framing  innovation   opportunities   while   staying   committed   to   an  organizational   epistemic   stance.   Information   Systems  Research ,   27 (2), 302 - 323.  Fillmore, C. J . (1975). The   future of se mantics. The   scope of  A merican   lingu istics .   In   Papers   of   the   First   Golden  Anniversary Symposium of the Linguistic Society of America .  Fiske, S., & Taylor, S. (1991).   Social   c ognition . McGraw - Hill.  Fiss, P. C., & Hirsch, P. M. (2005). The discourse of globalization:  Framing and sensemaking of an emerging concept.   American  Sociological Review ,   70 (1), 29 - 52.  Foucault, M. (1972).   The   archaeology of knowledge . Vintage  Books.  Foucault, M. (1977).   Discip line and   p unish: The   birth of the prison .  Random House.  Foucault, M. (1980).   Power/ k nowledge: Selected   interviews and  other writing s, 1972 - 1977 . Harvester Press.  Gaba, V., & Terlaak, A. (2013). Decomposing uncertainty and its  effects   on   imitation   in   firm   exit   decisions.   Organization  Science ,   24 (6), 1847 - 1869.  Gallivan,   M.   J.   (2001).   Meaning   to   change:   How   diverse  stakeholders   interpret   organizational   communication   about  change   initiatives.   IEEE   Transactions   on   Professional  Communication ,   44 (4), 243 - 266.  Gandrud, C. (2015 ) .   A   link between   topicmodels LDA and LDAvis .  R - bloggers.   https://www.r - bloggers.com/a - link - between -  topicmodels - lda - and - ldavis/  Ghoshal, S., & Moran, P. (1996). Bad for practice: A critique of the  transaction cost theory.   Academy of Manag ement Review ,  21 (1), 13 - 47.  Glaser,   B.   G.   (2008).   Doing   quantitative   grounded   theory .  Sociology Press.  Grün,   B.,   &   Hornik,   K.   (2017).   Topic   m odels:   Package  “ topicmodels .”   In   R   (Version   0.2 - 7).   https://cran.r -  project.org/web/packages/topicmodels/topicmodels.pdf  Hachten, W. A. (2005).   The   troubles of jour nalism: A   critical look  at what's right and wrong with the press .   Routledge.  Hardy, C., & Maguire, S. (2010). Discourse, field - configuring  events, and change in organizations and institutional fields:  Narratives of DDT and the Stockholm Convention.   Academy of  Management Journal ,   53 (6), 1365 - 1392.  Hardy, C., & Maguire, S. (2016). Organizing risk: Discourse,  power, and “riskificat ion . ”   Academy of Management Review ,  41 (1), 80 - 108.  Harford, T. (2015, March 6). Boom or bust for bitcoin?   Financial  Times .  Hargadon, A., & Sutton, R. I. (1997). Technology brokering and  innovation in a product development firm.   Administrative  Science Quar terly ,   42 (4), 716 - 749.  Harrison, D. A., & Klein, K. J. (2007). What ’ s the difference?  Diversity constructs as separation, variety, or disparity in  organizations.   Academy of Management Review ,   32 (4), 1199 -  1228.  Hauser, G. A. (1999).   Vernacular   v oices: The   rhetoric of publics  and public sphere s . University of South Carolina Press.  Heath, C., & Gonzalez, R. (1995). Interaction with others increases  decision confidence but not decision quality: Evidence against  information collection views of interactive dec ision making.  Organizational   Behavior   a nd   Human   Decision   P rocesses ,  61 (3), 305 - 326.  Heracleous, L. (2016). Discourse theory. In A. Langley & H.  Tsoukas (Eds.),   The Sage   handbook of process organ ization  Studies   (pp. 190 - 203). S AGE .  Hsu,   D.   H.,   &   Lim,   K.   (2014).   Knowledge   brokering   and  organizational   innovation:   Founder   imprinting   effects.  Organization Science ,   25 (4), 1134 - 1153.\n\n--- Page 23 ---\n\nMiranda et al. /   Discursive Fields and the Diversity - Coherence Paradox  MIS Quarterly Vo l . 4 6   No.   3   /   September   202 2   1443  Hsu, G. (2006). Jacks of all trades and masters of none: Audiences'  reactions   to   spanning   genres   in   feature   film   production.  Administrative Science Quarterly ,   51 (3), 420 - 450.  Iansiti, M.,   &   Lakhani, K. R.   ( 2017 ) . The   t ruth   a bout   b lockchain .  Harvard Busin ess Review ,   January - February , 3 - 11.  Ingram, P., & Baum, J. A. (1997). Opportunity and   c onstraint:  Organizations’   learning from the operating and competitive  experience   of   in dustries.   Strategic   Management   Journal ,  18 (S1), 75 - 98.  Jarrett,   D.   (1984).   Pragmat ic   coherence in an oral   formulaic  tradition : I   can read your lette rs /   sure can ’ t read your mind.   In  D. Tannen (Ed.),   Coherence in   spoken and written discou rse  (pp. 155 - 171). ABLEX.  Kaplan, S., & Tripsas, M. (2008). Thinking about technology:  Applying a c ognitive lens to technical change.   Research Policy ,  37 (5), 790 - 805.  Kastrenakes,   J.   (2021).   Your   million - dollar   NFT   can   break  tomorrow   if   you’re   not   careful.   The   Verge.  https://www.theverge.com/2021/3/25/22349242/nft - metadata -  explained - art - crypto - urls - links - ipfs  Khamis, S., Ang, L., & Welling, R. (2017). Self - branding,   “ micro -  celebrity ”   and the rise of   social media infl uencers.   Celebrity  Studies ,   8 (2), 191 - 208.  King, L.   (2007). Charting a discursive field: Environmentalists for  U . S .   population stabilization.   Sociological Inquiry ,   77 (3), 301 -  325.  Kurutz, S. (2021). Teens   cash in on   the NFT   art boom .   The New  York Times . https://www.nytimes.com/2021/08/14/style/teens -  nft - ar t.html  Lamont, M., & Thévenot, L. (2000). Rethinking comparative  cultural sociology.   Cambridge University Press.  Lin, A., & Silva, L. (2005). The social and political construction of  technological   frames.   European   Journal   of   Information  Systems ,   14 (1), 49 - 59.  Lok, J., & Willmott, H. (2006). Institutional theory, language, and  discourse analysis: A comment on Philips, Lawrence, and  Hardy.   Academy of Management Review ,   31 (2), 477 - 480.  Low, B., Ostrom, E., Simon, C., & Wilson, J. (2003). Redundancy  and diver sity:   D o they influence optimal management. In F.  Berkes, J. Colding, & C. Folke (Eds.),   Navigating social -  ecological systems: building resilience for complexity and  change   (pp. 83 - 114). Cambridge University Press.  Marcuse, H. (1964).   One - dimensional m an:   Studies   in the ideology  of advanced industrial socie ty . Psychology Press.  Marquis, C., & Tilcsik, A. (2013). Imprinting: Toward a multilevel  theory.   Academy of Management   A nnals ,   7 (1), 195 - 245.  Miranda, S., Kim, I., & Summers, J. (2015). Jamming with   so cial  med ia: How   cognitive structuring of organizing vision facets  affects   IT   innovation diff usion.   M IS   Quarterly ,   39 (3), 591 - 614.  Mohr, J. W. (1998). Measuring meaning structures.   Annual   R eview  of   S ociology ,   24,   345 - 370.  Murzintcev, N. (2016).   Tuning of the   l atent Dirichlet   allocation models  param eters:   Package   “ ldatuning . ”   I n   R   (Version   0.2.0).  https://cran.r - project.org/web/packages/ldatuning/   ldatuning.pdf  Myers,   C. G.   (2018).   Coactive vicarious learning:   Toward   a  relat ional   theory   of   vicarious   learning   in   organizations.  Academy of Management Review ,   43 (4), 610 - 634.  Nambisan, S., Lyytinen, K., Majchrzak, A., & Song, M. (2017).  Digital   innovation   management:   Reinventing   innovation  management research in a digital world.   M IS   Quarterly ,   41 (1),  223 - 238.  Orlikowski, W. J., & Gash, D. C. (1994). Technological frames:  Making sense of information technology in organizations.  ACM Transactions on Information Systems ,   12 (2), 174 - 207.  Orlikowski, W. J., & Scott, S. V. (2014). What happens when  evaluation goes on line? Exploring apparatuses of valuation in  the travel sector.   Organization Science ,   25 (3), 868 - 891.  Parker, I. (2014).   Discourse dynamics: Critical   analysis for social  and individual ps ychology . Routledge.  Pentland, B. T., & Singh, H. (2012). Materialit y: What are the  consequences. In P. M. Leonardi, B. A. Nardi, & J. Kallinikos  (Eds.),   Materiality and organizing: Social interaction in a  technological world   (pp. 287 - 295). Oxford University Press.  Phillips, N., Lawrence, T. B., & Hardy, C. (2004). Discou rse and  institutions.   Academy of Management Review ,   29 (4), 635 - 652.  Phillips, N., & Malhotra, N. (2017). Language, cognition and  institutions:   Studying   institutionalization   using   linguistic  methods.   In R. Greenwood, C. Oliver, T. B. Lawrence, & R. E.  Meye r   (Eds.)   The   Sage   handbook   of   organizational  institutionalism   ( pp. 389 - 412 ). SAGE .  Piezunka, H., & Dahlander, L. (2015). Distant search, narrow  attention:   How   crowding   alters organizations’   filtering   of  suggestions   in   crowdsourcing.   Academy   of   Management  Journal ,   58 (3), 856 - 880.  Powell, W. W., & Colyvas, J. A. (2008). Microfoundations of  institutional theory. In R. Greenwood, C. Oliver, K. Sahlin, &  R. Suddaby (Eds.),   The Sage handbook of organizational  institutionalism   (pp.   276 - 298). Sage.  Rao, H. (2017). Interorganizational   e co logy.   In   J. A. C. Baum (Ed.),  The Blackwell Companion to Organizations   (pp.   541 - 556 ) .  Wiley.  Rosemann,   M.,   &   Vessey,   I.   (2008).   Toward   improving   the  relevance of information systems research to practice:   T he role  of applicability checks.   M IS   Quarterly ,   32 (1), 1 - 22.  Rossi, M., Mueller - Bloch, C., Thatcher, J. B., & Beck, R. (2019).  Blockchain research in information systems: Current trends  and   an   inclusive   future   research   agenda.   Journal   of   the  Association for Information Systems ,   20 (9),   1390 - 1 405 .  Rousseeuw, P. J., & Van Zomeren, B. C. (1990). Unmasking  multivariate   outliers   and   leverage   points.   Journal   of   the  American Statistical Association ,   85 (411), 633 - 639.  Sahay, S., Palit, M., & Robey, D. (1994). A relativist approach to  studying the so cial construction of information technology.  European Journal of Information Systems ,   3 (4), 248 - 258.  Seidl, D. (2007). General strategy concepts and the ecology of  strategy   discourses:   A   systemic - discursive   perspective.  Organization Studies ,   28 (2), 197 - 21 8.  Sewell, W. H. (2005).   Logics of   h istory: Social   theory and social  transfor mation . University of Chicago Press.  Sgourev, S. V. (2013). How Paris gave rise to Cubism (and  Picasso): Ambiguity and fragmentation in radical innovation.  Organization Science ,   24 (6), 1601 - 1617.  Sievert, C., & Shirley, K. (2016).   Interactive   visualization of topic  models:   Package   “ LDAvis . ”   In   R   (Version   0.3.2) .  https://cran.r - project.org/web/packages/LDAvis/LDAvis.pdf  Simon, H. A. (1997).   Models of Bounded Rationality: Empirica lly  Grounded Economic Reason . MIT   P ress.\n\n--- Page 24 ---\n\nMiranda   et   al.   /   Discursive Fields and the Diversity - Coherence Paradox  1444   MIS   Quarterly   Vo l.   4 6   No.   3   /   September   202 2  Snow, D. A. (2008). Elaborating the discursive contexts of framing:  Discursive fields and spaces.   Studies in symbolic interaction ,  30 , 3 - 28.  Spillman, L. (1995). Culture, social structures, and discursive  fields.   Current   perspectives in social theory ,   15 (1), 129 - 154.  Stark, D. (2009).   The   sense of dissonance : Accounts   of worth in  economic life .   Princeton University Press.  Stuart, T. E., & Podolny, J. M. (1996). Local search and the  evolution of technological capabilities.   Strategic Management  Journal ,   17 (S1), 21 - 38.  Suddaby, R., & Foster, W. M. (2017).   History and organizational  change.   SAGE.  Swanson, E. B., & Ramiller, N.   C. (1997). The organizing vision in  information systems innovation.   Organization Science ,   8 (5),  458 - 474.  Tannen, D. (1984).   Coherence in   spoken and written disc ourse .  ABLEX .  Tapscott, D.,   &   Tapscott, A.   ( 2017 )   Realizing the potential of  blockchain: A mu ltistakeholder approach to the stewardship of  blockchain and cryptocurrencies.   World Economic Forum.  Thalmann, M., Souza, A. S., & Oberauer, K. (2019). How does  chunking help working memory?   Journal of Experimental  Psychology: Learning, Memory, and Cogniti on ,   45 (1), 37.  Thornton, P. H., & Ocasio, W. (2008). Institutional logics.   In   R.  Greenwood, C. Oliver, K. Sahlin, & R. Suddaby (Eds.) .   The  Sage handbook of organizational institutionalism   (pp. 99 - 128) .  SAGE.  Tracy, S. J. (2013).   Qualitative   research meth o ds: Collecting  Evidence,   crafting analysi s,   c ommunicating   i mpact . Wiley.  Urquhart, C., & Fernández, W. (2013). Using grounded theory  method in information systems: the researcher as blank slate  and other myths.   Journal of Information Technology ,   28 (3),  224 - 236.  Vaast, E., Davidson, E. J., & Mattson, T. (2013). Talking about  technology: The emergence of a new actor category through  new media.   M IS   Quarterly ,   37 (4), 1069 - 1092.  Wang, P. (2010). Chasing the   h ottest IT: Effects of   information  technology   fashion on organizations.   M IS   Quarterly ,   34 (1), 63 -  85.  Wang, P. (2021). Connecting the   parts with the wh ole: Toward an  information ecology theory of digital innovation ecosystem s.  M IS   Quarterly ,   45 (1), 397 - 422.  Wang, P., & Ramiller, N. C. (2009). Commun ity learning in  information technology innovation.   M IS   Quarterly ,   33 (4), 709 -  734.  Wang, P., & Swanson, E. B. (2007). Launching professional  services   automation:   Institutional   entrepreneurship   for  information   technology   innovations.   Information   and  Organization ,   17 (2), 59 - 88.  Wang,   P.,   &   Swanson,   E.   B.   (2008).   Customer   relationship  mana gement   as   advertised:   Exploiting   and   sustaining  technological momentum.   Information Technology & People ,  21 (4), 323 - 349.  Weber, K., & Dacin, M. T. (2011). The cultural construction of  organizational   life:   Introduction   to   the   special   issue.  Organization Sc ience ,   22 (2), 287.  Weedon,   C.   (1994).   Feminism   &   the   principles   of  poststructuralism.   In   J. Storey (Ed.),   Cultural theory and  popular culture: A reader   (pp. 172 - 184). Pearson Education  Limited.  Williams, R. H. (1995). Constructing the public good: Social  movements and cultural resources.   Social Problems ,   42 (1),  124 - 144.  Wuthnow, R. (2009).   Communities of   d iscourse: Ideology and  social structure in the   Reformation, the Enlightenment, and  European   s ocialism . Harvard University Press.  Zilber, T. B. (2017).   The evolving role of meaning in theorizing  institutions. In R. Greenwood, C. Oliver, T. B. Lawrence, & R.  E.   Meyer   (Eds.),   The   Sage   handbook   of   organizational  institutionalism   (pp. 651 - 678). S AGE .  About the   Author s  Shaila M. Miranda   is the W.P. Wood Professor of MIS at the Price  College of Business, the University of Oklahoma. Her research  focuses primarily on public discourse and shared meaning in the  arenas of digital activism and innovation diffusion. She employs a  combination of q ualitative and computational inductive techniques.  Shaila’s   research   has   appeared   in   journals   such   as   the   MIS  Quarterly ,   Information Systems Research ,   Journal of Management  Information Systems ,   Journal of Information Technology ,   Small  Group Research ,   Infor mation and Management , and   Data Base .  She serves   as senior editor   for   MIS Quarterly   and previously has  served as   senior editor   for   Information Systems Research .  Dawei (David) Wang   is an assistant professor of information  systems at Missouri University of S cience and Technology. David  received his Ph.D. in MIS from the Price College of Business at the  University of Oklahoma. His research focuses on information  security and digital innovation diffusion. His academic work has  appeared   in   journals   such   as   Leade rship   Quarterly   and   AIS  Transactions on Replication Research . David has also presented  his research at conferences such as the Academy of Management  Annual Meeting and the International Conference on Information  Systems.  Chuan (Annie) Tian   is an A ssistant   professor of management  information systems a t the Culverhouse College of Business School  of the University of Alabama. She holds a Ph.D . in management  information system s   from the University of Oklahoma. Dr. Tian's  research   interests   include   IT   innovation   and   diffusion,  cybersecurity,   IT   business   strategy,   and   computer - mediated  communication. Dr. Tian also has expertise in a wide range of  research methodologies, including econometrics, data/text mining,  and experiments.\n\n--- Page 25 ---\n\nMiranda et al. /   Discursive Fields and the Diversity - Coherence Paradox  MIS Quarterly Vo l . 4 6   No.   3   /   September   202 2   1445  Appendix   A  Validation   o f   Fields Representing Orders  To validate   the   fields selected to represent each order, we developed a lexicon of terms from pages 159 - 211 of Boltanski and   Thévenot   (2006)  and pages 103 - 126 of   Boltanski   and   Chiapello (2005)   and common synonyms for the terms. Following Boltanski and   Thévenot   (2006) , we  reasoned actors’ justification would be influenced by the situation, i.e., blockchain. We therefore expected the discourse to   reference the  industrial principle of blockchain, the market principle of the initial cryptocurrency applications, and the dominan t principle of a participant’s  field. Following Boltanski and   Thévenot   (1999) , we therefore sought “guiding texts” for each field and used these texts to validate our  sampling. See Table A1 for the texts and our rationale for selecting them.   If our theoret ical sample represents Boltanski   and colleagues ’  orders, the percentage of words signifying a field’s principle in its guiding text should exceed the percentage of words sign ifying that principle  in guiding texts of other fields. Table A2 confirms this, va lidating our sampling.  Table A 1 .   Guiding Texts for Validating Our Theoretical Sampling  Field   Actor   Guiding texts   Rationale for selection  Governments   United States (US) Federal  Government and  European Union (EU)  US and EU Constitutions   US and EU   constitutions specify parameters for  government enactments in the U . S .   and EU.  News media   US and European  newspapers  Webpages describing the criteria for  the Pulitzer and European Press  prizes  The Pulitzer and European Press prizes specify  the ideals for   reporting by U . S .   and European  news media respectively.  Science   Academic journals   Kuhn ’ s   Structure of Scientific  Revolutions , and National Science  Foundation (NSF) and European  Science Foundation (ESF) webpages  describing the organizations’ mission  and values  With over 100,000 Google Scholar citations,  Kuhn’s   Structure of Scientific Revolutions  deeply influences scientific wor k; NSF and ESF  webpages stipulate values and standards for  success by scientists pursuing funded research.  Idea  evangelists  TED speakers   Webpages describing the TED  organization, its history, and how it  works  TED (Technology, Entertainment, and Design)  is   a nonprofit focusing on surfacing and  disseminating exciting ideas that can advance  science, business, and society.  Corporations   Publicly traded companies   About Us webpages of the New York  (NYSE) and London (LSE) Stock  Exchanges  As sites for buying and   selling shares in  corporations, the stock exchange epitomizes   the  operation of the market principle. As the largest  stock exchanges in the Americas and Europe  respectively, the NYSE and LSE documents  guide   the   activity of stock market participants.  Proje cts   Crowdfunding seekers   About Us webpages of the Kickstarter  and GoFundMe websites  These webpages specify the ideals for projects  launched at the sites.  Social  influencers  Twitter influencers   About and Score pages of the Klout  website  One of the   earliest assessors and signalers of  social media reputation, Klout advanced the  concept of social influencers and their site -  specified qualities of influencers.  Table A 2 .   Validation of   Fields   Sampl ing †  Text   s ources   Order   principles   represented  Civic   Domestic   Industrial   Inspiration   Market   Network   Renown  Field   sampled   Governments   7.32   0.57   0.70   0.21   0.76   0.54   0.20  News media   1.58   2.95   1.59   1.70   1.45   0.51   4.84  Science   1.86   0.93   4.49   0.94   0.90   2.44   0.27  Idea   evangelists   1.02   0.63   2.29   3.36   1.07   0.83   0.48  Corporations   1.13   1.50   2.80   2.11   5.70   1.98   0.60  Projects   1.88   0.53   2.33   2.44   0.93   3.58   0.20  Social influencers   0.00   0.85   0.00   0.00   2.54   1.69   5.08  Note:   † Figures in   the   table represent average percentage of words in documents reflecting the principle of each order; row/column maximums  highlighted in grey\n\n--- Page 26 ---\n\nMiranda   et   al.   /   Discursive Fields and the Diversity - Coherence Paradox  1446   MIS   Quarterly   Vo l.   4 6   No.   3   /   September   202 2  Appendix   B  Surfacing   t he Discourse   Frames  We prepared the 4,925 documents for analysis thus. First, during tokenization, we combined fifteen word - pairs that typically represent a semantic  unit. Examples of such word - pairs include “new york”, “wall street”, and “satoshi nakamoto.” Second, we lemmat ized (a semantic reduction  approach that reduces different grammatical forms of a word — such as organizing, organize, organizes — to their basic vocabulary word). Finally,  we excluded standard stop - words — e.g., pronouns such as “I” and “me”, prepositions such   as “on” and “under”, conjunctions such as “but” and  “yet” — and other non - meaningful words, e.g., “http , ” and spelled - out numbers. We also eliminated numbers, punctuation, and non - ASCII  characters. We retained 2,446 terms.  Topic modeling via latent Dirichle t allocation (LDA) required four decisions. Our first decision was the number of topics underlying the corpus —  i.e., the LDA k parameter. Without an a priori basis for specifying the number of topics underlying the corpus of texts from   the different actors,  we used the   FindTopicsNumber   function from the   R   ldatuning   package to determine the optimal number of topics   (Murzintcev ,   2016) . We  explored alternate solutions from 2 to 100 topics and requested 2,000 iterations. (Our computing resources precluded more ite rations at this  stage.) The function produces four metrics, which suggested 38, 49, 43, and 10 topics. Visual inspection of th e output graph suggested the metrics  were simultaneously optimized at 22, 23, or 31 topics. We then used the   LDA   function from the   R   topicmodels   package   (Grün   &   Hornik 2017)  to surface topic models for each number of topics. We used the Gibbs sampling meth od, which surfaces a “ground - truth” model more efficiently  than Blei’s original variational expectation maximization algorithm   (Wang et al. ,   2009) . We requested 10,000 iterations, to optimize the stability  of the topic model solution surfaced, and set the   burn - in, i.e.,   the   number of initial iterations to be omitted, to 100.  Our second decision was with regards to the α and β hyperparameters used in surfacing the topics. These α and β hyperparamete rs specify prior  beliefs about the per - document topic and p er - topic term distributions respectively. Smaller values of α specify fewer topics per document; smaller  values of β stipulate fewer salient terms per topic. Barring an a priori basis for these specifications, we permitted the R   topicmodels   package to  esti mate the parameters, with the default starting value of 50/k for α and the value of 0.1 for β   (Grün   &   Hornik ,   2017) . Ending values of α for the  models of 49, 43, 38, 31, 23, 22, and 10 topics were 5.00, 2.27, 2.17, 1.32, 1.16, 1.08, and 1.02 respectively.  Our third decision entailed choosing among the seven candidate models. While fit metrics such as log - likelihood and perplexity are relevant to  selecting among alternate predictive topic models, they are less so for descriptive topic models   (Blei ,   2009) . I n such contexts, topic model choice  rests on a researcher’s ability to interpret candidate models. To evaluate candidate models, we used a function provided by G andrud   (2015)   to  import the LDA solution into the LDA visualization,   LDAvis   (Sievert   &   Shirley ,   2016) . The visualization applies multidimensional scaling to  position the topics in two - dimensional space, facilitating   the   interpretation of the topics and of the dimensions that distinguish among them. Each  research team member independently re viewed the seven solutions   and   wrote a memo indicating their preferred model and the rationale for their  choice. We then discussed the team’s individual preferences, and collaboratively picked the 23 - topics model.  Our fourth decision was to tag each docume nt with its “focal” topic. The LDA procedure outputs a distribution of topics over documents as each  document is deemed to contain more than a single topic. However, authors tend to have a focal thesis or topic in mind when co mposing a  document. This focal   topic may be supported by or juxtaposed against ancillary topics, engendering the LDA distribution of topics in documents.  Variations in topic distributions across documents speak to variation in authors’ elaboration of their focal thesis. To valid ate our   belief that  documents tend to have a single focal topic, we report   the   average densities of the first ten topics appearing in documents in Figure   B 1. Per -  document average topic densities ranged from 36% for the focal topic (i.e., the topic with the highes t density) to 2% for the topic with the   10th  highest density. Between the topics with the highest and second - highest densities, topic density falls steeply from 36% to less than 15%, i.e., less  than half. Thus, we conclude that focusing on the focal topic   was justified.  Density  Document Topic  Figure   B 1 . Average Topic Densities   a cross Documents\n\n--- Page 27 ---\n\nMiranda et al. /   Discursive Fields and the Diversity - Coherence Paradox  MIS Quarterly Vo l . 4 6   No.   3   /   September   202 2   1447  Appendix C  Constant Comparisons  Table   C1   reports community state metrics. Table   C2   reports the field process metrics. We first report consistency of our alternate indices as  correlations between them in Table   C3 . Next, we obtained bounds on correlations via jackknifing, i.e., re - assessing the correlation when  excluding each observation   in turn. Because bounds on the correlation between variety and separation contained zero when assessed across  fields, following Harrison and Klein   (2007) , we treated these as related, but distinct, diversity concepts. Rather than treat them as  interchangea ble, we therefore reported correlations of other concepts with each metric.   Correlations of process concepts and coherence with  variety and separation and bounds on them are depicted in Table   C4 . From the table, we see that alternate indices for each conce pt are  consistent in direction and bounds on all correlations   that   do not contain zero.  In our discussion of imprinting, we noted the possibility for fields to imprint on a set of frames rather than a single frame . To rule out this  possibility, we compared   the number of field texts that could be   expected   to focus on the initial frame by chance and the   observed   number of  texts focusing on that frame (see Table   C5 ). The expected number of texts is the total field texts in the first two years divided by the to tal  number of field frames in the first two years reported in Table   C5 . The observed number of field texts focusing on the initial frame consistently  exceeded the expected number of texts focusing on that frame. The exception is the idea evangelist field,   where observed and expected texts  per frame are identical. This field used only one frame across its first two years. Therefore, we conclude   that   fields imprint on a single frame  rather than   on   a set of frames.  Table   C1 .   Community State Indices  2012   2013   2014   2015   2016   2017  Framing diversity (variety)   0.39   0.55   0.83   0.97   0.96   0.95  Framing diversity (separation)   0.37   0.43   0.90   0.93   0.93   0.93  Density of frame network   0.00   0.00   0.14   0.21   0.27   0.32  Density of field network   0.06   0.05   0.05   0.14   0.28   0.31  Table   C 2 . Field Process Indices  Type   Discursive field   Imprinting   Imitating   Retracting   Foreshadowing  In 2017   All years   Rate   Speed   All years   In 2017   2015 onward   All years  Mediated   News media   10%   16%   0.53   0.82   0.04   0.01   0.32   0.24  Social   influencers   5%   4%   0.86   1.31   0.03   0.01   0.37   0.52  Hybrid   Science   10%   27%   0.43   0.38   0.15   0.09   0.14   0.14  Idea evangelists   85%   88%   0.05   0.17   0.38   0.27   0.17   0.17  Enactment   Corporations   35%   39%   0.50   0.43   0.10   0.08   0.23   0.23  Projects   63%   63%   0.52   0.37   0.29   0.07   0.18   0.16  Governments   29%   28%   0.30   0.29   0.43   0.20   0.19   0.19  Table   C3 . Reliability of Indices  Concept   Correlation   Jackknifed   b ounds  Diversity   0.98 (over time)  0.56 (across fields)  0.96 to 0.99  - 0.02 to 0.75 †  Imprinting   0.98 (across   fields)   0.95 to 1.00  Imitating   0.87 (across fields)   0.71 to 0.92  Retracting   0.88 (across fields)   0.85 to 0.95  Foreshadowing   0.88 (across fields)   0.87 to 0.99  Coherence   0.90 (over time)   0.84 to 0.96  Note:   † Jackknifed bounds contain zero,   indicating variety and separation are not interchangeable when assessing diversity across fields\n\n--- Page 28 ---\n\nMiranda   et   al.   /   Discursive Fields and the Diversity - Coherence Paradox  1448   MIS   Quarterly   Vo l.   4 6   No.   3   /   September   202 2  Table   C4 .   Bounds on Reported Correlations between Diversity and Other Concepts  Process   Index   Correlation with ( jackknifed bounds )  Variety   Separation  Imprinting   Proportion of texts retaining initial frame in 2017   - 0.97 ( - 0.93 to   - 0.98)   - 0.51 ( - 0.04 to   - 0.73)  Proportion of texts retaining initial frame in all years   - 0.96 ( - 0.93 to   - 0.96)   - 0.57 ( - 0.13 to   - 0.74)  Imitating   Imitation rate   0.70 (0.23 to 0.81)   0.91 (0.83 to 0.93)  Imitation speed   0.62 (0.57 to 0.72)   0.75 (0.70 to 0.85)  Retracting   Average year - over - year loss in frame use   for all years   - 0.59 ( - 0.38 to   - 0.92)   - 0.82 ( - 0.76 to   - 0.94)  Average loss in frame use in 2017   - 0.73 ( - 0.22 to   - 0.91)   - 0.96 ( - 0.94 to   - 0.99)  Foreshadowing   Similarity between field’s frame portfolio in one year and  community’s portfolio in following years from 2015 onward  0.52 (0.41 to 0.73)   0.70 (0.60 to 0.83)  Similarity between field’s   frame portfolio in one year and  community’s portfolio in following years for all years  0.41 (0.31 to 0.53)   0.56 (0.44 to 0.68)  Coherence   Edge - weighted density of frame network   0.93 (0.91 to 0.95)   0.91 (0.87 to 0.96)  Edge - weighted density of field   network   0.71 (0.64 to 0.82)   0.64 (0.54 to 0.85)  Table   C5 .   Validating the First Text Frame as Basis for   Imprinting  First two years  of   discourse  Discursive field  News media   Social  influencers   Science   Idea  evangelists   Corporations   Projects   Governments  Total texts   19   5   37   2   51   4   8  Total frames   2   2   9   1   9   2   5  Incidence   of texts per initial frame in fields’ first two participation years  Expected   9.5   2.5   4.1   2.0   5.7   2.0   1.6  Observed   13   3   15   2   28   3   2\n\n--- Page 29 ---\n\nMiranda et al. /   Discursive Fields and the Diversity - Coherence Paradox  MIS Quarterly Vo l . 4 6   No.   3   /   September   202 2   1449  Appendix D  Labeling Topics ( F rames)  We labeled topics (frames) using term probabilities, i.e., βs   (Blei et al. ,   2003) , and a relevance metric output by   LDAvis   (Sievert   &   Shirley  2016) . The relevance metric determines term salience to a topic by weighting the term pro bability, β, by a lift parameter, λ, defined as “the  ratio of a term’s probability within a topic to its marginal probability across the corpus”   (Sievert   &   Shirley ,   2016, p. 65) . The lift parameter  may vary between λ=1, when terms are evaluated solely base d on their topic - specific probability, and λ=0, when terms are evaluated solely  based on their lift. We iterated through topic labeling by varying λ between 0 and 1. For example, the top   five   terms at λ=1 for   T opic 8,  labeled   H EALTHCARE   A PPLICATIONS , were   data, blockchain, use, health, and information; the top   five   terms at λ=0 were patient, clinical,  traceable, iso, and metadata. Table   D1   depicts labels assigned to the 23 topics, along with the twenty terms with the highest βs (i.e., λ=1).  Table   D1   also re ports the number of documents in the corpus in which each topic dominated.  To validate our topic (frame) labels, we examined documents in which surfaced topics were densest. Table   D1   illustrates four blockchain  frames — one from each quadrant of Figure 3. Illustrations of all frames are available   in the Online Supplement   at   https://osf.io/75yq3/ .  Table   D1 .   Sample Illustrations of   Blockchain   Frame s  Frame  (focus of texts)  Theme   Terms with highest density (β)  in frame   Illustrations  3. Sociopolitical  s olutions (255)  I   ▪   just  ▪   think  ▪   thing  ▪   people  ▪   know  ▪   now  ▪   good  ▪   want  ▪   need  ▪   year  ▪   time  ▪   take  ▪   look  ▪   well  ▪   right  ▪   work  ▪   much  ▪   even  ▪   govern  ▪   change  I   want   alternatives…   peer - to - peer   communication,  which is   just   between you and me, this   worked   for a  while… Over the last couple   years , there has been the  advent of a new thing called a smart contract… I   think  with blockchain and smart contract configurations, we  can   start   to   see   a   humanitarian   Passport.   People  across the world can have a democratized identity… I  think   probably the most   exciting version is … the NGO  … produced a universal identity for Syrian refugees —  people   that   need   it most… the more   people   that   know  about   it   the   more   people   can   act   on   it.   ( I DEA  E VANGELISTS : Leni Ma’ia’i: 6/2017)  8. Healthcare  a pplications  (95)  II   ▪   data  ▪   blockchain  ▪   use  ▪   health  ▪   information  ▪   record  ▪   system  ▪   access  ▪   research  ▪   patient  ▪   secure  ▪   network  ▪   standard  ▪   technology  ▪   medicine  ▪   process  ▪   node  ▪   privacy  ▪   study  ▪   store  From   a   global   perspective,   the   application   of  Blockchain   technologies   in   the   context   of   clinical  research   is broad and promising. Indeed, tracking the  complex   data   flow with numerous diverse stakeholders,  and documenting it in real - time through a timestamping  workflow,   is   a   key   step   towards   proving   data  consist ency and inviolability, and will hence improve  clinical trial methodology… Rapha ë   l Porcher … has put  in perspective the potential of implementing   Blockchain  in   consent   process ,   with   regards   to   information   of  patients ,   ethics,   data   privacy …   Might   patients ’  concerns   over   the   security   of   their   data   and   the  importance of confidentiality make them cautious about  joining a trial if they had to   use   this   system ?... Might  people need to print or export a copy of the electronic  record   for   long - term   storage?   ( S CIENCE :  F1000Research, 1/2017)  13.  Infrastructure for  f inance (944)  III   ▪   fintech  ▪   blockchain  ▪   insurtech  ▪   iot  ▪   cryptocurrency  ▪   technology  ▪   bank  ▪   bitcoin  ▪   spirosmargaris  ▪   coindesk  ▪   banknxt  ▪   dlt  ▪   bourseettrading  ▪   disrupt  ▪   pay  ▪   machinelearn  #InternetOfThings   #IoT   #IoE   #IIoT   #AI   #BigData  #BlockChain   …   RT   @greg_deyli:   How   does  #blockchain   work.   #cryptocurrency   #bitcoin   #IoT   #tech  #startup   #vr #deathstar5 #bigdata   #fintech   #crypto #vt  #Sm… RT @joshmepstein: Great overview of diverse  #Fintech   landscape   -   #AI   #MachineLearning   #BigData  #Fintech   # Insurtech   #blockchain ” ( S OCIAL   I NFLUENCERS :  HerbertLSamuels, 10/2017)\n\n--- Page 30 ---\n\nMiranda   et   al.   /   Discursive Fields and the Diversity - Coherence Paradox  1450   MIS   Quarterly   Vo l.   4 6   No.   3   /   September   202 2  ▪   bigdata  ▪   startup  ▪   innovate  ▪   cybersecure   … #Blockchain   #fintech   #CyberSecurity   #Cloud  #Bigdata   #Industry40   #Bitcoin   #IoT   #DataSc… Wheel  of   #Disruption …   #BigData   #AI   #IoT  #PredictiveAnalytics   #blockchain   #fintech   #drones  #startups   #VC…   #CyberSecurity   #bitcoin   #fintech  #Insurtech ...   #Bitcoin   is a #bubble, but the   technology  behind it could tr ansform the world…   #fintech   #bitcoin  #CryptoCurrencies   #Insurtech ...” ( S OCIAL   I NFLUENCERS :  Andi_Staub, 12/2017)  19. Country  s overeignty (40)  IV   ▪   case  ▪   money  ▪   president  ▪   alphaville  ▪   scale  ▪   real  ▪   explain  ▪   state  ▪   uber  ▪   russian  ▪   object  ▪   man  ▪   free  ▪   electronic  ▪   coordinate  ▪   land  ▪   liberland  ▪   position  ▪   citizen  ▪   claim  …most recent   states   —   South Sudan, East Timor,  Eritrea   —   were carved from existing sovereignties…  Jedlicka had to check Facebook in   case   he was missing  some supporters... ‘I work in financial services…, and   if  it were discovered that I was… French ambassador  from   Liberland , everyone would… think I was washing  money …’ ‘It’s just not too   presidential   to travel by  subway,’ he said. ‘I’m going to call an   Uber .’ The  ambassador… advised the   President … about Bitcoin  and Blockchain. ( N EWS MEDIA :   The New York Times  8/2015)  What would be the impact of Catalonia seceding from  Spain on Barcelona's … tech startup ecosystem? …  Machines are not good at rare events. The current  solutions did not ‘ scale .’ They need   more humans… I'm  speaking   at   the   Blockchain   &   Cryptocurrency  Conference in Kiev this week. ( S OCIAL   I NFLUENCERS :  MikeButcher 10/2017)\n\n--- Page 31 ---\n\nMiranda et al. /   Discursive Fields and the Diversity - Coherence Paradox  MIS Quarterly Vo l . 4 6   No.   3   /   September   202 2   1451  Appendix E  Diversity / Coherence as Distinct Concepts  In Figure 8 (body of the paper), we showed   that diversity and coherence related very strongly over time. Given the size of the correlations,  one could ask if this is just a methodological artifact — if our measures for these constructs covary by definition. We include this Appendix  to show that this   is not the case. To demonstrate this,   Figure   E 2 depicts two frame networks possible after 2015, when all frames had been  introduced and all fields had entered the discourse.  Because all frames are represented in the networks, both networks reflect high fr aming diversity. The network in Figure   E 2 a   reflects  exceedingly low coherence (i.e., edge - weighted density   =   0.02) because only one of the seven fields uses each frame, splitting the network  into seven different groups of frames. In contrast, the network i n Figure   E 2 b   depicts optimal coherence (i.e., edge - weighted density   =   1.00)  when every field uses every frame. Thus, as high density need not be accompanied by high coherence, the two concepts are dist inct.  A: High   diversity,   low   coherence   B: High   diversity,   high   coherence  Figure   E 2 . Combinations of Diversity and Coherence\n\n--- Page 32 ---\n\nCopyright   of   MIS   Quarterly   is   the   property   of   MIS   Quarterly   and   its   content   may   not   be copied   or   emailed   to   multiple   sites   or   posted   to   a   listserv   without   the   copyright   holder's express   written   permission.   However,   users   may   print,   download,   or   email   articles   for individual   use.",
    "analysis": {
      "governance_power_accountability": "The system appears to leverage institutional power through the influence of 'actors' who shape the innovation discourse and hence the community's understanding of blockchain technology. These actors are not explicitly identified, and accountability mechanisms are unclear, suggesting potential issues in power distribution and accountability.",
      "plurality_inclusion_embodiment": "The paper acknowledges diverse 'frames' within the blockchain discourse, suggesting an emphasis on plurality. However, it doesn't explicitly mention the inclusion of non-western, Indigenous, or disabled perspectives, suggesting that these may be overlooked or marginalized.",
      "agency_codesign_self_determination": "The document suggests some degree of agency and co-design, with 'actors' contributing to the discourse around blockchain. However, the extent to which this system allows for community agency and self-determination is unclear, especially considering the potential domination of certain perspectives or interests.",
      "reflexivity_situated_praxis": "The text doesn't show explicit reflexivity or consideration of its own historical, cultural, or social positionality. It also does not overtly recognize structural inequities or biases shaping the innovation discourse and its outcomes.",
      "legitimacy_claims": {
        "source": "Technocratic",
        "mechanisms": "The legitimacy of blockchain is built through discourses shaped by certain 'actors', who legitimize the innovation and mobilize collective action for its diffusion. This reflects a technocratic source of legitimacy, valuing technical expertise and knowledge.",
        "tensions": "Potential tensions arise between the need for diverse viewpoints and the need for coherence in the discourse, which may limit the inclusion of alternative or challenging perspectives."
      },
      "key_insight": "The discourse around blockchain is influenced by certain 'actors', potentially limiting diverse perspectives and accountability.",
      "governance_scores": {
        "centralization": 70,
        "rights_focus": 50,
        "flexibility": 60,
        "market_power": 50,
        "procedurality": 70
      },
      "structural_pillars": {
        "risk": {
          "title": "Undefined Risk",
          "description": "The document doesn't explicitly discuss risk management or approach, suggesting a lack of focus on identifying and mitigating potential risks.",
          "badge": "Undeveloped",
          "quote": "Not available"
        },
        "enforcement": {
          "title": "Invisible Enforcement",
          "description": "Enforcement mechanisms or standards aren't discussed, indicating a potential lack of stringent enforcement or oversight.",
          "badge": "Lacking",
          "quote": "Not available"
        },
        "rights": {
          "title": "Unspecified Rights",
          "description": "The text does not explicitly address the rights of individuals or communities impacted by blockchain technology.",
          "badge": "Silent",
          "quote": "Not available"
        },
        "scope": {
          "title": "Broad Scope",
          "description": "The discourse encompasses diverse 'frames', suggesting a broad scope of considerations. However, details on the inclusivity or exclusivity of these frames are unclear.",
          "badge": "Expansive",
          "quote": "Innovation breakthroughs prompt sensemaking discourses that promote community learning and socially construct the innovation."
        }
      }
    },
    "cultural_framing": {
      "state_market_society": "This document frames the state-market-society relationship from a discourse-based perspective, with no explicit role for the state. It assumes that a community of interested actors, presumably from both market and civil society, collectively interprets and socialises innovations. The market shapes the innovation community and discourse, while also presumably responding to it. Civil society is implied to play a role as the collective of 'consumers with disparate preferences'. A high level of agency is assumed for these collective actors, with no evident role for state regulation or intervention.",
      "technology_role": "Technology, specifically blockchain in this case, is positioned as both a tool and as an infrastructure that prompts and shapes discourses. It is an opportunity for innovation and community building. It is also framed as a socially constructed entity, shaped by the sensemaking, sensegiving and discursive practices of the 'innovation community'. It is not seen as a threat but as a site of potential discursive diversity and coherence.",
      "rights_conception": "The document does not explicitly address individual or collective rights or procedural or substantive rights. However, the language around 'diversity', 'community understanding', and 'collective action' imply a focus on collective rights to understand, shape, and benefit from technological innovations. The conception of rights seem to be predominantly discursive and procedural.",
      "historical_context": "There is no explicit reference to historical or colonial contexts. However, the focus on 'innovation discourse' and 'community learning' could suggest a post-colonial focus on decolonising knowledge and empowering local communities. The absence of explicit historical or colonial context may reflect a universalising assumption prevalent in much of technology and innovation discourse.",
      "epistemic_authority": "The discursive fields, or contexts, in which blockchain technology is discussed and translated into practice are seen as the sites of epistemic authority. The knowledge of actors who have direct experience with the technology and those who are 'innovation ambassadors and gatekeepers' is privileged. The document emphasises a co-constructed form of knowledge legitimacy, where diverse actors contribute to a coherent understanding of the technology.",
      "cultural_distinctiveness_score": 0.3,
      "dominant_cultural_logic": "Co-constructive technocratic discourse"
    },
    "institutional_logics": {
      "logics": {
        "market": {
          "strength": 0.6,
          "champions": [
            "Innovation breakthroughs",
            "diffusion of innovation"
          ],
          "material": "The ecosystem in which the innovation is being developed and propagated; the diffusion process",
          "discursive": "The language emphasizes market concepts such as innovation, diffusion, and customer preferences.",
          "key_tensions": [
            "Tensions between ensuring coherence while catering to diverse customer preferences"
          ]
        },
        "state": {
          "strength": 0,
          "champions": [],
          "material": "",
          "discursive": "",
          "key_tensions": []
        },
        "professional": {
          "strength": 0.7,
          "champions": [
            "Research authors (Shaila M. Miranda, Dawei Wang, Chuan Tian)",
            "Interested actors",
            "innovation ambassadors and gatekeepers"
          ],
          "material": "The development and application of blockchain technology; the research process leading to the paper",
          "discursive": "The document relies on professional terminology and concepts such as discursive fields, sensemaking, innovation frames, etc.",
          "key_tensions": [
            "Potential mismatch between professional interpretation and consumer understanding of blockchain technology"
          ]
        },
        "community": {
          "strength": 0.8,
          "champions": [
            "Blockchain Community",
            "actors from enactment fields and mediated fields",
            "innovation community"
          ],
          "material": "The cooperative development, interpretation, and understanding of the blockchain technology within the community",
          "discursive": "The document emphasizes concepts of community learning, collective action, socially constructed innovation, and diverse views.",
          "key_tensions": [
            "Potential dissonance between different discursive fields within the community"
          ]
        }
      },
      "dominant_logic": "community",
      "logic_conflicts": [
        {
          "between": "market and community",
          "site_of_conflict": "In the tension between ensuring coherence for effective market diffusion while catering to diverse views within the community",
          "resolution_strategy": "The document suggests these two aspects can co-evolve, with diversity leading to a coherent understanding of innovation"
        },
        {
          "between": "professional and community",
          "site_of_conflict": "In the potential mismatch between professional interpretation and consumer understanding of blockchain technology",
          "resolution_strategy": "The document highlights the importance of communication and discursive fields in bridging this gap"
        }
      ],
      "overall_assessment": "The dominant logic of this text is community, highlighting the importance of collective understanding and action in the innovation process. There are tensions between market and community logics, and professional and community logics, which the discourse aims to reconcile through strategies of co-evolution and communication."
    }
  },
  {
    "id": "1764007155700",
    "title": "brazil",
    "description": "Uploaded 11/24/2025",
    "type": "PDF",
    "addedDate": "11/24/2025",
    "status": "Active Case",
    "colorClass": "bg-purple-100",
    "iconClass": "text-purple-600",
    "extractedText": "--- Page 1 ---\n\nBrazil: Bill on the use of Artificial Intelligence [2338/2023]  As published on 3 May 2023. Official text in original language plus DPA expert translation.  Explore   Plain Text Preamble FEDERAL SENATE BILL No. 2338, of 2023 It provides for the use of Artificial Intelligence. Author: Senator Rodrigo Pacheco (PSD/MG) CHAPTER I PRELIMINARY PROVISIONS Article 1 This Law establishes general national standards for the\n\n--- Page 2 ---\n\ndevelopment, implementation, and responsible use of artificial  intelligence (AI) systems in Brazil, with the objective of protecting fundamental rights and guaranteeing the implementation of safe and reliable systems, for the benefit of humankind, the democratic regime, and scientific and technological development. Article 2 The development, implementation, and use of artificial intelligence systems in Brazil are based on the following principles: I – the centrality of the human person; II – respect for human rights and democratic values; III – the free development of personality; IV – environmental protection and sustainable development; V – equality, non-discrimination, plurality and respect for labor rights; VI – technological development and innovation; VII – free enterprise, free competition and consumer protection; VIII – privacy, data protection and informational self- determination; IX – the promotion of research and development with the\n\n--- Page 3 ---\n\naim of stimulating innovation in the productive sectors and  in the public sector; and X – access to information and education, and awareness of artificial intelligence systems and their applications. Article 3 The development, implementation, and use of artificial intelligence systems will observe good faith and the following principles: I – inclusive growth, sustainable development and well- being; II – self-determination and freedom of decision and choice; III – human participation in the artificial intelligence lifecycle and effective human supervision; IV – non-discrimination; V – justice, equity and inclusion; VI – transparency, explainability, intelligibility and auditability; VII – reliability and robustness of artificial intelligence systems and information security; VIII – due process of law, contestability and adversarial process; IX – traceability of decisions throughout the lifecycle of\n\n--- Page 4 ---\n\nartificial intelligence systems as a means of accountability  and attribution of responsibility to a natural or legal person; X – accountability, liability, and full reparation of damages; XI – prevention, precaution and mitigation of systemic risks arising from intentional or unintentional uses and unforeseen effects of artificial intelligence systems; and XII – non-maleficence and proportionality between the methods employed and the determined and legitimate purposes of artificial intelligence systems. Article 4 For the purposes of this Law, the following definitions are adopted: I – Artificial intelligence system: a computational system, with varying degrees of autonomy, designed to infer how to achieve a given set of objectives, using approaches based on machine learning and/or logic and knowledge representation, through input data from machines or humans, with the goal of producing predictions, recommendations, or decisions that can influence the virtual or real environment; II – Artificial intelligence system provider: a natural or legal person, of a public or private nature, that develops an artificial intelligence system, directly or by commission, with a view to placing it on the market or applying it to a service provided by it, under its own name or brand, for a fee or free of charge;\n\n--- Page 5 ---\n\nIII – Artificial intelligence system operator: a natural or legal  person, of a public or private nature, who employs or uses, in their own name or for their benefit, an artificial intelligence system, unless said system is used within the scope of a personal activity of a non-professional nature; IV – Artificial intelligence agents: suppliers and operators of artificial intelligence systems; V – competent authority: the body or entity of the Federal Public Administration responsible for ensuring, implementing and monitoring compliance with this Law throughout the national territory; VI – discrimination: any distinction, exclusion, restriction or preference, in any area of   public or private life, whose purpose or effect is to nullify or restrict the recognition, enjoyment or exercise, under conditions of equality, of one or more rights or freedoms provided for in the legal system, on account of personal characteristics such as geographic origin, race, color or ethnicity, gender, sexual orientation, socioeconomic class, age, disability, religion or political opinions; VII – Indirect discrimination: discrimination that occurs when a seemingly neutral rule, practice, or criterion has the capacity to disadvantage or place people belonging to a specific group at a disadvantage, unless that rule, practice, or criterion has some reasonable and legitimate objective or justification in light of the right to equality and other fundamental rights; VIII – Text and data mining: the process of extracting and analyzing large amounts of data or partial or complete\n\n--- Page 6 ---\n\nexcerpts of textual content, from which patterns and  correlations are extracted that will generate relevant information for the development or use of artificial intelligence systems. CHAPTER II OF RIGHTS Section I General Provisions Article 5 Individuals affected by artificial intelligence systems have the following rights, to be exercised in the manner and under the conditions described in this Chapter: I – the right to prior information regarding their interactions with artificial intelligence systems; II – the right to an explanation regarding the decision, recommendation, or prediction made by artificial intelligence systems; III – the right to challenge decisions or predictions of artificial intelligence systems that produce legal effects or that significantly impact the interests of the affected party; IV – the right to human determination and participation in decisions regarding artificial intelligence systems, taking into account the context and the state of the art of technological development; V – the right to non-discrimination and to the correction of direct, indirect, illegal or abusive discriminatory biases;\n\n--- Page 7 ---\n\nand  VI – the right to privacy and the protection of personal data, in accordance with the relevant legislation. Sole paragraph. Artificial intelligence agents will provide clear and easily accessible information on the procedures necessary to exercise the rights described in the main clause. Article 6 The defense of the interests and rights provided for in this Law may be exercised before the competent administrative bodies, as well as in court, individually or collectively, in accordance with the provisions of the relevant legislation concerning instruments of individual, collective and diffuse protection. Section II On the rights associated with information and understanding of decisions made by artificial intelligence systems Article 7 Individuals affected by artificial intelligence systems have the right to receive, prior to contracting or using the system, clear and adequate information regarding the following aspects: I – automated nature of interaction and decision-making in processes or products that affect the person; II – general description of the system, types of decisions, recommendations or predictions it is intended to make, and the consequences of its use for the individual;\n\n--- Page 8 ---\n\nIII – Identification of the artificial intelligence system operators and governance measures adopted in the development and use of the system by the organization; IV – the role of the artificial intelligence system and the humans involved in the decision-making, forecasting, or recommendation process; V – categories of personal data used in the context of the operation of the artificial intelligence system; VI – security, non-discrimination and reliability measures adopted, including accuracy, precision and coverage; and VII – other information defined in regulations. § 1 Without prejudice to the provision of complete information in physical or digital media open to the public, the information referred to in item I of the heading of this article shall also be provided, where applicable, using easily recognizable icons or symbols. § 2. Individuals exposed to emotion recognition systems or biometric categorization systems will be informed about the use and operation of the system in the environment where the exposure occurs. § 3. Artificial intelligence systems intended for vulnerable groups, such as children, adolescents, the elderly, and people with disabilities, shall be developed in such a way that these people can understand their operation and their rights in relation to artificial intelligence agents.\n\n--- Page 9 ---\n\nArticle 8 The individual affected by an artificial intelligence system may request an explanation of the decision, prediction, or recommendation, including information about the criteria and procedures used, as well as the main factors affecting that specific prediction or decision, including information about: I – the rationality and logic of the system, the meaning and the expected consequences of such a decision for the person affected; II – the degree and level of contribution of the artificial intelligence system to decision-making; III – the data processed and its source, the criteria for decision-making and, where appropriate, its weighting, applied to the situation of the person affected; IV – the mechanisms by which a person can challenge the decision; and V – the possibility of requesting human intervention, under the terms of this Law. Sole paragraph. The information mentioned in the heading will be provided through a free and facilitated procedure, in language that allows the person to understand the result of the decision or forecast in question, within a period of up to fifteen days from the date of the request, with the possibility of extension, once, for an equal period, depending on the complexity of the case.\n\n--- Page 10 ---\n\nSection III On the right to contest decisions and to request human intervention Article 9 Individuals affected by artificial intelligence systems will have the right to challenge and request a review of decisions, recommendations, or predictions generated by such systems that produce relevant legal effects or significantly impact their interests. § 1. The right to correct incomplete, inaccurate, or outdated data used by artificial intelligence systems is guaranteed, as well as the right to request the anonymization, blocking, or deletion of unnecessary, excessive, or unlawfully processed data, pursuant to Article 18 of Law No. 13,709, of August 14, 2018, and relevant legislation. § 2. The right to contest, as provided for in the heading of this article, also covers decisions, recommendations, or predictions based on discriminatory or unreasonable inferences, or those that violate objective good faith, including inferences that: I - are based on data that is inadequate or abusive for the purposes of the processing; II – are based on imprecise or statistically unreliable methods; or III - They do not adequately consider the individuality and personal characteristics of individuals. Art. 10.\n\n--- Page 11 ---\n\nWhen a decision, prediction, or recommendation from an artificial intelligence system produces relevant legal effects or significantly impacts a person's interests, including through profiling and inferences, that person may request human intervention or review. Sole paragraph. Human intervention or review will not be required if its implementation is demonstrably impossible, in which case the person responsible for operating the artificial intelligence system will implement effective alternative measures to ensure a re-analysis of the contested decision, taking into account the arguments raised by the affected person, as well as the redress of any damages caused. Art. 11. In scenarios where decisions, predictions, or recommendations generated by artificial intelligence systems have an irreversible or difficult-to-reverse impact, or involve decisions that could pose risks to the life or physical integrity of individuals, there will be significant human involvement in the decision-making process and final human determination. Section IV On the right to non-discrimination and the correction of direct, indirect, illegal or abusive discriminatory biases Art. 12. People affected by decisions, predictions, or recommendations from artificial intelligence systems have the right to fair and equal treatment, and the implementation and use of artificial intelligence systems that may lead to direct, indirect, illegal, or abusive discrimination are prohibited, including:\n\n--- Page 12 ---\n\nI – due to the use of sensitive personal data or disproportionate impacts based on personal characteristics such as geographic origin, race, color or ethnicity, gender, sexual orientation, socioeconomic class, age, disability, religion or political opinions; or II – based on the establishment of disadvantages or the worsening of the vulnerability of people belonging to a specific group, even if apparently neutral criteria are used. Sole paragraph. The prohibition set forth in the main clause does not prevent the adoption of criteria for differentiation between individuals or groups when such differentiation is based on demonstrated, reasonable, and legitimate objectives or justifications in light of the right to equality and other fundamental rights. CHAPTER III RISK CATEGORIZATION Section I Preliminary Assessment Art. 13. Prior to being placed on the market or used in service, every artificial intelligence system will undergo a preliminary assessment by the supplier to classify its risk level, the registration of which will consider the criteria set forth in this chapter. § 1. Suppliers of general-purpose artificial intelligence systems shall include in their preliminary assessment the purposes or applications indicated, pursuant to Article 17 of this law.\n\n--- Page 13 ---\n\n§ 2. The preliminary assessment carried out by the supplier will be recorded and documented for accountability purposes in the event that the artificial intelligence system is not classified as high risk. § 3 The competent authority may determine the reclassification of the artificial intelligence system, upon prior notification, as well as determine the performance of an algorithmic impact assessment for the instruction of the ongoing investigation. § 4. If the result of the reclassification identifies the artificial intelligence system as high-risk, the performance of an algorithmic impact assessment and the adoption of the other governance measures provided for in Chapter IV will be mandatory, without prejudice to any penalties in case of a fraudulent, incomplete or untruthful preliminary assessment. Section II Excessive Risk Art. 14. The implementation and use of artificial intelligence systems are prohibited: I – that employ subliminal techniques that aim to or have the effect of inducing a natural person to behave in a way that is harmful or dangerous to their health or safety or contrary to the principles of this Law; II – that exploit any vulnerabilities of specific groups of natural persons, such as those associated with their age\n\n--- Page 14 ---\n\nor physical or mental disability, in order to induce them to  behave in a way that is harmful to their health or safety or contrary to the principles of this Law; III – by public authorities, to evaluate, classify or rank individuals based on their social behavior or personality attributes, through a universal scoring system, for access to goods, services and public policies, in an illegitimate or disproportionate manner. Art. 15. In the context of public safety activities, the use of remote biometric identification systems is only permitted, on a continuous basis, in spaces accessible to the public, when provided for in specific federal law and with judicial authorization in connection with individualized criminal prosecution activity, in the following cases: I – prosecution of crimes punishable by a maximum prison sentence of more than two years; II – searching for victims of crimes or missing persons; or III – crime caught in the act. Sole paragraph. The law referred to in the main clause shall provide for measures that are proportionate and strictly necessary to serve the public interest, observing due process of law and judicial review, as well as the principles and rights provided for in this Law, especially the guarantee against discrimination and the need for review of the algorithmic inference by the responsible public agent before taking any action against the identified person.\n\n--- Page 15 ---\n\nArt. 16. It will be up to the competent authority to regulate artificial intelligence systems that pose excessive risks. Section III High Risk Art. 17. Artificial intelligence systems used for the following purposes are considered high-risk: I – application as safety devices in the management and operation of critical infrastructures, such as traffic control and water and electricity supply networks; II – education and vocational training, including systems for determining access to educational or vocational training institutions or for evaluating and monitoring students; III – recruitment, screening, filtering, candidate evaluation, decision-making regarding promotions or terminations of employment contracts, task allocation, and monitoring and evaluation of the performance and behavior of individuals affected by such artificial intelligence applications in the areas of employment, worker management, and access to self-employment; IV – evaluation of criteria for access, eligibility, granting, review, reduction or revocation of private and public services that are considered essential, including systems used to assess the eligibility of individuals for public\n\n--- Page 16 ---\n\nassistance and social security services;  V – assessment of the debt capacity of individuals or establishment of their credit rating; VI – dispatching or establishing priorities for emergency response services, including firefighters and medical assistance; VII – administration of justice, including systems that assist judicial authorities in investigating facts and applying the law; VIII – autonomous vehicles, when their use may generate risks to the physical integrity of people; IX – applications in the health field, including those intended to assist in medical diagnoses and procedures; X – biometric identification systems; XI – criminal investigation and public security, in particular for individual risk assessments by the competent authorities, in order to determine the risk of a person committing offenses or of recidivism, or the risk to potential victims of criminal offenses, or to assess the personality traits and characteristics or past criminal behavior of individuals or groups; XII – analytical study of crimes related to natural persons, allowing police authorities to search large sets of complex data, related or unrelated, available from different data sources or in different data formats, in order to identify\n\n--- Page 17 ---\n\nunknown patterns or discover hidden relationships in the  data; XIII – investigation by administrative authorities to assess the credibility of evidence during the investigation or repression of offenses, to predict the occurrence or recurrence of an actual or potential offense based on the profiling of individuals; or XIV – Migration management and border control. Art. 18. The competent authority will be responsible for updating the list of excessively risky or high-risk artificial intelligence systems, identifying new scenarios based on at least one of the following criteria: I - the implementation must be on a large scale, taking into account the number of people affected and the geographical extent, as well as its duration and frequency; II – the system could negatively impact the exercise of rights and freedoms or the use of a service; III – the system has a high potential for material or moral harm, as well as discrimination; IV - the system affects people from a specific vulnerable group; V – the potential harmful results of the artificial intelligence system are irreversible or difficult to reverse;\n\n--- Page 18 ---\n\nVI - a similar artificial intelligence system has previously caused material or moral damages; VII – low degree of transparency, explainability, and auditability of the artificial intelligence system, which hinders its control or supervision; VIII – high level of identifiability of data subjects, including the processing of genetic and biometric data for the purpose of uniquely identifying a natural person, especially when the processing involves combining, matching or comparing data from multiple sources; IX – when the data subject has reasonable expectations regarding the use of their personal data in the artificial intelligence system, especially the expectation of confidentiality, such as in the processing of confidential or sensitive data. Sole paragraph. The updating of the list mentioned in the heading by the competent authority will be preceded by consultation with the competent sectoral regulatory body, if any, as well as by public consultation and hearings and a regulatory impact analysis. CHAPTER IV ON THE GOVERNANCE OF ARTIFICIAL INTELLIGENCE SYSTEMS Section I General Provisions Art. 19. Artificial intelligence agents will establish governance structures and internal processes capable of ensuring the\n\n--- Page 19 ---\n\nsecurity of the systems and the protection of the rights of  affected individuals, as provided for in Chapter II of this Law and relevant legislation, which will include, at least: I – Transparency measures regarding the use of artificial intelligence systems in interaction with natural persons, which includes the use of adequate and sufficiently clear and informative human-machine interfaces; II – transparency regarding the governance measures adopted in the development and use of the artificial intelligence system by the organization; III – appropriate data management measures for mitigating and preventing potential discriminatory biases; IV – legitimizing data processing in accordance with data protection legislation, including through the adoption of privacy measures by design and by default, and the adoption of techniques that minimize the use of personal data; V – adoption of appropriate parameters for separating and organizing data for training, testing, and validating system results; and VI – Adoption of appropriate information security measures from the design to the operation of the system. § 1 The governance measures for artificial intelligence systems are applicable throughout their entire life cycle, from initial conception to the termination of their activities and discontinuation.\n\n--- Page 20 ---\n\n§ 2. The technical documentation for high-risk artificial  intelligence systems shall be prepared before they are made available on the market or used for service provision and shall be kept up-to-date during their use. Section II Governance Measures for High-Risk Artificial Intelligence Systems Art. 20. In addition to the measures indicated in Article 19, artificial intelligence agents that provide or operate high-risk systems shall adopt the following governance measures and internal processes: I – documentation, in a format appropriate to the development process and the technology used, regarding the system's operation and the decisions involved in its construction, implementation, and use, considering all relevant stages in the system's life cycle, such as the design, development, evaluation, operation, and discontinuation stages of the system; II – use of tools for automatic recording of system operation, in order to allow the evaluation of its accuracy and robustness and to identify potential discriminatory factors, and implementation of the risk mitigation measures adopted, with special attention to adverse effects; III – conducting tests to assess appropriate levels of reliability, according to the sector and type of application of the artificial intelligence system, including robustness, accuracy, precision, and coverage tests;\n\n--- Page 21 ---\n\nIV – Data management measures to mitigate and prevent  discriminatory biases, including: a)   {{a)}}evaluation of the data with appropriate measures to control for human cognitive biases that may affect data collection and organization, and to avoid the generation of biases due to classification problems, failures or lack of information regarding affected groups, lack of coverage or distortions in representativeness, according to the intended application, as well as corrective measures to avoid the incorporation of structural social biases that may be perpetuated and amplified by technology; and b)   {{b)}}Inclusive team composition responsible for the design and development of the system, guided by the pursuit of diversity. V – Adoption of technical measures to enable the explainability of the results of artificial intelligence systems and measures to provide operators and potential stakeholders with general information about the functioning of the artificial intelligence model employed, explaining the logic and criteria relevant to the production of results, as well as, upon request from the interested party, providing adequate information that allows the interpretation of the results actually produced, respecting industrial and commercial confidentiality. Sole paragraph. Human supervision of high-risk artificial intelligence systems will seek to prevent or minimize risks to the rights and freedoms of individuals that may arise from their normal use or from their use under reasonably foreseeable conditions of misuse, enabling those\n\n--- Page 22 ---\n\nresponsible for human supervision to:  I – to understand the capabilities and limitations of the artificial intelligence system and properly control its operation, so that signs of anomalies, malfunctions, and unexpected performance can be identified and resolved as quickly as possible; II – to be aware of the potential tendency to automatically trust or over-trust the results produced by the artificial intelligence system; III – correctly interpret the results of the artificial intelligence system, taking into account the system's characteristics and the available interpretation tools and methods; IV – to decide, in any specific situation, not to use the high-risk artificial intelligence system or to ignore, annul, or reverse its result; and V – Interfering with the operation of a high-risk artificial intelligence system or disrupting its operation. Art. 21. In addition to the governance measures established in this chapter, public bodies and entities of the Union, States, Federal District and Municipalities, when contracting, developing or using artificial intelligence systems considered high-risk, shall adopt the following measures: I – Conducting prior public consultations and hearings on the planned use of artificial intelligence systems, with\n\n--- Page 23 ---\n\ninformation on the data to be used, the general logic of  operation, and the results of tests performed; II – definition of system access and usage protocols that allow for the recording of who used it, for what specific situation, and for what purpose; III – use of data from reliable sources that are accurate, relevant, up-to-date, and representative of the affected populations and tested against discriminatory biases, in accordance with Law No. 13,709, of August 14, 2018, and its regulatory acts; IV – facilitated and effective guarantee to the citizen, before the public authorities, of the right to human explanation and review of decisions made by artificial intelligence systems that generate relevant legal effects or that significantly impact the interests of the affected party, to be carried out by the competent public agent; V – use of an application programming interface that allows its use by other systems for interoperability purposes, as regulated; and VI – publication in easily accessible media, preferably on their websites, of preliminary assessments of artificial intelligence systems developed, implemented, or used by the public authorities of the Union, States, Federal District, and Municipalities, regardless of the degree of risk, without prejudice to the provisions of article 43. § 1 The use of biometric systems by the public authorities of the Union, States, Federal District and Municipalities shall\n\n--- Page 24 ---\n\nbe preceded by the issuance of a normative act that  establishes guarantees for the exercise of the rights of the affected person and protection against direct, indirect, illegal or abusive discrimination, prohibiting the processing of data on race, color or ethnicity, except as expressly provided by law. § 2 In the event that it is impossible to eliminate or substantively mitigate the risks associated with the artificial intelligence system identified in the algorithmic impact assessment provided for in Article 22 of this Law, its use will be discontinued. Section III Algorithmic Impact Assessment Art. 22. An algorithmic impact assessment of artificial intelligence systems is mandatory for AI agents whenever the system is deemed high-risk by the preliminary assessment. Sole paragraph. The competent authority will be notified about the high-risk system through the sharing of the preliminary and algorithmic impact assessments. Art. 23. The algorithmic impact assessment will be conducted by a professional or team of professionals with the necessary technical, scientific, and legal knowledge to prepare the report, and with functional independence. Sole paragraph. The competent authority will be responsible for regulating the cases in which the performance or\n\n--- Page 25 ---\n\nauditing of the impact assessment will necessarily be  conducted by a professional or team of professionals external to the supplier; Art. 24. The impact assessment methodology will include, at least, the following steps: I - preparation; II – risk awareness; III – mitigating the risks identified; IV – Monitoring. § 1 The impact assessment shall consider and record, at least: a)   {{a)}}known and foreseeable risks associated with the artificial intelligence system at the time it was developed, as well as the risks that can reasonably be expected from it; b)   {{b)}}benefits associated with the artificial intelligence system; (   c) probability of adverse consequences, including the number of people potentially impacted; d)   {{d)}}severity of adverse consequences, including the effort required to mitigate them;\n\n--- Page 26 ---\n\ne)   {{e)}}logic of operation of the artificial intelligence system; f)   {{f)}}process and results of tests and assessments and mitigation measures carried out to verify possible impacts on rights, with special emphasis on potential discriminatory impacts; (   g) Training and awareness campaigns regarding the risks associated with artificial intelligence systems; h)   {{h)}} mitigation measures and indication and justification of the residual risk of the artificial intelligence system, accompanied by frequent quality control tests; and i)   {{i)}}Transparency measures to the public, especially to potential users of the system, regarding residual risks, particularly when they involve a high degree of harmfulness or danger to the health or safety of users, in accordance with articles 9 and 10 of Law No. 8,078, of September 11, 1990 (Consumer Protection Code). § 2 In accordance with the precautionary principle, when using artificial intelligence systems that may generate irreversible or difficult-to-reverse impacts, the algorithmic impact assessment will also take into account incipient, incomplete, or speculative evidence. § 3 The competent authority may establish other criteria and elements for the preparation of impact assessments, including the participation of different affected social segments, according to the risk and economic size of the\n\n--- Page 27 ---\n\norganization.  § 4. The competent authority shall regulate the frequency of updating impact assessments, taking into account the life cycle of high-risk artificial intelligence systems and their fields of application, and may incorporate best sectoral practices. § 5. Artificial intelligence agents that, after their introduction to the market or use in service, become aware of an unexpected risk they present to the rights of natural persons, shall immediately communicate this fact to the competent authorities and to the persons affected by the artificial intelligence system. Art. 25. Algorithmic impact assessment will consist of a continuous iterative process, executed throughout the entire lifecycle of high-risk artificial intelligence systems, requiring periodic updates. § 1 The competent authority shall be responsible for regulating the frequency of updating impact assessments. § 2 The updating of the algorithmic impact assessment will also include public participation, through a consultation procedure with stakeholders, albeit in a simplified manner. Art. 26. With industrial and commercial secrets protected, the conclusions of the impact assessment will be made public,\n\n--- Page 28 ---\n\ncontaining at least the following information:  I – Description of the intended purpose for which the system will be used, as well as its context of use and territorial and temporal scope; II – risk mitigation measures, as well as their residual level, once such measures have been implemented; and III – description of the participation of different affected segments, if any, in accordance with paragraph 3 of article 24 of this Law. CHAPTER V OF CIVIL LIABILITY Art. 27. The supplier or operator of an artificial intelligence system that causes property, moral, individual, or collective damage is obligated to fully compensate for it, regardless of the system's degree of autonomy. § 1 In the case of high-risk or excessively risky artificial intelligence systems, the supplier or operator is objectively liable for the damages caused, to the extent of their participation in the damage. § 2. When the system in question is not a high-risk artificial intelligence system, the fault of the agent causing the damage will be presumed, and the burden of proof will be reversed in favor of the victim. Art. 28.\n\n--- Page 29 ---\n\nArtificial intelligence agents will not be held liable when:  I – prove that they did not put into circulation, use, or take advantage of the artificial intelligence system; or II - prove that the damage resulted from an act solely attributable to the victim or a third party, as well as from an external fortuitous event. Art. 29. The hypotheses of civil liability arising from damages caused by artificial intelligence systems within the scope of consumer relations remain subject to the rules set forth in Law No. 8,078, of September 11, 1990 (Consumer Protection Code), without prejudice to the application of the other provisions of this Law. CHAPTER VI CODES OF GOOD PRACTICE AND GOVERNANCE Art. 30. Artificial intelligence agents may, individually or through associations, formulate codes of good practice and governance that establish the conditions of organization, the operating regime, the procedures, including those regarding complaints from affected individuals, the safety standards, the technical standards, the specific obligations for each implementation context, the educational actions, the internal mechanisms for supervision and risk mitigation, and the appropriate technical and organizational security measures for managing the risks arising from the application of the systems. § 1. When establishing rules of good practice, the purpose, probability, and severity of the risks and benefits arising\n\n--- Page 30 ---\n\ntherefrom shall be considered, as exemplified by the  methodology set forth in Article 24 of this Law. § 2. Developers and operators of artificial intelligence systems may: I – Implement a governance program that, at a minimum: a)   {{a)}} demonstrate your commitment to adopting internal processes and policies that ensure comprehensive compliance with standards and best practices regarding non-maleficence and proportionality between the methods employed and the determined and legitimate purposes of artificial intelligence systems; b)   {{b)}} be adapted to the structure, scale and volume of its operations, as well as its potential for harm; c)   {{c)}} aims to establish a relationship of trust with the affected people, through transparent actions that ensure participation mechanisms as per article 24, § 3, of this Law; d)   {{d)}} is integrated into its overall governance structure and establishes and applies internal and external oversight mechanisms; (e)   {{e)}}have response plans in place to reverse any potentially harmful outcomes from the artificial intelligence system; and f)   {{f)}} be constantly updated based on information obtained from continuous monitoring and periodic evaluations.\n\n--- Page 31 ---\n\n§ 3. Voluntary adherence to a code of good practices and governance may be considered indicative of good faith on the part of the agent and will be taken into account by the competent authority for the purpose of applying administrative sanctions. § 4 The competent authority may establish a procedure for analyzing the compatibility of the code of conduct with current legislation, with a view to its approval, publication and periodic updating. CHAPTER VII ON REPORTING SERIOUS INCIDENTS Art. 31. Artificial intelligence agents will report serious security incidents to the competent authority, including those involving risks to life and physical integrity, disruption of critical infrastructure operations, serious damage to property or the environment, as well as serious violations of fundamental rights, in accordance with the regulations. § 1 The communication will be made within a reasonable timeframe, as defined by the competent authority. § 2 The competent authority will verify the seriousness of the incident and may, if necessary, order the agent to take steps and measures to reverse or mitigate the effects of the incident. CHAPTER VIII ON SUPERVISION AND INSPECTION Section I Of the Competent Authority\n\n--- Page 32 ---\n\nArt. 32.  The Executive Branch will designate the competent authority to oversee the implementation and enforcement of this Law. Sole paragraph. It is the responsibility of the competent authority to: I – to ensure the protection of fundamental rights and other rights affected by the use of artificial intelligence systems; II – to promote the development, updating, and implementation of the Brazilian Artificial Intelligence Strategy with the relevant competent bodies; III – To promote and develop studies on best practices in the development and use of artificial intelligence systems; IV – to encourage the adoption of best practices, including codes of conduct, in the development and use of artificial intelligence systems; V – to promote cooperation actions with authorities responsible for the protection and development and use of artificial intelligence systems in other countries, of an international or transnational nature; VI – to issue regulations for this Law, including regarding: a)   {{a)}} procedures associated with the exercise of the rights provided for in this Law;\n\n--- Page 33 ---\n\nb)   Procedures and requirements for preparing the  algorithmic impact assessment; c)   {{c)}}Form and requirements of the information to be made public about the use of artificial intelligence systems; and d)   Procedures for certification of the development and use of high-risk systems. VII – to coordinate with public regulatory authorities to exercise their powers in specific sectors of economic and governmental activities subject to regulation; VIII – to oversee, independently or jointly with other competent public bodies, the dissemination of the information provided for in Articles 7 and 43; IX – to oversee and apply sanctions in cases of development or use of artificial intelligence systems carried out in violation of the law, through an administrative process that ensures due process, the right to a full defense, and the right to appeal; X – to request, at any time, from public entities that develop or use artificial intelligence systems, specific information on the scope, nature of the data, and other details of the processing carried out, with the possibility of issuing a supplementary technical opinion to ensure compliance with this Law; XI – to enter into agreements, at any time, with artificial intelligence agents to eliminate irregularities, legal\n\n--- Page 34 ---\n\nuncertainty, or contentious situations within the scope of  administrative processes, in accordance with the provisions of Decree-Law No. 4,657, of September 4, 1942; XII – to review petitions against the artificial intelligence system operator, after proof of submission of a complaint that was not resolved within the time frame established in regulations; and XIII – to prepare annual reports on its activities. Sole paragraph. In exercising the powers of the main clause, the competent body may establish differentiated conditions, requirements, communication and dissemination channels for suppliers and operators of artificial intelligence systems qualified as micro or small enterprises, under the terms of Complementary Law No. 123, of December 14, 2006, and startups, under the terms of Complementary Law No. 182, of June 1, 2021. Art. 33. The competent authority will be the central body for the application of this Law and for establishing rules and guidelines for its implementation. Art. 34. The competent authority and the public bodies and entities responsible for regulating specific sectors of economic and governmental activity shall coordinate their activities, within their respective spheres of action, with a view to ensuring compliance with this Law.\n\n--- Page 35 ---\n\n§ 1 The competent authority shall maintain a permanent forum for communication, including through technical cooperation, with bodies and entities of the public administration responsible for regulating specific sectors of economic and governmental activity, in order to facilitate its regulatory, supervisory and sanctioning powers. § 2 In experimental regulatory environments (regulatory sandboxes) involving artificial intelligence systems, conducted by public bodies and entities responsible for regulating specific sectors of economic activity, the competent authority will be notified and may comment on compliance with the purposes and principles of this law. Art. 35. The regulations and standards issued by the competent authority will be preceded by public consultation and hearings, as well as regulatory impact analyses, in accordance with articles 6 to 12 of Law No. 13,848, of June 25, 2019, where applicable. Section II Administrative Sanctions Art. 36. Artificial intelligence agents, due to violations of the rules established in this Law, are subject to the following administrative sanctions applicable by the competent authority: I – warning;\n\n--- Page 36 ---\n\nII – a simple fine, limited in total to R$ 50,000,000.00 (fifty  million reais) per infraction, being, in the case of a private legal entity, up to 2% (two percent) of its revenue, of its group or conglomerate in Brazil in its last fiscal year, excluding taxes; III – Publicizing the infraction after it has been duly investigated and its occurrence confirmed; IV – prohibition or restriction on participating in a regulatory sandbox regime provided for in this Law, for up to five years; V – partial or total, temporary or permanent suspension of the development, supply, or operation of the artificial intelligence system; and VI – Prohibition of processing certain databases. § 1 The sanctions will be applied after an administrative procedure that allows for the opportunity of full defense, in a gradual, isolated or cumulative manner, according to the peculiarities of the specific case and considering the following parameters and criteria: I – the seriousness and nature of the offenses and the possible violation of rights; II - the good faith of the offender; III – the advantage gained or intended by the offender; IV – the economic condition of the offender;\n\n--- Page 37 ---\n\nV – recidivism; VI – the degree of damage; VII – the cooperation of the offender; VIII – the repeated and demonstrated adoption of internal mechanisms and procedures capable of minimizing risks, including algorithmic impact analysis and effective implementation of a code of ethics; IX – the adoption of a policy of good practices and governance; X – the prompt adoption of corrective measures; XI – the proportionality between the seriousness of the offense and the severity of the sanction; and XII – the accumulation with other administrative sanctions that may have already been definitively applied for the same unlawful act. § 2 Before or during the administrative process of § 1, the competent authority may adopt preventive measures, including a penalty fine, observing the total limit referred to in item II of the main clause, when there is evidence or well- founded fear that the artificial intelligence agent: I – cause or may cause irreparable or difficult-to-repair injury; or II – render the final result of the process ineffective.\n\n--- Page 38 ---\n\n§ 3 The provisions of this article do not replace the application of administrative, civil or criminal sanctions defined in Law No. 8,078, of September 11, 1990, in Law No. 13,709, of August 14, 2018, and in specific legislation. § 4 In the case of the development, supply or use of excessively risky artificial intelligence systems, at a minimum, a fine will be applied and, in the case of a legal entity, the partial or total, temporary or permanent suspension of its activities. § 5 The application of the sanctions provided for in this article does not, under any circumstances, exclude the obligation to fully compensate for the damage caused, pursuant to article 27. Art. 37. The competent authority will define, through its own regulations, the procedure for investigating and the criteria for applying administrative sanctions to violations of this Law, which will be subject to public consultation, without prejudice to the provisions of Decree-Law No. 4,657, of September 4, 1942, Law No. 9,784, of January 29, 1999, and other relevant legal provisions. Sole paragraph. The methodologies referred to in the heading of this article will be published in advance and will objectively present the forms and dosages of the sanctions, which will contain a detailed justification of all their elements, demonstrating compliance with the criteria provided for in this Law.\n\n--- Page 39 ---\n\nSection III Measures to promote innovation  Art. 38. The competent authority may authorize the operation of an experimental regulatory environment for innovation in artificial intelligence (regulatory sandbox) for entities that request it and meet the requirements specified by this Law and regulations. Art. 39. Applications for authorization for regulatory sandboxes will be submitted to the competent authority through a project whose characteristics include, among others: I – innovation in the use of technology or in the alternative use of existing technologies; II – improvements aimed at efficiency gains, cost reduction, increased safety, risk reduction, benefits to society and consumers, among others; III – Discontinuation plan, outlining the measures to be taken to ensure the operational viability of the project once the regulatory sandbox authorization period has ended. Art. 40. The competent authority will issue regulations to establish the procedures for requesting and authorizing the operation of regulatory sandboxes, and may limit or interrupt their operation, as well as issue recommendations, taking into\n\n--- Page 40 ---\n\naccount, among other aspects, the preservation of  fundamental rights, the rights of potentially affected consumers, and the security and protection of personal data that are the subject of processing. Art. 41. Participants in the artificial intelligence regulation testing environment remain liable, under applicable liability legislation, for any damages inflicted on third parties as a result of experimentation taking place in the testing environment. Art. 42. The automated use of works, such as extraction, reproduction, storage, and transformation, in data and text mining processes using artificial intelligence systems, in activities carried out by research organizations and institutions, journalism, and by museums, archives, and libraries, does not constitute copyright infringement, provided that: I – does not aim solely at the reproduction, exhibition or dissemination of the original work itself; II – the use occurs to the extent necessary to achieve the objective; III – does not unjustifiably harm the economic interests of the holders; and IV – does not interfere with the normal operation of the works.\n\n--- Page 41 ---\n\n§ 1 Any reproductions of works for data mining activities shall be kept under strict security conditions, and only for the time necessary to carry out the activity or for the specific purpose of verifying the results of the scientific research. § 2. The provisions of the main clause apply to data and text mining activities for other analytical activities in artificial intelligence systems, provided that the conditions of the main clause and § 1 are met, and provided that the activities do not communicate the work to the public and that access to the works has been legitimate. § 3 The activity of text and data mining involving personal data will be subject to the provisions of Law No. 13.709, of August 14, 2018 (General Law on the Protection of Personal Data). Section IV Public Artificial Intelligence Database Art. 43. It is the responsibility of the competent authority to create and maintain a publicly accessible, high-risk artificial intelligence database containing public impact assessment documents, respecting commercial and industrial secrets, in accordance with the regulations. CHAPTER IX FINAL PROVISIONS Art. 44. The rights and principles expressed in this Law do not exclude others provided for in the national legal system or in\n\n--- Page 42 ---\n\ninternational treaties to which the Federative Republic of Brazil  is a party. Art. 45. This law shall enter into force one year after its publication. JUSTIFICATION The development and popularization of artificial intelligence technologies have revolutionized various areas of human activity. Furthermore, predictions indicate that artificial intelligence (AI) will bring about even more profound economic and social changes in the near future. Recognizing the importance of this issue, several legislative proposals have recently been introduced in both the Federal Senate and the Chamber of Deputies, aiming to establish guidelines for the development and application of artificial intelligence systems in Brazil. In particular, noteworthy bills include Bill No. 5,051 of 2019, authored by Senator Styvenson Valentim, which establishes the principles for the use of Artificial Intelligence in Brazil; Bill No. 21 of 2020, by Federal Deputy Eduardo Bismarck, which establishes the foundations, principles, and guidelines for the development and application of artificial intelligence in Brazil; and provides other measures, and which was approved by the Chamber of Deputies; and Bill No. 872 of 2021, by Senator Veneziano Vital do Rêgo, which deals with the use of Artificial Intelligence. On February 3, 2022, these three projects began to be processed jointly in the Federal Senate and, subsequently, on February 17 of the same year, through Act No. 4 of 2022 of the President of the\n\n--- Page 43 ---\n\nFederal Senate, authored by me at the suggestion of Senator  Eduardo Gomes, with the aim of drafting a legal text with the most advanced technicality, a Commission of Jurists was established to support the drafting of a substitute bill for them. Composed of renowned jurists, the commission included leading experts in the fields of civil law and digital law, to whom I am grateful for their time, dedication, and sharing of the final text, which I now present. The panel included: Minister of the Superior Court of Justice, Ricardo Villas Bôas Cueva (President); Laura Schertel Ferreira Mendes (Rapporteur); Ana de Oliveira Frazão; Bruno Ricardo Bioni; Danilo Cesar Maganhoto Doneda (in memoriam); Fabrício de Mota Alves; Miriam Wimmer; Wederson Advincula Siqueira; Claudia Lima Marques; Juliano Souza de Albuquerque Maranhão; Thiago Luís Santos Sombra; Georges Abboud; Frederico Quadros D'Almeida; Victor Marcel Pinheiro; Estela Aranha; Clara Iglesias Keller; Mariana Giorgetti Valente; and Filipe José Medon Affonso. Furthermore, I must also express my gratitude to the technical staff of the Federal Senate, especially the Legislative Consulting office and the employees who provided support to the committee: Reinilson Prado dos Santos; Renata Felix Perez and Donaldo Portela Rodrigues. The aforementioned Commission held a series of public hearings, as well as an international seminar, listening to more than seventy experts on the subject, representing various segments: organized civil society, government, academia, and the private sector. It also opened the opportunity for participation by any interested parties through written contributions, receiving 102 submissions, which were individually analyzed and organized according to their proposals. Finally, the Commission requested a study from the Legislative Advisory Office of the Federal Senate on the regulation of artificial intelligence in more than thirty countries belonging to the Organisation for Economic Co-\n\n--- Page 44 ---\n\noperation and Development (OECD), which allowed for an  analysis of the global regulatory landscape on the subject. Based on all this extensive material, on December 6, 2022, the Commission of Jurists presented its final report, along with a draft bill for the regulation of artificial intelligence. In this context, the present initiative is based on the conclusions of the aforementioned Commission and seeks to reconcile, within the legal framework, the protection of fundamental rights and freedoms, the promotion of work and the dignity of the human person, and the technological innovation represented by artificial intelligence. The project has a dual objective. On the one hand, it establishes rights to protect the most vulnerable party in question, the individual who is already impacted daily by artificial intelligence systems, from content recommendations and targeted advertising on the Internet to their eligibility analysis for credit and certain public policies. On the other hand, by providing governance tools and an institutional framework for oversight and supervision, it creates conditions for predictability regarding its interpretation and, ultimately, legal certainty for innovation and technological development. The proposition therefore starts from the premise that there is no trade-off between the protection of fundamental rights and freedoms, the valorization of work and the dignity of the human person in the face of the economic order and the creation of new value chains. On the contrary, its foundations and its principled basis seek such harmonization, in accordance with the Federal Constitution. Structurally, the proposal establishes risk-based regulation and a\n\n--- Page 45 ---\n\nrights-based regulatory model. It also presents governance  instruments for adequate accountability of economic agents who develop and use artificial intelligence, encouraging good faith action and effective risk management. The proposed text initially defines the fundamentals and general principles for the development and use of artificial intelligence systems, which underpin all other specific provisions. It dedicates a specific chapter to protecting the rights of people affected by artificial intelligence systems, in which it: guarantees appropriate access to information and adequate understanding of the decisions made by these systems; establishes and regulates the right to contest automated decisions and to request human intervention; and governs the right to non-discrimination and the correction of discriminatory biases. In addition to establishing basic and transversal rights for any and all contexts in which there is interaction between machines and humans, such as information and transparency, this obligation intensifies when the AI   system produces relevant legal effects or significantly impacts individuals (e.g., the right to contest and human intervention). Thus, the weight of regulation is calibrated according to the potential risks of the technology's application context. Symmetrically to these rights, certain general and specific governance measures were established for, respectively, artificial intelligence systems with any degree of risk and those categorized as high-risk. When addressing the categorization of artificial intelligence risks, the proposal establishes the requirement for preliminary assessment; defines prohibited applications due to excessive risk; and defines high-risk applications subject to stricter control standards.\n\n--- Page 46 ---\n\nRegarding the governance of systems, the project lists the measures to be adopted to ensure transparency and the mitigation of biases; establishes additional measures for high-risk systems and for governmental artificial intelligence systems; and standardizes the procedure for algorithmic impact assessment. The text also addresses the rules of civil liability involving artificial intelligence systems, including defining the circumstances under which those responsible for their development and use will not be held liable. According to the gradation of standards based on the risk posed by the system – which permeates the entire draft of the proposal – an important distinction is made in the chapter on civil liability: when dealing with a high-risk or excessively risky AI system, the supplier or operator is objectively liable for the damages caused, to the extent of each party's participation in the damage. And when dealing with AI that is not high-risk, the fault of the agent causing the damage will be presumed, applying the reversal of the burden of proof in favor of the victim. The project also reinforces protection against discrimination through various instruments, such as the right to information and understanding, the right to contest, and a specific right to correct direct, indirect, illegal, or abusive discriminatory biases, in addition to preventive governance measures. Besides adopting definitions of direct and indirect discrimination – thus incorporating definitions from the Inter-American Convention against Racism, promulgated in 2022 – the text focuses on (hyper)vulnerable groups, both for the qualification of what constitutes a high-risk system and for the reinforcement of certain rights. Regarding the oversight of artificial intelligence, the bill stipulates\n\n--- Page 47 ---\n\nthat the Executive Branch designate an authority to ensure  compliance with the established rules, specifies its competencies, and sets administrative sanctions. Measures are also planned to foster innovation in artificial intelligence, notably through an experimental regulatory environment (regulatory sandbox). Thus, using a mixed approach of ex-ante and ex-post provisions, the proposal outlines criteria for evaluating and triggering the types of actions that should be taken to mitigate the risks involved, also involving the sectors interested in the regulatory process, through co-regulation. Furthermore, in line with international law, it establishes guidelines for aligning copyright and intellectual property rights with the notion that data should be a common good and, therefore, circulate for machine training and the development of artificial intelligence systems – without, however, implying harm to the holders of such rights. This leads to implications for how regulation can foster innovation. Given the above, and aware of the challenge this matter represents, we count on the collaboration of our esteemed colleagues to improve this proposal. Session Room, Senator Rodrigo Pacheco",
    "analysis": {
      "governance_power_accountability": "The system embeds power in the hands of the developers and operators of AI systems, who are private or public entities. These are detailed in Articles 4 (II) and (III). The legislation also demands accountability throughout the lifecycle of AI systems (Article 3, IX) and ensures full reparation of damages (Article 3, X). However, the text lacks clear mechanisms to enforce such accountability beyond self-imposed guidelines and does not specify who bears the risk of unintended consequences.",
      "plurality_inclusion_embodiment": "The legislation emphasizes plurality, non-discrimination, equity, and inclusion (Article 2, V and Article 3, V), signaling an effort to include diverse knowledge systems and embodied experiences. However, the text does not explicitly mention the inclusion of Indigenous, disability, or non-Western perspectives, which might be overlooked in practice.",
      "agency_codesign_self_determination": "The law promotes self-determination (Article 2, VIII and Article 3, II). It implies the right to refuse through the principle of informational self-determination and freedom of choice. However, the legislation does not explicitly provide for community agency or co-design possibilities.",
      "reflexivity_situated_praxis": "The legislation exhibits reflexivity in acknowledging the potential risks of AI systems (Article 3, XI) and the need for human supervision (Article 3, III). Yet, the text does not clearly demonstrate situated praxis, as it does not consider the historical and socioeconomic factors that might influence the development and implementation of AI systems.",
      "legitimacy_claims": {
        "source": "Democratic",
        "mechanisms": "The legislation establishes its legitimacy by invoking democratic governance principles, human-centric approach, and respect for human rights (Article 2, II).",
        "tensions": "The text exhibits tension between democratic legitimacy (based on principles of human rights and inclusivity) and technocratic legitimacy (based on principles of technological development and innovation - Article 2, VI)."
      },
      "key_insight": "The Brazilian AI Bill emphasizes inclusion, human rights, and democratic values, but lacks explicit mechanisms for accountability enforcement, inclusion of non-dominant perspectives, and reflexivity.",
      "governance_scores": {
        "centralization": 50,
        "rights_focus": 70,
        "flexibility": 50,
        "market_power": 40,
        "procedurality": 60
      },
      "structural_pillars": {
        "risk": {
          "title": "Systemic Risk Mitigation",
          "description": "The legislation acknowledges the systemic risks of AI systems and requires measures for their prevention, precaution, and mitigation.",
          "badge": "Risk-aware",
          "quote": "Article 3, XI: prevention, precaution and mitigation of systemic risks arising from intentional or unintentional uses and unforeseen effects of artificial intelligence systems."
        },
        "enforcement": {
          "title": "Accountability Enforcement",
          "description": "The law provides for accountability throughout the life cycle of AI systems, but lacks clear enforcement mechanisms.",
          "badge": "Inconsistent",
          "quote": "Article 3, IX: traceability of decisions throughout the lifecycle of artificial intelligence systems as a means of accountability..."
        },
        "rights": {
          "title": "Human Rights Protection",
          "description": "The legislation strongly emphasizes the protection of human rights.",
          "badge": "Rights-focused",
          "quote": "Article 2, II: respect for human rights and democratic values"
        },
        "scope": {
          "title": "Broad Scope",
          "description": "The legislation applies to all AI systems developed, implemented, or used in Brazil.",
          "badge": "Comprehensive",
          "quote": "Article 1: This Law establishes general national standards for the development, implementation, and responsible use of artificial intelligence systems in Brazil..."
        }
      }
    },
    "cultural_framing": {
      "state_market_society": "The document assumes a significant role for the government in setting standards for AI technologies, which implies a belief in a strong state-market relationship. The market is seen as an actor in the development and application of AI technologies, but within a regulatory framework defined by the government. Society is assumed to be both a beneficiary of and a participant in technological development in terms of research and innovation, labor rights, and consumer protection.",
      "technology_role": "AI technology is positioned as a tool and an infrastructure that can significantly impact social life. It is seen as an opportunity for technological development and innovation but also as a potential threat due to possible unforeseen effects and systemic risks, which necessitates a regulatory approach.",
      "rights_conception": "There is an emphasis on both individual and collective rights. Procedural rights are reflected in the principles of due process of law, contestability, and adversarial process. Substantive rights are highlighted in the principles of privacy, data protection, informational self-determination, and full reparation of damages.",
      "historical_context": "Brazil's historical experiences with inequality and human rights violations, as well as its ongoing democratic evolution, shape this approach to AI technologies. The bill's focus on human rights, equality, non-discrimination, and pluralism reflects these historical experiences.",
      "epistemic_authority": "The bill privileges scientific knowledge and technological expertise in the development and implementation of AI systems. At the same time, it also validates democratic values, human rights principles, and legal norms. This suggests a blend of technocratic and democratic legitimacy in the bill's approach to AI governance.",
      "cultural_distinctiveness_score": 0.7,
      "dominant_cultural_logic": "Democratic technocratic governance"
    },
    "institutional_logics": {
      "logics": {
        "market": {
          "strength": 0.6,
          "champions": [
            "Article 2: VII – free enterprise, free competition and consumer protection;",
            "Article 3: I - inclusive growth, sustainable development and well-being"
          ],
          "material": "Encourages free enterprise, competition, and consumer protection. Prioritizes growth and sustainable development.",
          "discursive": "Language promoting free enterprise, competition, and consumer protection. Frames AI as a tool for growth and sustainable development.",
          "key_tensions": [
            "State Logic: Regulatory controls and public interest versus free enterprise and competition",
            "Community Logic: Collective well-being versus individual growth and profit"
          ]
        },
        "state": {
          "strength": 0.8,
          "champions": [
            "Preamble: for the benefit of humankind, the democratic regime, and scientific and technological development",
            "Article 2: II – respect for human rights and democratic values",
            "Article 3: VIII – due process of law, contestability and adversarial process"
          ],
          "material": "Lays down regulations for AI development, implementation, and use. Prioritizes human rights and democratic values.",
          "discursive": "Language promoting democratic values, human rights, and due process of law. Frames AI as a tool for upholding the democratic regime and human rights.",
          "key_tensions": [
            "Market Logic: Free enterprise and competition versus regulatory controls and public interest"
          ]
        },
        "professional": {
          "strength": 0.6,
          "champions": [
            "Author: Senator Rodrigo Pacheco",
            "Article 2: IX – the promotion of research and development",
            "Article 3: VI – transparency, explainability, intelligibility and auditability"
          ],
          "material": "Encourages the professional development and research in AI. Prioritizes transparency, explainability, and auditability of AI systems.",
          "discursive": "Language promoting research and development, transparency, explainability, and auditability. Frames AI as a professional field requiring a high degree of expertise and standardization.",
          "key_tensions": [
            "State Logic: Democratic accountability and public interest versus professional autonomy and technical expertise",
            "Community Logic: Participation and collective well-being versus technical expertise and professional autonomy"
          ]
        },
        "community": {
          "strength": 0.4,
          "champions": [
            "Article 2: V – equality, non-discrimination, plurality and respect for labor rights",
            "Article 3: V – justice, equity and inclusion"
          ],
          "material": "Promotes equality, non-discrimination, and labor rights. Prioritizes justice, equity, and inclusion.",
          "discursive": "Language promoting equality, non-discrimination, and social justice. Frames AI as a tool for promoting community well-being.",
          "key_tensions": [
            "Market Logic: Free enterprise and competition versus collective well-being",
            "Professional Logic: Technical expertise and professional autonomy versus participation and community solidarity"
          ]
        }
      },
      "dominant_logic": "state",
      "logic_conflicts": [
        {
          "between": "market and state",
          "site_of_conflict": "Regulation of AI",
          "resolution_strategy": "Bill attempts to balance free enterprise and competition with regulatory controls and public interest"
        },
        {
          "between": "professional and community",
          "site_of_conflict": "Implementation and use of AI",
          "resolution_strategy": "Bill attempts to promote technical expertise and standardization while ensuring community participation and well-being"
        }
      ],
      "overall_assessment": "This policy document on AI in Brazil embodies a predominance of state logic, with a focus on regulatory control, democratic values and public interest. It also effectively balances professional and market logics by promoting both technical expertise and free enterprise. Community logic is relatively less dominant, but still present as the document prioritizes inclusivity, equality, and collective well-being."
    }
  },
  {
    "id": "1764008116106ihnzqf0yc",
    "title": "[Trace] Backpaying employees upon next successful funding round : r/startups",
    "description": "Jan 16, 2024 ... 75 votes, 132 comments. Is this a normal thing seed-stage founders do? For context, I'm trying to hire someone whom I feel will be critical ...",
    "type": "Trace",
    "extractedText": "Jan 16, 2024 ... 75 votes, 132 comments. Is this a normal thing seed-stage founders do? For context, I'm trying to hire someone whom I feel will be critical ...\n\nSource: https://www.reddit.com/r/startups/comments/19863b8/backpaying_employees_upon_next_successful_funding/\n\nFound via search for: \"Brazil PL 2338 startup founders compliance burden reddit\"",
    "addedDate": "Nov 24, 2025",
    "status": "Active Case",
    "colorClass": "bg-blue-100",
    "iconClass": "text-blue-600"
  },
  {
    "id": "176400811610641au55jhv",
    "title": "[Trace] r/startups on Reddit: What do you HATE about software salespeople",
    "description": "Oct 17, 2023 ... I came from a very tech non sales related role to sales role as a founder with domain knowledge in the domain where my audience typically is. I ...",
    "type": "Trace",
    "extractedText": "Oct 17, 2023 ... I came from a very tech non sales related role to sales role as a founder with domain knowledge in the domain where my audience typically is. I ...\n\nSource: https://www.reddit.com/r/startups/comments/17a70gh/what_do_you_hate_about_software_salespeople/\n\nFound via search for: \"Brazil PL 2338 startup founders compliance burden reddit\"",
    "addedDate": "Nov 24, 2025",
    "status": "Active Case",
    "colorClass": "bg-blue-100",
    "iconClass": "text-blue-600"
  },
  {
    "id": "1764008116106flqdae8ye",
    "title": "[Trace] Microsoft Enforces Tougher Layoff Rules: No Severance, Immediate ...",
    "description": "Jan 31, 2025 ... However, Microsoft is cutting costs and having no severance and benefits extended are a clear example. Trying to free funds maybe to acquire or ...",
    "type": "Trace",
    "extractedText": "Jan 31, 2025 ... However, Microsoft is cutting costs and having no severance and benefits extended are a clear example. Trying to free funds maybe to acquire or ...\n\nSource: https://www.reddit.com/r/Layoffs/comments/1iefv1r/microsoft_enforces_tougher_layoff_rules_no/\n\nFound via search for: \"Brazil PL 2338 startup founders compliance burden reddit\"",
    "addedDate": "Nov 24, 2025",
    "status": "Active Case",
    "colorClass": "bg-blue-100",
    "iconClass": "text-blue-600"
  },
  {
    "id": "1764008116106cump2llde",
    "title": "[Trace] Minimum wage from April 2025 : r/UKJobs",
    "description": "Jan 5, 2025 ... Completely- I think there will be lots of claims for underpayment which will surprise a lot of companies who think they are compliant with the ...",
    "type": "Trace",
    "extractedText": "Jan 5, 2025 ... Completely- I think there will be lots of claims for underpayment which will surprise a lot of companies who think they are compliant with the ...\n\nSource: https://www.reddit.com/r/UKJobs/comments/1hu6nu3/minimum_wage_from_april_2025/\n\nFound via search for: \"Brazil PL 2338 startup founders compliance burden reddit\"",
    "addedDate": "Nov 24, 2025",
    "status": "Active Case",
    "colorClass": "bg-blue-100",
    "iconClass": "text-blue-600"
  },
  {
    "id": "1764008116106xc56uqrj1",
    "title": "[Trace] Thinking of quitting my job to start selling macarons : r/careerchange",
    "description": "Jan 8, 2024 ... I see more growth potential in starting my own business vs staying and climbing the corporate ladder. Plus almost every other position in Pharma ...",
    "type": "Trace",
    "extractedText": "Jan 8, 2024 ... I see more growth potential in starting my own business vs staying and climbing the corporate ladder. Plus almost every other position in Pharma ...\n\nSource: https://www.reddit.com/r/careerchange/comments/191tnzd/thinking_of_quitting_my_job_to_start_selling/\n\nFound via search for: \"Brazil PL 2338 startup founders compliance burden reddit\"",
    "addedDate": "Nov 24, 2025",
    "status": "Active Case",
    "colorClass": "bg-blue-100",
    "iconClass": "text-blue-600",
    "resistance_analysis": {
      "strategy_detected": "Refusal",
      "evidence_quote": "I see more growth potential in starting my own business vs staying and climbing the corporate ladder.",
      "interpretation": "This quote represents the strategy of 'refusal' as the individual is choosing to opt out of the corporate ladder and instead start their own business. This can be seen as a form of resistance against the algorithmic power of corporate hierarchies.",
      "confidence": "Medium"
    }
  }
]