# DEEP FORENSIC ANALYSIS: Absent Actor Verification

**KNOWN DOMINANT DISCOURSES (Use these preferentially for categorization):**
precautionary principle / risk aversion, social equity, data protection / privacy, bureaucratic standardization, technical expertise


## Task
Perform deep forensic analysis on 5 pre-screened absent actor candidates.
For each candidate, verify their absence and assess their potential impact if they were included.

**CRITICAL: If a candidate is NOT verified as a meaningful absence after deep analysis, OMIT them from the `absentActors` array. However, you MUST ALWAYS return the `institutionalLogics` and `methodologicalNotes` objects.**

## Existing Network Actors
Brazilian AI Regulatory Framework, Risk-Based Regulation Model, Rights-Based Regulatory Model, AI Risk Categorization Regime, AI Governance Framework, Civil Liability Regime for AI, Regulatory Sandbox for AI, Artificial Intelligence System, Preliminary Risk Assessment Process, Algorithmic Impact Assessment Process, High-Risk AI Governance Measures, Excessive-Risk AI Prohibition Mechanism, Human Oversight and Intervention Mechanism, Rights to Information and Explanation Mechanism, Contestability and Review Mechanism, Non-Discrimination Safeguard Mechanism, Administrative Sanctions Procedure, Codes of Good Practice and Governance, Serious Incident Reporting Channel, Text and Data Mining Exception Regime, AI System Providers, AI System Operators, Artificial Intelligence Agents, Competent Authority for AI, Sectoral Regulatory Bodies, Public Authorities Using AI, Individuals Affected by AI Systems, Hypervulnerable Groups, Research and Cultural Institutions, Fundamental Rights and Freedoms, Non-Discrimination and Equality, Transparency and Explainability, Privacy and Data Protection, Accountability and Traceability, Safety, Reliability, and Robustness, Legal Certainty for Innovation, Environmental Protection and Sustainable Development, Democratic Values and Human-Centricity, Access to Information and Education on AI, Innovation and Technological Development, Public AI Database of High-Risk Systems

## Institutional Logic Framework
Assess the document's dominant logics:
- **Market Logic**: Profit, efficiency, competition, innovation
- **State Logic**: Regulation, public interest, sovereignty, compliance
- **Professional Logic**: Expertise, credentials, technical standards
- **Community Logic**: Participation, solidarity, local knowledge, equity

---

## CANDIDATES TO ANALYZE

### Candidate: "People with Disabilities"
**Initial reason**: The document fails to highlight how AI impacts accessibility for individuals with disabilities.
**Initial score**: 85/100

**Relevant document sections:**
[Article 16 — It will be up to the competent authority to regulate artificial intelligence systems that pose excessive risks. Section III High Risk Art. 17. Artificial intelligence systems used for the following purposes are considered high-risk: I – application as safety devices in the management and operation of critical infrastructures, such as traffic control and water and electricity supply networks; II – education and vocational training, including systems for determining access to educational or vocational training institutions or for evaluating and monitoring students; III – recruitment, screening, filtering, candidate evaluation, decision-making regarding promotions or terminations of employment contracts, task allocation, and monitoring and evaluation of the performance and behavior of individuals affected by such artificial intelligence applications in the areas of employment, worker management, and access to self-employment; IV – evaluation of criteria for access, eligibility, granting, review, reduction or revocation of private and public services that are considered essential, including systems used to assess the eligibility of individuals for public]
Art. 16. It will be up to the competent authority to regulate artificial intelligence systems that pose excessive risks. Section III High Risk Art. 17. Artificial intelligence systems used for the following purposes are considered high-risk: I – application as safety devices in the management and operation of critical infrastructures, such as traffic control and water and electricity supply networks; II – education and vocational training, including systems for determining access to educational or vocational training institutions or for evaluating and monitoring students; III – recruitment, screening, filtering, candidate evaluation, decision-making regarding promotions or terminations of employment contracts, task allocation, and monitoring and evaluation of the performance and behavior of individuals affected by such artificial intelligence applications in the areas of employment, worker management, and access to self-employment; IV – evaluation of criteria for access, eligibility, granting, review, reduction or revocation of private and public services that are considered essential, including systems used to assess the eligibility of individuals for public

--- Page 16 ---

assistance and social security services;  V – assessment of the debt capacity of individuals or establishment of their credit rating; VI – dispatching or establishing priorities for emergency response services, including firefighters and medical assistance; VII – administration of justice, including systems that assist judicial authorities in investigating facts and applying the law; VIII – autonomous vehicles, when their use may generate risks to the physical integrity of people; IX – applications in the health field, including those intended to assist in medical diagnoses and procedures; X – biometric identification systems; XI – criminal investigation and public security, in particular for individual risk assessments by the competent authorities, in order to determine the risk of a person committing offenses or of recidivism, or the risk to potential victims of criminal offenses, or to assess the personality traits and characteristics or past criminal behavior of individuals or groups; XII – analytical study of crimes related to natural persons, allowing police authorities to search large sets of complex data, related or unrelated, available from different data sources or in different data formats, in order to identify

--- Page 17 ---

unknown patterns or discover hidden relationships in the  data; XIII – investigation by administrative authorities to assess the credibility of evidence during the investigation or repression of offenses, to predict the occurrence or recurrence of an actual or
... [truncated due to length budget]



---

### Candidate: "Ethical AI Advocates"
**Initial reason**: Voices advocating for ethical considerations in AI design and implementation are not present.
**Initial score**: 82/100

**Relevant document sections:**
[Article 16 — It will be up to the competent authority to regulate artificial intelligence systems that pose excessive risks. Section III High Risk Art. 17. Artificial intelligence systems used for the following purposes are considered high-risk: I – application as safety devices in the management and operation of critical infrastructures, such as traffic control and water and electricity supply networks; II – education and vocational training, including systems for determining access to educational or vocational training institutions or for evaluating and monitoring students; III – recruitment, screening, filtering, candidate evaluation, decision-making regarding promotions or terminations of employment contracts, task allocation, and monitoring and evaluation of the performance and behavior of individuals affected by such artificial intelligence applications in the areas of employment, worker management, and access to self-employment; IV – evaluation of criteria for access, eligibility, granting, review, reduction or revocation of private and public services that are considered essential, including systems used to assess the eligibility of individuals for public]
Art. 16. It will be up to the competent authority to regulate artificial intelligence systems that pose excessive risks. Section III High Risk Art. 17. Artificial intelligence systems used for the following purposes are considered high-risk: I – application as safety devices in the management and operation of critical infrastructures, such as traffic control and water and electricity supply networks; II – education and vocational training, including systems for determining access to educational or vocational training institutions or for evaluating and monitoring students; III – recruitment, screening, filtering, candidate evaluation, decision-making regarding promotions or terminations of employment contracts, task allocation, and monitoring and evaluation of the performance and behavior of individuals affected by such artificial intelligence applications in the areas of employment, worker management, and access to self-employment; IV – evaluation of criteria for access, eligibility, granting, review, reduction or revocation of private and public services that are considered essential, including systems used to assess the eligibility of individuals for public

--- Page 16 ---

assistance and social security services;  V – assessment of the debt capacity of individuals or establishment of their credit rating; VI – dispatching or establishing priorities for emergency response services, including firefighters and medical assistance; VII – administration of justice, including systems that assist judicial authorities in investigating facts and applying the law; VIII – autonomous vehicles, when their use may generate risks to the physical integrity of people; IX – applications in the health field, including those intended to assist in medical diagnoses and procedures; X – biometric identification systems; XI – criminal investigation and public security, in particular for individual risk assessments by the competent authorities, in order to determine the risk of a person committing offenses or of recidivism, or the risk to potential victims of criminal offenses, or to assess the personality traits and characteristics or past criminal behavior of individuals or groups; XII – analytical study of crimes related to natural persons, allowing police authorities to search large sets of complex data, related or unrelated, available from different data sources or in different data formats, in order to identify

--- Page 17 ---

unknown patterns or discover hidden relationships in the  data; XIII – investigation by administrative authorities to assess the credibility of evidence during the investigation or repression of offenses, to predict the occurrence or recurrence of an actual or
... [truncated due to length budget]



---

### Candidate: "Gig Economy Workers"
**Initial reason**: Their specific concerns regarding rights and protections in the rapidly evolving AI landscape are not addressed.
**Initial score**: 80/100

**Relevant document sections:**
[Article 16 — It will be up to the competent authority to regulate artificial intelligence systems that pose excessive risks. Section III High Risk Art. 17. Artificial intelligence systems used for the following purposes are considered high-risk: I – application as safety devices in the management and operation of critical infrastructures, such as traffic control and water and electricity supply networks; II – education and vocational training, including systems for determining access to educational or vocational training institutions or for evaluating and monitoring students; III – recruitment, screening, filtering, candidate evaluation, decision-making regarding promotions or terminations of employment contracts, task allocation, and monitoring and evaluation of the performance and behavior of individuals affected by such artificial intelligence applications in the areas of employment, worker management, and access to self-employment; IV – evaluation of criteria for access, eligibility, granting, review, reduction or revocation of private and public services that are considered essential, including systems used to assess the eligibility of individuals for public]
Art. 16. It will be up to the competent authority to regulate artificial intelligence systems that pose excessive risks. Section III High Risk Art. 17. Artificial intelligence systems used for the following purposes are considered high-risk: I – application as safety devices in the management and operation of critical infrastructures, such as traffic control and water and electricity supply networks; II – education and vocational training, including systems for determining access to educational or vocational training institutions or for evaluating and monitoring students; III – recruitment, screening, filtering, candidate evaluation, decision-making regarding promotions or terminations of employment contracts, task allocation, and monitoring and evaluation of the performance and behavior of individuals affected by such artificial intelligence applications in the areas of employment, worker management, and access to self-employment; IV – evaluation of criteria for access, eligibility, granting, review, reduction or revocation of private and public services that are considered essential, including systems used to assess the eligibility of individuals for public

--- Page 16 ---

assistance and social security services;  V – assessment of the debt capacity of individuals or establishment of their credit rating; VI – dispatching or establishing priorities for emergency response services, including firefighters and medical assistance; VII – administration of justice, including systems that assist judicial authorities in investigating facts and applying the law; VIII – autonomous vehicles, when their use may generate risks to the physical integrity of people; IX – applications in the health field, including those intended to assist in medical diagnoses and procedures; X – biometric identification systems; XI – criminal investigation and public security, in particular for individual risk assessments by the competent authorities, in order to determine the risk of a person committing offenses or of recidivism, or the risk to potential victims of criminal offenses, or to assess the personality traits and characteristics or past criminal behavior of individuals or groups; XII – analytical study of crimes related to natural persons, allowing police authorities to search large sets of complex data, related or unrelated, available from different data sources or in different data formats, in order to identify

--- Page 17 ---

unknown patterns or discover hidden relationships in the  data; XIII – investigation by administrative authorities to assess the credibility of evidence during the investigation or repression of offenses, to predict the occurrence or recurrence of an actual or
... [truncated due to length budget]



---

### Candidate: "Mental Health Experts"
**Initial reason**: The potential effects of AI on mental health and well-being are inadequately addressed.
**Initial score**: 78/100

**Relevant document sections:**
[Article 16 — It will be up to the competent authority to regulate artificial intelligence systems that pose excessive risks. Section III High Risk Art. 17. Artificial intelligence systems used for the following purposes are considered high-risk: I – application as safety devices in the management and operation of critical infrastructures, such as traffic control and water and electricity supply networks; II – education and vocational training, including systems for determining access to educational or vocational training institutions or for evaluating and monitoring students; III – recruitment, screening, filtering, candidate evaluation, decision-making regarding promotions or terminations of employment contracts, task allocation, and monitoring and evaluation of the performance and behavior of individuals affected by such artificial intelligence applications in the areas of employment, worker management, and access to self-employment; IV – evaluation of criteria for access, eligibility, granting, review, reduction or revocation of private and public services that are considered essential, including systems used to assess the eligibility of individuals for public]
Art. 16. It will be up to the competent authority to regulate artificial intelligence systems that pose excessive risks. Section III High Risk Art. 17. Artificial intelligence systems used for the following purposes are considered high-risk: I – application as safety devices in the management and operation of critical infrastructures, such as traffic control and water and electricity supply networks; II – education and vocational training, including systems for determining access to educational or vocational training institutions or for evaluating and monitoring students; III – recruitment, screening, filtering, candidate evaluation, decision-making regarding promotions or terminations of employment contracts, task allocation, and monitoring and evaluation of the performance and behavior of individuals affected by such artificial intelligence applications in the areas of employment, worker management, and access to self-employment; IV – evaluation of criteria for access, eligibility, granting, review, reduction or revocation of private and public services that are considered essential, including systems used to assess the eligibility of individuals for public

--- Page 16 ---

assistance and social security services;  V – assessment of the debt capacity of individuals or establishment of their credit rating; VI – dispatching or establishing priorities for emergency response services, including firefighters and medical assistance; VII – administration of justice, including systems that assist judicial authorities in investigating facts and applying the law; VIII – autonomous vehicles, when their use may generate risks to the physical integrity of people; IX – applications in the health field, including those intended to assist in medical diagnoses and procedures; X – biometric identification systems; XI – criminal investigation and public security, in particular for individual risk assessments by the competent authorities, in order to determine the risk of a person committing offenses or of recidivism, or the risk to potential victims of criminal offenses, or to assess the personality traits and characteristics or past criminal behavior of individuals or groups; XII – analytical study of crimes related to natural persons, allowing police authorities to search large sets of complex data, related or unrelated, available from different data sources or in different data formats, in order to identify

--- Page 17 ---

unknown patterns or discover hidden relationships in the  data; XIII – investigation by administrative authorities to assess the credibility of evidence during the investigation or repression of offenses, to predict the occurrence or recurrence of an actual or
... [truncated due to length budget]



---

### Candidate: "Rural Communities"
**Initial reason**: The unique impacts of AI on rural populations, particularly in accessing services, are not considered.
**Initial score**: 77/100

**Relevant document sections:**
[Article 16 — It will be up to the competent authority to regulate artificial intelligence systems that pose excessive risks. Section III High Risk Art. 17. Artificial intelligence systems used for the following purposes are considered high-risk: I – application as safety devices in the management and operation of critical infrastructures, such as traffic control and water and electricity supply networks; II – education and vocational training, including systems for determining access to educational or vocational training institutions or for evaluating and monitoring students; III – recruitment, screening, filtering, candidate evaluation, decision-making regarding promotions or terminations of employment contracts, task allocation, and monitoring and evaluation of the performance and behavior of individuals affected by such artificial intelligence applications in the areas of employment, worker management, and access to self-employment; IV – evaluation of criteria for access, eligibility, granting, review, reduction or revocation of private and public services that are considered essential, including systems used to assess the eligibility of individuals for public]
Art. 16. It will be up to the competent authority to regulate artificial intelligence systems that pose excessive risks. Section III High Risk Art. 17. Artificial intelligence systems used for the following purposes are considered high-risk: I – application as safety devices in the management and operation of critical infrastructures, such as traffic control and water and electricity supply networks; II – education and vocational training, including systems for determining access to educational or vocational training institutions or for evaluating and monitoring students; III – recruitment, screening, filtering, candidate evaluation, decision-making regarding promotions or terminations of employment contracts, task allocation, and monitoring and evaluation of the performance and behavior of individuals affected by such artificial intelligence applications in the areas of employment, worker management, and access to self-employment; IV – evaluation of criteria for access, eligibility, granting, review, reduction or revocation of private and public services that are considered essential, including systems used to assess the eligibility of individuals for public

--- Page 16 ---

assistance and social security services;  V – assessment of the debt capacity of individuals or establishment of their credit rating; VI – dispatching or establishing priorities for emergency response services, including firefighters and medical assistance; VII – administration of justice, including systems that assist judicial authorities in investigating facts and applying the law; VIII – autonomous vehicles, when their use may generate risks to the physical integrity of people; IX – applications in the health field, including those intended to assist in medical diagnoses and procedures; X – biometric identification systems; XI – criminal investigation and public security, in particular for individual risk assessments by the competent authorities, in order to determine the risk of a person committing offenses or of recidivism, or the risk to potential victims of criminal offenses, or to assess the personality traits and characteristics or past criminal behavior of individuals or groups; XII – analytical study of crimes related to natural persons, allowing police authorities to search large sets of complex data, related or unrelated, available from different data sources or in different data formats, in order to identify

--- Page 17 ---

unknown patterns or discover hidden relationships in the  data; XIII – investigation by administrative authorities to assess the credibility of evidence during the investigation or repression of offenses, to predict the occurrence or recurrence of an actual or
... [truncated due to length budget]



---

## Global Document Context (For Logic Assessment)
[Article 8 — The individual affected by an artificial intelligence system may request an explanation of the decision, prediction, or recommendation, including information about the criteria and procedures used, as well as the main factors affecting that specific prediction or decision, including information about: I – the rationality and logic of the system, the meaning and the expected consequences of such a decision for the person affected; II – the degree and level of contribution of the artificial intelligence system to decision-making; III – the data processed and its source, the criteria for decision-making and, where appropriate, its weighting, applied to the situation of the person affected; IV – the mechanisms by which a person can challenge the decision; and V – the possibility of requesting human intervention, under the terms of this Law. Sole paragraph. The information mentioned in the heading will be provided through a free and facilitated procedure, in language that allows the person to understand the result of the decision or forecast in question, within a period of up to fifteen days from the date of the request, with the possibility of extension, once, for an equal period, depending on the complexity of the case.]
Article 8 The individual affected by an artificial intelligence system may request an explanation of the decision, prediction, or recommendation, including information about the criteria and procedures used, as well as the main factors affecting that specific prediction or decision, including information about: I – the rationality and logic of the system, the meaning and the expected consequences of such a decision for the person affected; II – the degree and level of contribution of the artificial intelligence system to decision-making; III – the data processed and its source, the criteria for decision-making and, where appropriate, its weighting, applied to the situation of the person affected; IV – the mechanisms by which a person can challenge the decision; and V – the possibility of requesting human intervention, under the terms of this Law. Sole paragraph. The information mentioned in the heading will be provided through a free and facilitated procedure, in language that allows the person to understand the result of the decision or forecast in question, within a period of up to fifteen days from the date of the request, with the possibility of extension, once, for an equal period, depending on the complexity of the case.

--- Page 10 ---

Section III On the right to contest decisions and to request human intervention Article 9 Individuals affected by artificial intelligence systems will have the right to challenge and request a review of decisions, recommendations, or predictions generated by such systems that produce relevant legal effects or significantly impact their interests. § 1. The right to correct incomplete, inaccurate, or outdated data used by artificial intelligence systems is guaranteed, as well as the right to request the anonymization, blocking, or deletion of unnecessary, excessive, or unlawfully processed data, pursuant to Article 18 of Law No. 13,709, of August 14, 2018, and relevant legislation. § 2. The right to contest, as provided for in the heading of this article, also covers decisions, recommendations, or predictions based on discriminatory or unreasonable inferences, or those that violate objective good faith, including inferences that: I - are based on data that is inadequate or abusive for the purposes of the processing; II – are based on imprecise or statistically unreliable methods; or III - They do not adequately consider the individuality and personal characteristics of individuals. Art. 10.

--- Page 11 ---

When a decision, prediction, or recommendation from an artificial intelligence system produces relevant legal effects or significantly impacts a person's interests, including through profiling and inferences, that person may request human intervention or review. Sole paragraph. Human intervention or review will not be required if its implementation is demonstrably impossible, in which case the person responsible for operating the artificial intelligence system will implement effective alternative measures to ensure a re-analysis of the contested decision, taking into account the arguments raised by the affected person, as well as the redress of any damages caused. Art. 11. In scenarios where decisions, predictions, or recommendations generated by artificial intelligence systems have an irreversible or difficult-to-reverse impact, or involve decisions that could pose risks to the life or physical integrity of individuals, there will be significant human involvement in the decision-making process and final human determination. Section IV On the right to non-discrimination and the correction of direct, indirect, illegal or abusive discriminatory biases Art. 12. People affected by decisions, predictions, or recommendations from artificial intelligence systems have the right to fair and equal treatment, and the implementation and use of artificial intelligence systems that may lead to direct, indirect, illegal, or abusive discrimination are prohibited, including:

--- Page 12 ---

I – due to the use of sensitive personal data or disproportionate impacts based on personal characteristics such as geographic origin, race, color or ethnicity, gender, sexual orientation, socioeconomic class, age, disability, religion or political opinions; or II – based on the establishment of disadvantages or the worsening of the vulnerability of people belonging to a specific group, even if apparently neutral criteria are used. Sole paragraph. The prohibition set forth in the main clause does not prevent the adoption of criteria for differentiation between individuals or groups when such differentiation is based on demonstrated, reasonable, and legitimate objectives or justifications in light of the right to equality and other fundamental rights. CHAPTER III RISK CATEGORIZATION Section I Preliminary Assessment Art. 13. Prior to being placed on the market or used in service, every artificial intelligence system will undergo a preliminary assessment by the supplier to classify its risk level, the registration of which will consider the criteria set forth in this chapter. § 1. Suppliers of general-purpose artificial intelligence systems shall include in their preliminary assessment the purposes or applications indicated, pursuant to Article 17 of this law.

--- Page 13 ---

§ 2. The preliminary assessment carried out by the supplier will be recorded and documented for accountability purposes in the event that the artificial intelligence system is not classified as high risk. § 3 The competent authority may determine the reclassification of the artificial intelligence system, upon prior notification, as well as determine the performance of an algorithmic impact assessment for the instruction of the ongoing investigation. § 4. If the result of the reclassification identifies the artificial intelligence system as high-risk, the performance of an algorithmic impact assessment and the adoption of the other governance measures provided for in Chapter IV will be mandatory, without prejudice to any penalties in case of a fraudulent, incomplete or untruthful preliminary assessment. Section II Excessive Risk Art. 14. The implementation and use of artificial intelligence systems are prohibited: I – that employ subliminal techniques that aim to or have the effect of inducing a natural person to behave in a way that is harmful or dangerous to their health or safety or contrary to the principles of this Law; II – that exploit any vulnerabilities of specific groups of natural persons, such as those associated with their age

--- Page 14 ---

or physical or mental disability, in order to induce them to  behave in a way that is harmful to their health or safety or contrary to the principles of this Law; III – by public authorities, to evaluate, classify or rank individuals based on their social behavior or personality attributes, through a universal scoring system, for access to goods, services and public policies, in an illegitimate or disproportionate manner. Art. 15. In the context of public safety activities, the use of remote biometric identification systems is only permitted, on a continuous basis, in spaces accessible to the public, when provided for in specific federal law and with judicial authorization in connection with individualized criminal prosecution activity, in the following cases: I – prosecution of crimes punishable by a maximum prison sentence of more than two years; II – searching for victims of crimes or missing persons; or III – crime caught in the act. Sole paragraph. The law referred to in the main clause shall provide for measures that are proportionate and strictly necessary to serve the public interest, observing due process of law and judicial review, as well as the principles and rights provided for in this Law, especially the guarantee against discrimination and the need for review of the algorithmic inference by the responsible public agent before taking any action against the identified person.

--- Page 15 ---

[Article 16 — It will be up to the competent authority to regulate artificial intelligence systems that pose excessive risks. Section III High Risk Art. 17. Artificial intelligence systems used for the following purposes are considered high-risk: I – application as safety devices in the management and operation of critical infrastructures, such as traffic control and water and electricity supply networks; II – education and vocational training, including systems for determining access to educational or vocational training institutions or for evaluating and monitoring students; III – recruitment, screening, filtering, candidate evaluation, decision-making regarding promotions or terminations of employment contracts, task allocation, and monitoring and evaluation of the performance and behavior of individuals affected by such artificial intelligence applications in the areas of employment, worker management, and access to self-employment; IV – evaluation of criteria for access, eligibility, granting, review, reduction or revocation of private and public services that are considered essential, including systems used to assess the eligibility of individuals for public]
Art. 16. It will be up to the competent authority to regulate artificial intelligence systems that pose excessive risks. Section III High Risk Art. 17. Artificial intelligence systems used for the following purposes are considered high-risk: I – application as safety devices in the management and operation of critical infrastructures, such as traffic control and water and electricity supply networks; II – education and vocational training, including systems for determining access to educational or vocational training institutions or for evaluating and monitoring students; III – recruitment, screening, filtering, candidate evaluation, decision-making regarding promotions or terminations of employment contracts, task allocation, and monitoring and evaluation of the performance and behavior of individuals affected by such artificial intelligence applications in the areas of employment, worker management, and access to self-employment; IV – evaluation of criteria for access, eligibility, granting, review, reduction or revocation of private and public services that are considered essential, including systems used to assess the eligibility of individuals for public

--- Page 16 ---

assistance and social security services;  V – assessment of the debt capacity of individuals or establishment of their credit rating; VI – dispatching or establishing priorities for emergency response services, including firefighters and medical assistance; VII – administration of justice, including systems that assist judicial authorities in investigating facts and applying the law; VIII – autonomous vehicles, when their use may generate risks to the physical integrity of people; IX – applications in the health field, including those intended to assist in medical diagnoses and procedures; X – biometric identification systems; XI – criminal investigation and public security, in particular for individual risk assessments by the competent authorities, in order to determine the risk of a person committing offenses or of recidivism, or the risk to potential victims of criminal offenses, or to assess the personality traits and characteristics or past criminal behavior of individuals or groups; XII – analytical study of crimes related to natural persons, allowing police authorities to search large sets of complex data, related or unrelated, available from different data sources or in different data formats, in order to identify

--- Page 17 ---

unknown patterns or discover hidden relationships in the  data; XIII – investigation by administrative authorities to assess the credibility of evidence during the investigation or repression of offenses, to predict the occurrence or recurrence of an actual or potential offense based on the profiling of individuals; or XIV – Migration management and border control. Art. 18. The competent authority will be responsible for updating the list of excessively risky or high-risk artificial intelligence systems, identifying new scenarios based on at least one of the following criteria: I - the implementation must be on a large scale, taking into account the number of people affected and the geographical extent, as well as its duration and frequency; II – the system could negatively impact the exercise of rights and freedoms or the use of a service; III – the system has a high potential for material or moral harm, as well as discrimination; IV - the system affects people from a specific vulnerable group; V – the potential harmful results of the artificial intelligence system are irreversible or difficult to reverse;

--- Page 18 ---

VI - a similar artificial intelligence system has previously caused material or moral damages; VII – low degree of transparency, explainability, and auditability of the artificial intelligence system, which hinders its control or supervision; VIII – high level of identifiability of data subjects, including the processing of genetic and biometric data for the purpose of uniquely identifying a natural person, especially when the processing involves combining, matching or comparing data from multiple sources; IX – when the data subject has reasonable expectations regarding the use of their personal data in the artificial intelligence system, especially the expectation of confidentiality, such as in the processing of confidential or sensitive data. Sole paragraph. The updating of the list mentioned in the heading by the competent authority will be preceded by consultation with the competent sectoral regulatory body, if any, as well as by public consultation and hearings and a regulatory impact analysis. CHAPTER IV ON THE GOVERNANCE OF ARTIFICIAL INTELLIGENCE SYSTEMS Section I General Provisions Art. 19. Artificial intelligence agents will establish governance structures and internal processes capable of ensuring the

--- Page 19 ---

security of the systems and the protection of the rights of  affected individuals, as provided for in Chapter II of this Law and relevant legislation, which will include, at least: I – Transparency measures regarding the use of artificial intelligence systems in interaction with natural persons, which includes the use of adequate and sufficiently clear and informative human-machine interfaces; II – transparency regarding the governance measures adopted in the development and use of the artificial intelligence system by the organization; III – appropriate data management measures for mitigating and preventing potential discriminatory biases; IV – legitimizing data processing in accordance with data protection legislation, including through the adoption of privacy measures by design and by default, and the adoption of techniques that minimize the use of personal data; V – adoption of appropriate parameters for separating and organizing data for training, testing, and validating system results; and VI – Adoption of appropriate information security measures from the design to the operation of the system. § 1 The governance measures for artificial intelligence systems are applicable throughout their entire life cycle, from initial conception to the termination of their activities and discontinuation.

--- Page 20 ---

§ 2. The technical documentation for high-risk artificial  intelligence systems shall be prepared before they are made available on the market or used for service provision and shall be kept up-to-date during their use. Section II Governance Measures for High-Risk Artificial Intelligence Systems Art. 20. In addition to the measures indicated in Article 19, artificial intelligence agents that provide or operate high-risk systems shall adopt the following governance measures and internal processes: I – documentation, in a format appropriate to the development process and the technology used, regarding the system's operation and the decisions involved in its construction, implementation, and use, considering all relevant stages in the system's life cycle, such as the design, development, evaluation, operation, and discontinuation stages of the system; II – use of tools for automatic recording of system operation, in order to allow the evaluation of its accuracy and robustness and to identify potential discriminatory factors, and implementation of the risk mitigation measures adopted, with special attention to adverse effects; III – conducting tests to assess appropriate levels of reliability, according to the sector and type of application of the artificial intelligence system, including robustness, accuracy, precision, and coverage tests;

--- Page 21 ---

IV – Data management measures to mitigate and prevent  discriminatory biases, including: a)   {{a)}}evaluation of the data with appropriate measures to control for human cognitive biases that may affect data collection and organization, and to avoid the generation of biases due to classification problems, failures or lack of information regarding affected groups, lack of coverage or distortions in representativeness, according to the intended application, as well as corrective measures to avoid the incorporation of structural social biases that may be perpetuated and amplified by technology; and b)   {{b)}}Inclusive team composition responsible for the design and development of the system, guided by the pursuit of diversity. V – Adoption of technical measures to enable the explainability of the results of artificial intelligence systems and measures to provide operators and potential stakeholders with general information about the functioning of the artificial intelligence model employed, explaining the logic and criteria relevant to the production of results, as well as, upon request from the interested party, providing adequate information that allows the interpretation of the results actually produced, respecting industrial and commercial confidentiality. Sole paragraph. Human supervision of high-risk artificial intelligence systems will seek to prevent or minimize risks to the rights and freedoms of individuals that may arise from their normal use or from their use under reasonably foreseeable conditions of misuse, enabling those

--- Page 22 ---

responsible for human supervision to:  I – to understand the capabilities and limitations of the artificial intelligence system and properly control its operation, so that signs of anomalies, malfunctions, and unexpected performance can be identified and resolved as quickly as possible; II – to be aware of the potential tendency to automatically trust or over-trust the results produced by the artificial intelligence system; III – correctly interpret the results of the artificial intelligence system, taking into account the system's characteristics and the available interpretation tools and methods; IV – to decide, in any specific situation, not to use the high-risk artificial intelligence system or to ignore, annul, or reverse its result; and V – Interfering with the operation of a high-risk artificial intelligence system or disrupting its operation. Art. 21. In addition to the governance measures established in this chapter, public bodies and entities of the Union, States, Federal District and Municipalities, when contracting, developing or using artificial intelligence systems considered high-risk, shall adopt the following measures: I – Conducting prior public consultations and hearings on the planned use of artificial intelligence systems, with

--- Page 23 ---

information on the data to be used, the general logic of  operation, and the results of tests performed; II – definition of system access and usage protocols that allow for the recording of who used it, for what specific situation, and for what purpose; III – use of data from reliable sources that are accurate, relevant, up-to-date, and representative of the affected populations and tested against discriminatory biases, in accordance with Law No. 13,709, of August 14, 2018, and its regulatory acts; IV – facilitated and effective guarantee to the citizen, before the public authorities, of the right to human explanation and review of decisions made by artificial intelligence systems that generate relevant legal effects or that significantly impact the interests of the affected party, to be carried out by the competent public agent; V – use of an application programming interface that allows its use by other systems for interoperability purposes, as regulated; and VI – publication in easily accessible media, preferably on their websites, of preliminary assessments of artificial intelligence systems developed, implemented, or used by the public authorities of the Union, States, Federal District, and Municipalities, regardless of the degree of risk, without prejudice to the provisions of article 43. § 1 The use of biometric systems by the public authorities of the Union, States, Federal District and Municipalities shall

--- Page 24 ---

be preceded by the issuance of a normative act that  establishes guarantees for the exercise of the rights of the affected person and protection against direct, indirect, illegal or abusive discrimination, prohibiting the processing of data on race, color or ethnicity, except as expressly provided by law. § 2 In the event that it is impossible to eliminate or substantively mitigate the risks associated with the artificial intelligence system identified in the algorithmic impact assessment provided for in Article 22 of this Law, its use will be discontinued. Section III Algorithmic Impact Assessment Art. 22. An algorithmic impact assessment of artificial intelligence systems is mandatory for AI agents whenever the system is deemed high-risk by the preliminary assessment. Sole paragraph. The competent authority will be notified about the high-risk system through the sharing of the preliminary and algorithmic impact assessments. Art. 23. The algorithmic impact assessment will be conducted by a professional or team of professionals with the necessary technical, scientific, and legal knowledge to prepare the report, and with functional independence. Sole paragraph. The competent authority will be responsible for regulating the cases in which the performance or

--- Page 25 ---

auditing of the impact assessment will necessarily be  conducted by a professional or team of professionals external to the supplier; Art. 24. The impact assessment methodology will include, at least, the following steps: I - preparation; II – risk awareness; III – mitigating the risks identified; IV – Monitoring. § 1 The impact assessment shall consider and record, at least: a)   {{a)}}known and foreseeable risks associated with the artificial intelligence system at the time it was developed, as well as the risks that can reasonably be expected from it; b)   {{b)}}benefits associated with the artificial intelligence system; (   c) probability of adverse consequences, including the number of people potentially impacted; d)   {{d)}}severity of adverse consequences, including the effort required to mitigate them;

--- Page 26 ---

e)   {{e)}}logic of operation of the artificial intelligence system; f)   {{f)}}process and results of tests and assessments and mitigation measures carried out to verify possible impacts on rights, with special emphasis on potential discriminatory impacts; (   g) Training and awareness campaigns regarding the risks associated with artificial intelligence systems; h)   {{h)}} mitigation measures and indication and justification of the residual risk of the artificial intelligence system, accompanied by frequent quality control tests; and i)   {{i)}}Transparency measures to the public, especially to potential users of the system, regarding residual risks, particularly when they involve a high degree of harmfulness or danger to the health or safety of users, in accordance with articles 9 and 10 of Law No. 8,078, of September 11, 1990 (Consumer Protection Code). § 2 In accordance with the precautionary principle, when using artificial intelligence systems that may generate irreversible or difficult-to-reverse impacts, the algorithmic impact assessment will also take into account incipient, incomplete, or speculative evidence. § 3 The competent authority may establish other criteria and elements for the preparation of impact assessments, including the participation of different affected social segments, according to the risk and economic size of the

--- Page 27 ---

organization.  § 4. The competent authority shall regulate the frequency of updating impact assessments, taking into account the life cycle of high-risk artificial intelligence systems and their fields of application, and may incorporate best sectoral practices. § 5. Artificial intelligence agents that, after their introduction to the market or use in service, become aware of an unexpected risk they present to the rights of natural persons, shall immediately communicate this fact to the competent authorities and to the persons affected by the artificial intelligence system. Art. 25. Algorithmic impact assessment will consist of a continuous iterative process, executed throughout the entire lifecycle of high-risk artificial intelligence systems, requiring periodic updates. § 1 The competent authority shall be responsible for regulating the frequency of updating impact assessments. § 2 The updating of the algorithmic impact assessment will also include public participation, through a consultation procedure with stakeholders, albeit in a simplified manner. Art. 26. With industrial and commercial secrets protected, the conclusions of the impact assessment will be made public,

--- Page 28 ---

containing at least the following information:  I – Description of the intended purpose for which the system will be used, as well as its context of use and territorial and temporal scope; II – risk mitigation measures, as well as their residual level, once such measures have been implemented; and III – description of the participation of different affected segments, if any, in accordance with paragraph 3 of article 24 of this Law. CHAPTER V OF CIVIL LIABILITY Art. 27. The supplier or operator of an artificial intelligence system that causes property, moral, individual, or collective damage is obligated to fully compensate for it, regardless of the system's degree of autonomy. § 1 In the case of high-risk or excessively risky artificial intelligence systems, the supplier or operator is objectively liable for the damages caused, to the extent of their participation in the damage. § 2. When the system in question is not a high-risk artificial intelligence system, the fault of the agent causing the damage will be presumed, and the burden of proof will be reversed in favor of the victim. Art. 28.

--- Page 29 ---

Artificial intelligence agents will not be held liable when:  I – prove that they did not put into circulation, use, or take advantage of the artificial intelligence system; or II - prove that the damage resulted from an act solely attributable to the victim or a third party, as well as from an external fortuitous event. Art. 29. The hypotheses of civil liability arising from damages caused by artificial intelligence systems within the scope of consumer relations remain subject to the rules set forth in Law No. 8,078, of September 11, 1990 (Consumer Protection Code), without prejudice to the application of the other provisions of this Law. CHAPTER VI CODES OF GOOD PRACTICE AND GOVERNANCE Art. 30. Artificial intelligence agents may, individually or through associations, formulate codes of good practice and governance that establish the conditions of organization, the operating regime, the procedures, including those regarding complaints from affected individuals, the safety standards, the technical standards, the specific obligations for each implementation context, the educational actions, the internal mechanisms for supervision and risk mitigation, and the appropriate technical and organizational security measures for managing the risks arising from the application of the systems. § 1. When establishing rules of good practice, the purpose, probability, and severity of the risks and benefits arising

--- Page 30 ---

therefrom shall be considered, as exemplified by the  methodology set forth in Article 24 of this Law. § 2. Developers and operators of artificial intelligence systems may: I – Implement a governance program that, at a minimum: a)   {{a)}} demonstrate your commitment to adopting internal processes and policies that ensure comprehensive compliance with standards and best practices regarding non-maleficence and proportionality between the methods employed and the determined and legitimate purposes of artificial intelligence systems; b)   {{b)}} be adapted to the structure, scale and volume of its operations, as well as its potential for harm; c)   {{c)}} aims to establish a relationship of trust with the affected people, through transparent actions that ensure participation mechanisms as per article 24, § 3, of this Law; d)   {{d)}} is integrated into its overall governance structure and establishes and applies internal and external oversight mechanisms; (e)   {{e)}}have response plans in place to reverse any potentially harmful outcomes from the artificial intelligence system; and f)   {{f)}} be constantly updated based on information obtained from continuous monitoring and periodic evaluations.

--- Page 31 ---

§ 3. Voluntary adherence to a code of good practices and governance may be considered indicative of good faith on the part of the agent and will be taken into account by the competent authority for the purpose of applying administrative sanctions. § 4 The competent authority may establish a procedure for analyzing the compatibility of the code of conduct with current legislation, with a view to its approval, publication and periodic updating. CHAPTER VII ON REPORTING SERIOUS INCIDENTS Art. 31. Artificial intelligence agents will report serious security incidents to the competent authority, including those involving risks to life and physical integrity, disruption of critical infrastructure operations, serious damage to property or the environment, as well as serious violations of fundamental rights, in accordance with the regulations. § 1 The communication will be made within a reasonable timeframe, as defined by the competent authority. § 2 The competent authority will verify the seriousness of the incident and may, if necessary, order the agent to take steps and measures to reverse or mitigate the effects of the incident. CHAPTER VIII ON SUPERVISION AND INSPECTION Section I Of the Competent Authority

--- Page 32 ---

[Article 32 — The Executive Branch will designate the competent authority to oversee the implementation and enforcement of this Law. Sole paragraph. It is the responsibility of the competent authority to: I – to ensure the protection of fundamental rights and other rights affected by the use of artificial intelligence systems; II – to promote the development, updating, and implementation of the Brazilian Artificial Intelligence Strategy with the relevant competent bodies; III – To promote and develop studies on best practices in the development and use of artificial intelligence systems; IV – to encourage the adoption of best practices, including codes of conduct, in the development and use of artificial intelligence systems; V – to promote cooperation actions with authorities responsible for the protection and development and use of artificial intelligence systems in other countries, of an international or transnational nature; VI – to issue regulations for this Law, including regarding: a)   {{a)}} procedures associated with the exercise of the rights provided for in this Law;]
Art. 32.  The Executive Branch will designate the competent authority to oversee the implementation and enforcement of this Law. Sole paragraph. It is the responsibility of the competent authority to: I – to ensure the protection of fundamental rights and other rights affected by the use of artificial intelligence systems; II – to promote the development, updating, and implementation of the Brazilian Artificial Intelligence Strategy with the relevant competent bodies; III – To promote and develop studies on best practices in the development and use of artificial intelligence systems; IV – to encourage the adoption of best practices, including codes of conduct, in the development and use of artificial intelligence systems; V – to promote cooperation actions with authorities responsible for the protection and development and use of artificial intelligence systems in other countries, of an international or transnational nature; VI – to issue regulations for this Law, including regarding: a)   {{a)}} procedures associated with the exercise of the rights provided for in this Law;

--- Page 33 ---

b)   Procedures and requirements for preparing the  algorithmic impact assessment; c)   {{c)}}Form and requirements of the information to be made public about the use of artificial intelligence systems; and d)   Procedures for certification of the development and use of high-risk systems. VII – to coordinate with public regulatory authorities to exercise their powers in specific sectors of economic and governmental activities subject to regulation; VIII – to oversee, independently or jointly with other competent public bodies, the dissemination of the information provided for in Articles 7 and 43; IX – to oversee and apply sanctions in cases of development or use of artificial intelligence systems carried out in violation of the law, through an administrative process that ensures due process, the right to a full defense, and the right to appeal; X – to request, at any time, from public entities that develop or use artificial intelligence systems, specific information on the scope, nature of the data, and other details of the processing carried out, with the possibility of issuing a supplementary technical opinion to ensure compliance with this Law; XI – to enter into agreements, at any time, with artificial intelligence agents to eliminate irregularities, legal

--- Page 34 ---

uncertainty, or contentious situations within the scope of  administrative processes, in accordance with the provisions of Decree-Law No. 4,657, of September 4, 1942; XII – to review petitions against the artificial intelligence system operator, after proof of submission of a complaint that was not resolved within the time frame established in regulations; and XIII – to prepare annual reports on its activities. Sole paragraph. In exercising the powers of the main clause, the competent body may establish differentiated conditions, requirements, communication and dissemination channels for suppliers and operators of artificial intelligence systems qualified as micro or small enterprises, under the terms of Complementary Law No. 123, of December 14, 2006, and startups, under the terms of Complementary Law No. 182, of June 1, 2021. Art. 33. The competent authority will be the central body for the application of this Law and for establishing rules and guidelines for its implementation. Art. 34. The competent authority and the public bodies and entities responsible for regulating specific sectors of economic and governmental activity shall coordinate their activities, within their respective spheres of action, with a view to ensuring compliance with this Law.

--- Page 35 ---

§ 1 The competent authority shall maintain a permanent forum for communication, including through technical cooperation, with bodies and entities of the public administration responsible for regulating specific sectors of economic and governmental activity, in order to facilitate its regulatory, supervisory and sanctioning powers. § 2 In experimental regulatory environments (regulatory sandboxes) involving artificial intelligence systems, conducted by public bodies and entities responsible for regulating specific sectors of economic activity, the competent authority will be notified and may comment on compliance with the purposes and principles of this law. Art. 35. The regulations and standards issued by the competent authority will be preceded by public consultation and hearings, as well as regulatory impact analyses, in accordance with articles 6 to 12 of Law No. 13,848, of June 25, 2019, where applicable. Section II Administrative Sanctions Art. 36. Artificial intelligence agents, due to violations of the rules established in this Law, are subject to the following administrative sanctions applicable by the competent authority: I – warning;

--- Page 36 ---

II – a simple fine, limited in total to R$ 50,000,000.00 (fifty  million reais) per infraction, being, in the case of a private legal entity, up to 2% (two percent) of its revenue, of its group or conglomerate in Brazil in its last fiscal year, excluding taxes; III – Publicizing the infraction after it has been duly investigated and its occurrence confirmed; IV – prohibition or restriction on participating in a regulatory sandbox regime provided for in this Law, for up to five years; V – partial or total, temporary or permanent suspension of the development, supply, or operation of the artificial intelligence system; and VI – Prohibition of processing certain databases. § 1 The sanctions will be applied after an administrative procedure that allows for the opportunity of full defense, in a gradual, isolated or cumulative manner, according to the peculiarities of the specific case and considering the following parameters and criteria: I – the seriousness and nature of the offenses and the possible violation of rights; II - the good faith of the offender; III – the advantage gained or intended by the offender; IV – the economic condition of the offender;

--- Page 37 ---

V – recidivism; VI – the degree of damage; VII – the cooperation of the offender; VIII – the repeated and demonstrated adoption of internal mechanisms and procedures capable of minimizing risks, including algorithmic impact analysis and effective implementation of a code of ethics; IX – the adoption of a policy of good practices and governance; X – the prompt adoption of corrective measures; XI – the proportionality between the seriousness of the offense and the severity of the sanction; and XII – the accumulation with other administrative sanctions that may have already been definitively applied for the same unlawful act. § 2 Before or during the administrative process of § 1, the competent authority may adopt preventive measures, including a penalty fine, observing the total limit referred to in item II of the main clause, when there is evidence or well- founded fear that the artificial intelligence agent: I – cause or may cause irreparable or difficult-to-repair injury; or II – render the final result of the process ineffective.

--- Page 38 ---

§ 3 The provisions of this article do not replace the application of administrative, civil or criminal sanctions defined in Law No. 8,078, of September 11, 1990, in Law No. 13,709, of August 14, 2018, and in specific legislation. § 4 In the case of the development, supply or use of excessively risky artificial intelligence systems, at a minimum, a fine will be applied and, in the case of a legal entity, the partial or total, temporary or permanent suspension of its activities. § 5 The application of the sanctions provided for in this article does not, under any circumstances, exclude the obligation to fully compensate for the damage caused, pursuant to article 27. Art. 37. The competent authority will define, through its own regulations, the procedure for investigating and the criteria for applying administrative sanctions to violations of this Law, which will be subject to public consultation, without prejudice to the provisions of Decree-Law No. 4,657, of September 4, 1942, Law No. 9,784, of January 29, 1999, and other relevant legal provisions. Sole paragraph. The methodologies referred to in the heading of this article will be published in advance and will objectively present the forms and dosages of the sanctions, which will contain a detailed justification of all their elements, demonstrating compliance with the criteria provided for in this Law.

--- Page 39 ---

Section III Measures to promote innovation  Art. 38. The competent authority may authorize the operation of an experimental regulatory environment for innovation in artificial intelligence (regulatory sandbox) for entities that request it and meet the requirements specified by this Law and regulations. Art. 39. Applications for authorization for regulatory sandboxes will be submitted to the competent authority through a project whose characteristics include, among others: I – innovation in the use of technology or in the alternative use of existing technologies; II – improvements aimed at efficiency gains, cost reduction, increased safety, risk reduction, benefits to society and consumers, among others; III – Discontinuation plan, outlining the measures to be taken to ensure the operational viability of the project once the regulatory sandbox authorization period has ended. Art. 40. The competent authority will issue regulations to establish the procedures for requesting and authorizing the operation of regulatory sandboxes, and may limit or interrupt their operation, as well as issue recommendations, taking into

--- Page 40 ---

account, among other aspects, the preservation of  fundamental rights, the rights of potentially affected consumers, and the security and protection of personal data that are the subject of processing. Art. 41. Participants in the artificial intelligence regulation testing environment remain liable, under applicable liability legislation, for any damages inflicted on third parties as a result of experimentation taking place in the testing environment. Art. 42. The automated use of works, such as extraction, reproduction, storage, and transformation, in data and text mining processes using artificial intelligence systems, in activities carried out by research organizations and institutions, journalism, and by museums, archives, and libraries, does not constitute copyright infringement, provided that: I – does not aim solely at the reproduction, exhibition or dissemination of the original work itself; II – the use occurs to the extent necessary to achieve the objective; III – does not unjustifiably harm the economic interests of the holders; and IV – does not interfere with the normal operation of the works.

--- Page 41 ---

§ 1 Any reproductions of works for data mining activities shall be kept under strict security conditions, and only for the time necessary to carry out the activity or for the specific purpose of verifying the results of the scientific research. § 2. The provisions of the main clause apply to data and text mining activities for other analytical activities in artificial intelligence systems, provided that the conditions of the main clause and § 1 are met, and provided that the activities do not communicate the work to the public and that access to the works has been legitimate. § 3 The activity of text and data mining involving personal data will be subject to the provisions of Law No. 13.709, of August 14, 2018 (General Law on the Protection of Personal Data). Section IV Public Artificial Intelligence Database Art. 43. It is the responsibility of the competent authority to create and maintain a publicly accessible, high-risk artificial intelligence database containing public impact assessment documents, respecting commercial and industrial secrets, in accordance with the regulations. CHAPTER IX FINAL PROVISIONS Art. 44. The rights and principles expressed in this Law do not exclude others provided for in the national legal system or in

--- Page 42 ---

international treaties to which the Federative Republic of Brazil  is a party. Art. 45. This law shall enter into force one year after its publication. JUSTIFICATION The development and popularization of artificial intelligence technologies have revolutionized various areas of human activity. Furthermore, predictions indicate that artificial intelligence (AI) will bring about even more profound economic and social changes in the near future. Recognizing the importance of this issue, several legislative proposals have recently been introduced in both the Federal Senate and the Chamber of Deputies, aiming to establish guidelines for the development and application of artificial intelligence systems in Brazil. In particular, noteworthy bills include Bill No. 5,051 of 2019, authored by Senator Styvenson Valentim, which establishes the principles for the use of Artificial Intelligence in Brazil; Bill No. 21 of 2020, by Federal Deputy Eduardo Bismarck, which establishes the foundations, principles, and guidelines for the development and application of artificial intelligence in Brazil; and provides other measures, and which was approved by the Chamber of Deputies; and Bill No. 872 of 2021, by Senator Veneziano Vital do Rêgo, which deals with the use of Artificial Intelligence. On February 3, 2022, these three projects began to be processed jointly in the Federal Senate and, subsequently, on February 17 of the same year, through Act No. 4 of 2022 of the President of the

--- Page 43 ---

Federal Senate, authored by me at the suggestion of Senator  Eduardo Gomes, with the aim of drafting a legal text with the most advanced technicality, a Commission of Jurists was established to support the drafting of a substitute bill for them. Composed of renowned jurists, the commission included leading experts in the fields of civil law and digital law, to whom I am grateful for their time, dedication, and sharing of the final text, which I now present. The panel included: Minister of the Superior Court of Justice, Ricardo Villas Bôas Cueva (President); Laura Schertel Ferreira Mendes (Rapporteur); Ana de Oliveira Frazão; Bruno Ricardo Bioni; Danilo Cesar Maganhoto Doneda (in memoriam); Fabrício de Mota Alves; Miriam Wimmer; Wederson Advincula Siqueira; Claudia Lima Marques; Juliano Souza de Albuquerque Maranhão; Thiago Luís Santos Sombra; Georges Abboud; Frederico Quadros D'Almeida; Victor Marcel Pinheiro; Estela Aranha; Clara Iglesias Keller; Mariana Giorgetti Valente; and Filipe José Medon Affonso. Furthermore, I must also express my gratitude to the technical staff of the Federal Senate, especially the Legislative Consulting office and the employees who provided support to the committee: Reinilson Prado dos Santos; Renata Felix Perez and Donaldo Portela Rodrigues. The aforementioned Commission held a series of public hearings, as well as an international seminar, listening to more than seventy experts on the subject, representing various segments: organized civil society, government, academia, and the private sector. It also opened the opportunity for participation by any interested parties through written contributions, receiving 102 submissions, which were individually analyzed and organized according to their proposals. Finally, the Commission requested a study from the Legislative Advisory Office of the Federal Senate on the regulation of artificial intelligence in more than thirty countries belonging to the Organisation for Economic Co-

--- Page 44 ---

operation and Development (OECD), which allowed for an  analysis of the global regulatory landscape on the subject. Based on all this extensive material, on December 6, 2022, the Commission of Jurists presented its final report, along with a draft bill for the regulation of artificial intelligence. In this context, the present initiative is based on the conclusions of the aforementioned Commission and seeks to reconcile, within the legal framework, the protection of fundamental rights and freedoms, the promotion of work and the dignity of the human person, and the technological innovation represented by artificial intelligence. The project has a dual objective. On the one hand, it establishes rights to protect the most vulnerable party in question, the individual who is already impacted daily by artificial intelligence systems, from content recommendations and targeted advertising on the Internet to their eligibility analysis for credit and certain public policies. On the other hand, by providing governance tools and an institutional framework for oversight and supervision, it creates conditions for predictability regarding its interpretation and, ultimately, legal certainty for innovation and technological development. The proposition therefore starts from the premise that there is no trade-off between the protection of fundamental rights and freedoms, the valorization of work and the dignity of the human person in the face of the economic order and the creation of new value chains. On the contrary, its foundations and its principled basis seek such harmonization, in accordance with the Federal Constitution. Structurally, the proposal establishes risk-based regulation and a

--- Page 45 ---

rights-based regulatory model. It also presents governance  instruments for adequate accountability of economic agents who develop and use artificial intelligence, encouraging good faith action and effective risk management. The proposed text initially defines the fundamentals and general principles for the development and use of artificial intelligence systems, which underpin all other specific provisions. It dedicates a specific chapter to protecting the rights of people affected by artificial intelligence systems, in which it: guarantees appropriate access to information and adequate understanding of the decisions made by these systems; establishes and regulates the right to contest automated decisions and to request human intervention; and governs the right to non-discrimination and the correction of discriminatory biases. In addition to establishing basic and transversal rights for any and all contexts in which there is interaction between machines and humans, such as information and transparency, this obligation intensifies when the AI   system produces relevant legal effects or significantly impacts individuals (e.g., the right to contest and human intervention). Thus, the weight of regulation is calibrated according to the potential risks of the technology's application context. Symmetrically to these rights, certain general and specific governance measures were established for, respectively, artificial intelligence systems with any degree of risk and those categorized as high-risk. When addressing the categorization of artificial intelligence risks, the proposal establishes the requirement for preliminary assessment; defines prohibited applications due to excessive risk; and defines high-risk applications subject to stricter control standards.

--- Page 46 ---

Regarding the governance of systems, the project lists the measures to be adopted to ensure transparency and the mitigation of biases; establishes additional measures for high-risk systems and for governmental artificial intelligence systems; and standardizes the procedure for algorithmic impact assessment. The text also addresses the rules of civil liability involving artificial intelligence systems, including defining the circumstances under which those responsible for their development and use will not be held liable. According to the gradation of standards based on the risk posed by the system – which permeates the entire draft of the proposal – an important distinction is made in the chapter on civil liability: when dealing with a high-risk or excessively risky AI system, the supplier or operator is objectively liable for the damages caused, to the extent of each party's participation in the damage. And when dealing with AI that is not high-risk, the fault of the agent causing the damage will be presumed, applying the reversal of the burden of proof in favor of the victim. The project also reinforces protection against discrimination through various instruments, such as the right to information and understanding, the right to contest, and a specific right to correct direct, indirect, illegal, or abusive discriminatory biases, in addition to preventive governance measures. Besides adopting definitions of direct and indirect discrimination – thus incorporating definitions from the Inter-American Convention against Racism, promulgated in 2022 – the text focuses on (hyper)vulnerable groups, both for the qualification of what constitutes a high-risk system and for the reinforcement of certain rights. Regarding the oversight of artificial intelligence, the bill stipulates

--- Page 47 ---

that the Executive Branch designate an authority to ensure  compliance with the established rules, specifies its competencies, and sets administrative sanctions. Measures are also planned to foster innovation in artificial intelligence, notably through an experimental regulatory environment (regulatory sandbox). Thus, using a mixed approach of ex-ante and ex-post provisions, the proposal outlines criteria for evaluating and triggering the types of actions that should be taken to mitigate the risks involved, also involving the sectors interested in the regulatory process, through co-regulation. Furthermore, in line with international law, it establishes guidelines for aligning copyright and intellectual property rights with the notion that data should be a common good and, therefore, circulate for machine training and the development of artificial intelligence systems – without, however, implying harm to the holders of such rights. This leads to implications for how regulation can foster innovation. Given the above, and aware of the challenge this matter represents, we count on the collaboration of our esteemed colleagues to improve this proposal. Session Room, Senator Rodrigo Pacheco

---

Return ONLY valid JSON. 

**CRITICAL: You MUST return ALL three top-level keys: `institutionalLogics`, `absentActors`, and `methodologicalNotes`. If NO absent actors are verified, return `"absentActors": []`. NEVER return an empty object `{}`.**

### Valid Enum Values (use exactly one of these for each field — do NOT put pipes in your response):
- **absenceType**: textual-absence, structural-exclusion, discursive-marginalization, constitutive-silence
- **exclusionType**: silenced, marginalized, structurally-excluded, displaced
- **conflictType** (for discourseThreats): contradicts, undermines, complicates, challenges
- **dominantDiscourse** (for discourseThreats): use one of: market efficiency, economic competitiveness, national security, environmental sustainability, social equity, technical expertise, bureaucratic standardization, innovation / flexibility, fiscal responsibility, democratic participation, data protection / privacy, human rights, geopolitical sovereignty, precautionary principle / risk aversion, or "Other: [brief label]"

```json
{
  "institutionalLogics": {
    "market": { "strength": 0.8, "champions": ["Actor A", "Actor B"], "material": "Description of material practices", "discursive": "Description of discursive patterns" },
    "state": { "strength": 0.5, "champions": ["Actor C"], "material": "...", "discursive": "..." },
    "professional": { "strength": 0.3, "champions": ["Actor D"], "material": "...", "discursive": "..." },
    "community": { "strength": 0.1, "champions": ["Actor E"], "material": "...", "discursive": "..." }
  },
  "absentActors": [
    {
      "name": "Example Absent Actor",
      "absenceType": "structural-exclusion",
      "reason": "Detailed explanation citing document framing (min 100 chars for strong absences)",
      "absenceStrength": 75,
      "exclusionType": "silenced",
      "institutionalLogics": { "market": 0.1, "state": 0.2, "professional": 0.1, "community": 0.9 },
      "potentialConnections": [
        {
          "targetActor": "EXACT name from existing network",
          "relationshipType": "would have consultation rights with",
          "evidence": "Section 12 states 'communities shall be informed' but defines communities as municipal governments only."
        }
      ],
      "discourseThreats": [
        {
          "dominantDiscourse": "market efficiency",
          "conflictType": "contradicts",
          "explanation": "1-2 sentences. Quote evidence from the document."
        }
      ]
    }
  ],
  "methodologicalNotes": "Brief explanation of analytical approach and any potential biases"
}
```

### Example Mapping of an Absence:
**Candidate**: "Local Farmers"
**Absence Type**: "structural-exclusion"
**Reason**: "The policy centers industrial export-led agriculture, providing no mechanism for small-holder participation or protection."
**Potential Connection**:
- **targetActor**: "National Agricultural Registry"
- **relationshipType**: "excludes from registration criteria"
- **evidence**: "Section 4 defines eligible producers as 'entities with minimum 100 hectares', which structurally excludes small-scale farmers."

## Quality Requirements
1. **Evidence should cite document framing** using verbatim quotes where possible.
2. **targetActor must use EXACT names** from the existing network
3. **absenceStrength** — recalibrate based on deep evidence:
   - 0-30 (Weak): Mentioned but underrepresented
   - 30-60 (Moderate): Missing from key sections
   - 60-85 (Strong): Systematically excluded
   - 85-100 (Critical): Exclusion undermines policy legitimacy
4. If a candidate does NOT hold up under deep analysis, **lower its score or omit it**
5. Include **methodologicalNotes** explaining your analytical approach
6. For **discourseThreats**, identify 0–2 dominant discourses that this actor's inclusion would challenge. Use the standardized labels when possible. Quote evidence. If no clear conflict exists, return an empty array.

## Reflexivity Check
Before finalizing, consider:
- Am I over-representing certain actor types due to training data biases?
- Am I assuming governance norms that may not apply to this policy?
- Am I flagging absences that are genuinely irrelevant to this policy domain?